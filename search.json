[
  {
    "objectID": "assure-analyze/vis.html",
    "href": "assure-analyze/vis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Effective data visualizations - the ones that stick with us, can help us understand complex issues, and those that encourage us to change our behaviors and actions to be more equitable, inclusive, and kind - they are more that points on a map. Great data visualizations are communication tools that are user-centered and tell a compelling story that connects with the audience.\nWhen thinking of the Data-Information-Knowledge-Wisdom graphic at the top of the Data Analysis page - data visualizations are one tool we can use to add context and meaning to data to create information and knowledge. Ideally, the insights gleaned from our visualizations can then be used to make data and equity-informed decisions and to take effective and impactful action.\n\n\nWhen it comes to creating data visualizations with an equity lens, it boils down to making decisions that consider equity and inclusion in the way results are shown/communicated and that promote accessibility of the data, information, and tool as a whole. As you develop your data visualization or equivalent application - be sure to keep the below considerations in mind and make data, communication, and design choices that support the advancement of equity, inclusion, and justice.\n\n\n\n\nConsider who the audience will be for the product you’re developing and make decisions that will prioritize their needs so they are able to easily and efficiently use, engage, interact, and experience the product.\n\n\n\nGraphic illustrating three key components of the user-centered design process: Research, Empathy, and Iteration. Image from Justinmind (2020)\n\n\nResearch - Let’s use an equity lens here. Instead of “researching” your users and audience, try getting to know them by prioritizing relationship building and engagement. Ideally, by the time you’re at the data visualization phase, you have already identified key partners and have been working or engaging with them during other phases of the project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\nEmpathy - According to Dr. Brené Brown: empathy is about feeling WITH people, and requires four qualities:\n\nPerspective Taking, or putting yourself in someone else’s shoes\nStaying out of judgement and listening\nRecognizing emotion in another person that you have maybe felt before\nCommunicating that you can recognize that emotion\n\nIntegrating empathy into our data-intensive work requires us to consider how our audience and/or the communities whose data are being used will percieve or be impacted by our work. It means ensuring we’re developing the product and thinking about the data as more than mere points on a map or visualization, but as representing real humans, environments, or conditions that should be contextualized and considered with care. Some questions to consider include:\n\nWho is vulnerable in this context and how would they want to be counted?\nWhat information would they need to improve their lives?\nWho is undercounted or possibly missing entirely?\nWho was counted? Who did the counting? Why were they asking these people?\nWho benefits or is harmed if you forget the dots are people?\n\nIteration - The key here is knowing and planning for an iterative process from the beginning of your process. Add that feedback, implementation, and testing loop into your plan and allocate appropriate time to each piece to occur. When working with partners or communities using an iterative approach, be sure to consider:\n\nWhen it is appropriate to ask for feedback, and when it might be burdensome\nDifferent ways feedback might be gathered. We might think sending an email with a poll or survey linked is the easiest, but our partner might find it easier to talk through questions over the phone with you. Knowing which method(s) to use comes with time and relationship. When in doubt - ask for what people prefer and do your best to accommodate those requests.\nHow much time is adequate for folks to be able to review what you send them and provide their feedback. When in doubt - plan for a longer feedback period than you think might be needed and confirm timelines with your partners (and adjust them if you can when your partners indicate more time is needed)\n\n\n\n\nIt’s not uncommon for project teams to have gaps in data, even after all of your work and investment into the planning, data preparation and collection steps of the project. Often, this limitation of the data is out of your control - especially when you are using data from external sources. When this happens, it’s important to acknowledge, document, communicate those gaps and who is not adequately represented in your data product in a way that is accessible.\n\n\n\nHow we aggregate or disaggregate the data can impact which groups are “seen” and well represented (or not) in our visualizations. This can also influence who is centered, valued, or prioritized in the narrative of the visualization, and who is excluded.\nCarefully consider how groups are lumped or split - by aggregating many groups in the visualization beyond what might be statistically necessary (and not acknowledging who is being grouped together and why), we can unintentionally misrepresent said groups, minimize inequities and perpetuate invisible and erased experiences of those communities. The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we’re thinking about how we will aggregate the data (or not):\n\nIs important data/nuance lost by combining categories? Ensure there is not a meaningful difference in equity outcomes between groups that would be lost if combined.\nDoes the inclusion of uncombined data negatively impact the interpretation of the data visualization? Having too many groups can make visualizations cluttered and hard to interpret. Additionally, disaggregation leads to smaller group sizes, making comparisons to larger groups more difficult and making statistical significance more challenging. For that reason, it can sometimes be best to combine groups.\nDoes sharing uncombined data compromise confidential information (e.g., Personal Identifiable Information) or information considered private by the community from which it comes (e.g., locations of sacred tribal practices)? This will depend on the audience you are sharing the visualization with (e.g. internal vs public) and what information it contains.\n\nIf you ultimately decide to aggregate / combine groups, be sure to:\n\nBe transparent about why you’re making those decisions (including the trade offs you considered) and documenting those decisions accordingly.\nAcknowledge who is now not included in the data or visualization and explain what groups have been combined and why. Use comments, tooltips, or footnotes that can be easily accessed within the visualization to make it easier for users to find this information.\nThink carefully about how groups are lumped in the “other” category of our analysis or visualization. Sometimes it’s necessary to combine groups into a single “other” category (e.g. to generalize small groups to protect confidentiality or to achieve adequate sample size for your analysis). The Urban Institute’s Do No Harm Data Visualizaation Recommendations include considering alternatives to using the term “other” as a catch-all category, including:\n\n\nAnother ______ (e.g. Another race or Another group)\nAdditional ______ (e.g. Additional races or Additional languages)\nAll other self-descriptions\nPeople identifying as other or multiple races\nIdentity not listed\nIdentity not listed in the survey or dataset\n\n\n\n\nPlain Language - Plain language is writing designed to ensure the audience can understand what you’re trying to communicate as easily, quickly, and comprehensively as possible. This means:\n\nAvoiding convoluted or verbose language\nAvoiding the use of jargon and acronyms\nMaking critical information easy to see and understand\nUsing a conversational rather than legal or bureaucratic tone\n\nFor more guidance on plain language, see:\n\nCenter for Plain Language\nPlainLanguage.gov\nWater Boards Staff may also request plain language review from the Office of Public Participation, although that service is more geared towards the review of fact sheets, brochures, and FAQs rather than data visualizations or other data products.\nHealthy Watershed Partnership Guidance on Communicating Results\n\nAccessible Language - Making language accessible to your audience may also require the translation of products (or product components) into the languages used by your audience. The Water Board’s Linguistic Isolation Tool can be used to help understand the different languages that are used by communities across the state.\nWater Boards Staff may also request translation services from the Office of Public Participation, although that service is more geared towards the translation of documents, rather than data visualizations or other data products.\nInclusive Language - Words matter. It’s important to be mindful of the language and terms we use in general (see Non-Inclusive Terms to Avoid for some examples). The language we use in our products can be a way for us to show respect, empathy, and care for the communities connected to or impacted by the data. The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we need to decide on language to use in our data products:\n\nDo the words seek to fix, blame, shame, or change communities that are most marginalized, OR do they seek to address the oppressive systems that impact these communities?\nAre the words racialized? Do the terms have a racist or colonialist implication? An example is the phrase “at risk.” Close your eyes and say the phrase “at risk.” Does a picture of a certain group or community come to mind? If so, stop and pick another word.\nIs the language people-centered? Or do the phrases objectify communities? Distinguish between calling communities a name and describing what they are experiencing. For example: “people with disabilities,” “a person with asthma,” or “communities of color.”\nHow am I framing the words? What is the context and culture we are creating and perpetuating by using the words? Are the words positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nDo the words dehumanize the communities we serve? Words that take away agency, self-determination, and personal power and do not recognize communities’ inherent strengths and assets should be avoided.\n\n\n\n\n\n“While data may seem cut and dry, people are not”\n- P. Kim Bui, Director of audience innovation at the Arizona Republic and author of Designing data visualisations with empathy\n\nHumans are hardwired for story. Plopping points on a map or throwing together an interactive chart without adding context is not only insufficient, its ineffective!\nTake the time to tell a story with the visualization or product you’re developing. Walk the audience through the the messages you want to convey, and back it up with the data. If you’re unsure what the story of your data is, or how to communicate it effectively, try collecting your thoughts using a COMPASS Message Box:\n\n\n\nThe COMPASS Message Box is a tool to help scientists sort and distill their (often technical or complex) knowledge in a way that will resonate with their chosen audience. Users complete the sections in any order and can then use the distilled information to communicate their science more effectively.\n\n\n\n\n\n\nThis section walks through the different design decisions data practitioners must think through when developing a data visualization or similar data product, and provides recommendations on how to approach the work through a lens of accessibility, equity, and inclusion.\nAs one would expect, some figures, maps and visualizations may be too complex to meet all accessibility requirements. However, data practitioners should do their best to meet recommendations described below and follow best practices to make data products as accessible and inclusive as possible.\n\n\nAlt Text is descriptive text, usually associated with images or figures, that conveys the meaning and context of the item. When creating alt text, remember:\n\nIt should complement but not replicate the caption\nUse language to describe what the audience should take away from looking at the item\nIf there is text in the item, that text should be spelled out in the alt text.\n\nTest to see if your alt text is effective by asking a colleague or project partner to listen to you (or a screen reader - see below) read the text aloud to them BEFORE they look at the item. Then, have them look at the item and ask if there are other details about the item that stand out to them as contributing to the meaning behind the item. If there is, add it in!\nScreen readers are computer software that read the text aloud and include programs like NVDA, JAWS, Acrobat, Word. NVDA is free to download and is the Water Boards standard.\n\n\n\nColor can be a powerful communicator that should be selected purposefully. If we aren’t intentional with the colors we choose we may be unintentionally perpetuating stereotypes, creating hierarchies, or using color palettes that are not accessible to those with color vision differences.\nWhen selecting colors for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Data Visualization Style Guide:\n\nSelect the color palettes according to the message you’re trying to convey:\n\nCategorical palettes are best when you want to distinguish discrete chunks of data that do not have an inherent ordering. It’s best to use a palette that has bold, contrasting, non-gradient colors.\nSequential palettes are best when data range from relatively low or uninteresting values to relatively high or interesting values. It’s best to use a palette that has a relatively subtle shift in hue accompanied by a large shift in brightness and saturation.\n\nAvoid reinforcing stereotypes (e.g. using pink to represent female populations, and blue to represent male populations, using colors associated with skin tones or racial stereotypes).\nWhen thinking about demographic data:\n\nNot using color to differentiate between demographic groups helps avoid unintentionally creating a hierarchy between groups and reinforcing stereotypes, and instead enables color to instead be used to highlight group differences and assist with interpreting the data.\nIf color is determined to be necessary to differentiate between groups, the palette should be chosen carefully. Avoid using incremental color palettes (e.g., light to dark) to represent different demographic groups. Bold and contrasting, non-gradient color palettes are a best practice when displaying demographic data and incorporating color.\nConsider plotting data disaggregated by race or ethnicity in their own separate charts, instead of plotting all disaggregated groups together in a single chart.\n\nPlotting all groups on the same chart encourages comparison using a “deficit-based perspective” that focuses attention on what low-performing / worse off groups are lacking when compared with the high performing / better off groups.\nPlotting a set of small multiples faceted by each racial and ethnic group can better encourage viewers to think about the specific needs and challenges facing each group.\n\n\nDo not use color or shading alone to convey data or information. Instead pair color with patterns, textures, or shapes to convey data or information.\nAvoid using too many colors/shapes/patterns at the same time. Our brains can really only hold (or effectively understand) three to five complex concepts at once. If you need to use six or more colors to represent the data - it might feel cluttered or overwhelming to the viewers. Consider consolidating categories or breaking up the chart into multiple smaller charts as described above.\nA 3:1 contrast ratio is required for figures if a person must perceive a graphic in order to understand the content (e.g., colored lines in graphs, points on maps). However, text that is embedded in a graphic must meet a contrast ratio of 4.5:1 unless the text is incidental or there is no way of presenting the graphic with sufficient contrast without undermining the meaning. Use the Colour Contrast Analyser (CCA) or equivalent software to determine if the figure meets the ratio requirements.\n\nTools to help with color selection may already be integrated into your data visualization tools - if not, some other resources include:\n\nColor Brewer – tool to help create accessible color schemes for maps\nNon-exhaustive list of color vision difference simulators – Color Oracle, Coblis\nNon-exhaustive list of color palette packages for R – Base R Color Palette Guide, RColorBrewer, rcartocolor, viridis\nPaletton – tool to help explore colors and palettes\nWebAIM – color contract checker\n\n\n\n\nAs data practitioners, you decide how your audience will be able to view the data through your visualization or data product. While we might think that data are objective (spoiler - they’re not), how they are presented impacts how they are interpreted, preceived, and ultimately valued and acted upon. How we present the data impacts which stories we tell (or don’t tell) using that data.\nThe graphic below, from Data Feminism, illustrates how the same data can tell different stories depending on their presentation. The graphic on the left tells a story of how the unemployment has fallen substantially since it’s recent peak and might be interpreted as “good news”. The graphic on the right tells a story of how the unemployment rate has been consistently above 8% for a long time period and might be interpreted as “bad news”.\n\n\n\nTwo figures presenting the same data in different ways. Graphic from Data Feminism\n\n\nWhen selecting figure or chart types for your visualization or data product, consider:\n\nHow does the chart you use frame or present the data? What is the context and culture we are creating and perpetuating by presenting the data in that way? Are the charts we’re using positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nIs the chart used appropriate for the data you want to display? Think about your variables (string/categorical and numeric), the volume of data, and the question you are attempting to answer (or the story you’re trying to tell) through the visualization or data product.\nWill the chart type be easily interpreted by the audience? Sometimes data practitioners might be inclined to use a complex chart so we can display more data at once (or just because we think it’s cool!). In moments like these, remember the words of Leonardo da Vinci — “Simplicity is the ultimate sophistication.” Think about what will be the most easy to understand and impactful for your audience.\nHow can I use the textual components of the chart (i.e., title, axis and other labels, legend, notes, caption, tooltips) to communicate simply and with clarity? More is not always more here. If context is provided elsewhere in the visualization or data product, maybe the title, caption, or tooltips are redundant. If there is only one data category that can be described by the title or caption, maybe a legend is not necessary. Think about what is essential and what can be removed or omitted to improve your clarity and communication.\nHow can other chart components (i.e. gridlines, axis lines/ticks, scales, spacing) be used to communicate simply and with clarity? If you have a lot of data on the chart - do gridlines help make sense of things, or cause confusion and clutter? Are there enough empty or negative spaces in the chart so that viewers are given the “visual breathing room” needed to easily absorb the messages you’re hoping the data conveys?\n\nTools to help with chart selection include:\n\nCool Infographics Data Visualization Guides – collection of data visualization chart choosers, reference guides, cheat sheets, websites and infographics about data visualization design best practices.\n\n\n\n\nThe fonts and typography we use determine how easy (or not) it is for the audience to efficiently read and comprehend whatever it is you’re trying to communicate. Typography (the art and technique of arranging type to make written language legible, readable and appealing when displayed) within a chart and visualization us a way to establish a hierarchy among elements within the visualization or data product and guides the reader through the visual product.\nWhen selecting fonts and typography for your visualization or data product, consider:\n\nAre the fonts you’re using accessible? The Water Boards recommends that all fonts (including in tables and figures) must be sans serif (e.g. Arial, Calibri, Helvetica), 12 point or larger. Serif fonts (e.g. Times New Roman) and fonts sized 11 points or lower (even in captions, tables, figures, etc.) should be avoided.\nHow many different font & size combinations are being used at once? Similar to the principles outlined in the color section above, using too many different types of fonts and sizes can make the visualization or data product feel cluttered or overwhelming to the viewers. Consider consolidating your use of fonts by giving different types of information with the same level of importance the same font and size.\nDoes the text have enough space around it? Some users may have cognitive disabilities or visual impairments that results in them having trouble reading lines of text when they’re too closely spaced. By prioritizing types of cognitive accessibility through thoughtful spacing, you contribute to a more inclusive visualization or data product.\n\nFor paragraphs, give the reader at least 1.15 pt spacing within paragraphs and at least 6 pt spacing after (or between) each paragraph.\nFor lists - if the list contains only a few words on each line, 1.15 pt spacing may be sufficient. If each item in the list has multiple lines of text, treat it as you would a paragraph and use 1.15 pt spacing within each list and at least 3 pt spacing after (or between) each item in the list.\nFor items within a chart - make sure the default spacing is sufficient. If items feel cramped, see if you can add spacing among items or reduce the amount of text needed for that particular item (e.g. shorten the title so it’s only a few words on a single line rather that a full sentence)\nSpecific recommendations the World Wide Web Consortium Web Content Accessibility Guidelines (WCAG) 2.1 on Text Spacing include:\n\nLine height (line spacing) to at least 1.5 times the font size;\nSpacing following paragraphs to at least 2 times the font size;\nLetter spacing (tracking) to at least 0.12 times the font size;\nWord spacing to at least 0.16 times the font size.\n\n\n\nTools to help with fonts & typography include:\n\nUrban Institute Data Visualization Style Guide - Chart Typography Section\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidy\ndon’t use highlights as information\ncaptions\n\n\n\n\n\n\n\nJustinmind Blog (2020) User-centered design: a beginner’s guide\nUrban Institute Resources\n\nData Visualization Style Guide\nDo No Harm Guide: Applying Equity Awareness in Data Visualization\n\nWe All Count - Reverse Engineering Data Viz for Equity\nTEDx Talk (2014) Hardwired for story\n\n\n\n\nCalIndian’s California Public Domain Allotment (PDA) Water Rights Map",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html#data-visualization-with-an-equity-lens",
    "href": "assure-analyze/vis.html#data-visualization-with-an-equity-lens",
    "title": "Data Visualization",
    "section": "",
    "text": "When it comes to creating data visualizations with an equity lens, it boils down to making decisions that consider equity and inclusion in the way results are shown/communicated and that promote accessibility of the data, information, and tool as a whole. As you develop your data visualization or equivalent application - be sure to keep the below considerations in mind and make data, communication, and design choices that support the advancement of equity, inclusion, and justice.\n\n\n\n\nConsider who the audience will be for the product you’re developing and make decisions that will prioritize their needs so they are able to easily and efficiently use, engage, interact, and experience the product.\n\n\n\nGraphic illustrating three key components of the user-centered design process: Research, Empathy, and Iteration. Image from Justinmind (2020)\n\n\nResearch - Let’s use an equity lens here. Instead of “researching” your users and audience, try getting to know them by prioritizing relationship building and engagement. Ideally, by the time you’re at the data visualization phase, you have already identified key partners and have been working or engaging with them during other phases of the project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\nEmpathy - According to Dr. Brené Brown: empathy is about feeling WITH people, and requires four qualities:\n\nPerspective Taking, or putting yourself in someone else’s shoes\nStaying out of judgement and listening\nRecognizing emotion in another person that you have maybe felt before\nCommunicating that you can recognize that emotion\n\nIntegrating empathy into our data-intensive work requires us to consider how our audience and/or the communities whose data are being used will percieve or be impacted by our work. It means ensuring we’re developing the product and thinking about the data as more than mere points on a map or visualization, but as representing real humans, environments, or conditions that should be contextualized and considered with care. Some questions to consider include:\n\nWho is vulnerable in this context and how would they want to be counted?\nWhat information would they need to improve their lives?\nWho is undercounted or possibly missing entirely?\nWho was counted? Who did the counting? Why were they asking these people?\nWho benefits or is harmed if you forget the dots are people?\n\nIteration - The key here is knowing and planning for an iterative process from the beginning of your process. Add that feedback, implementation, and testing loop into your plan and allocate appropriate time to each piece to occur. When working with partners or communities using an iterative approach, be sure to consider:\n\nWhen it is appropriate to ask for feedback, and when it might be burdensome\nDifferent ways feedback might be gathered. We might think sending an email with a poll or survey linked is the easiest, but our partner might find it easier to talk through questions over the phone with you. Knowing which method(s) to use comes with time and relationship. When in doubt - ask for what people prefer and do your best to accommodate those requests.\nHow much time is adequate for folks to be able to review what you send them and provide their feedback. When in doubt - plan for a longer feedback period than you think might be needed and confirm timelines with your partners (and adjust them if you can when your partners indicate more time is needed)\n\n\n\n\nIt’s not uncommon for project teams to have gaps in data, even after all of your work and investment into the planning, data preparation and collection steps of the project. Often, this limitation of the data is out of your control - especially when you are using data from external sources. When this happens, it’s important to acknowledge, document, communicate those gaps and who is not adequately represented in your data product in a way that is accessible.\n\n\n\nHow we aggregate or disaggregate the data can impact which groups are “seen” and well represented (or not) in our visualizations. This can also influence who is centered, valued, or prioritized in the narrative of the visualization, and who is excluded.\nCarefully consider how groups are lumped or split - by aggregating many groups in the visualization beyond what might be statistically necessary (and not acknowledging who is being grouped together and why), we can unintentionally misrepresent said groups, minimize inequities and perpetuate invisible and erased experiences of those communities. The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we’re thinking about how we will aggregate the data (or not):\n\nIs important data/nuance lost by combining categories? Ensure there is not a meaningful difference in equity outcomes between groups that would be lost if combined.\nDoes the inclusion of uncombined data negatively impact the interpretation of the data visualization? Having too many groups can make visualizations cluttered and hard to interpret. Additionally, disaggregation leads to smaller group sizes, making comparisons to larger groups more difficult and making statistical significance more challenging. For that reason, it can sometimes be best to combine groups.\nDoes sharing uncombined data compromise confidential information (e.g., Personal Identifiable Information) or information considered private by the community from which it comes (e.g., locations of sacred tribal practices)? This will depend on the audience you are sharing the visualization with (e.g. internal vs public) and what information it contains.\n\nIf you ultimately decide to aggregate / combine groups, be sure to:\n\nBe transparent about why you’re making those decisions (including the trade offs you considered) and documenting those decisions accordingly.\nAcknowledge who is now not included in the data or visualization and explain what groups have been combined and why. Use comments, tooltips, or footnotes that can be easily accessed within the visualization to make it easier for users to find this information.\nThink carefully about how groups are lumped in the “other” category of our analysis or visualization. Sometimes it’s necessary to combine groups into a single “other” category (e.g. to generalize small groups to protect confidentiality or to achieve adequate sample size for your analysis). The Urban Institute’s Do No Harm Data Visualizaation Recommendations include considering alternatives to using the term “other” as a catch-all category, including:\n\n\nAnother ______ (e.g. Another race or Another group)\nAdditional ______ (e.g. Additional races or Additional languages)\nAll other self-descriptions\nPeople identifying as other or multiple races\nIdentity not listed\nIdentity not listed in the survey or dataset\n\n\n\n\nPlain Language - Plain language is writing designed to ensure the audience can understand what you’re trying to communicate as easily, quickly, and comprehensively as possible. This means:\n\nAvoiding convoluted or verbose language\nAvoiding the use of jargon and acronyms\nMaking critical information easy to see and understand\nUsing a conversational rather than legal or bureaucratic tone\n\nFor more guidance on plain language, see:\n\nCenter for Plain Language\nPlainLanguage.gov\nWater Boards Staff may also request plain language review from the Office of Public Participation, although that service is more geared towards the review of fact sheets, brochures, and FAQs rather than data visualizations or other data products.\nHealthy Watershed Partnership Guidance on Communicating Results\n\nAccessible Language - Making language accessible to your audience may also require the translation of products (or product components) into the languages used by your audience. The Water Board’s Linguistic Isolation Tool can be used to help understand the different languages that are used by communities across the state.\nWater Boards Staff may also request translation services from the Office of Public Participation, although that service is more geared towards the translation of documents, rather than data visualizations or other data products.\nInclusive Language - Words matter. It’s important to be mindful of the language and terms we use in general (see Non-Inclusive Terms to Avoid for some examples). The language we use in our products can be a way for us to show respect, empathy, and care for the communities connected to or impacted by the data. The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we need to decide on language to use in our data products:\n\nDo the words seek to fix, blame, shame, or change communities that are most marginalized, OR do they seek to address the oppressive systems that impact these communities?\nAre the words racialized? Do the terms have a racist or colonialist implication? An example is the phrase “at risk.” Close your eyes and say the phrase “at risk.” Does a picture of a certain group or community come to mind? If so, stop and pick another word.\nIs the language people-centered? Or do the phrases objectify communities? Distinguish between calling communities a name and describing what they are experiencing. For example: “people with disabilities,” “a person with asthma,” or “communities of color.”\nHow am I framing the words? What is the context and culture we are creating and perpetuating by using the words? Are the words positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nDo the words dehumanize the communities we serve? Words that take away agency, self-determination, and personal power and do not recognize communities’ inherent strengths and assets should be avoided.\n\n\n\n\n\n“While data may seem cut and dry, people are not”\n- P. Kim Bui, Director of audience innovation at the Arizona Republic and author of Designing data visualisations with empathy\n\nHumans are hardwired for story. Plopping points on a map or throwing together an interactive chart without adding context is not only insufficient, its ineffective!\nTake the time to tell a story with the visualization or product you’re developing. Walk the audience through the the messages you want to convey, and back it up with the data. If you’re unsure what the story of your data is, or how to communicate it effectively, try collecting your thoughts using a COMPASS Message Box:\n\n\n\nThe COMPASS Message Box is a tool to help scientists sort and distill their (often technical or complex) knowledge in a way that will resonate with their chosen audience. Users complete the sections in any order and can then use the distilled information to communicate their science more effectively.\n\n\n\n\n\n\nThis section walks through the different design decisions data practitioners must think through when developing a data visualization or similar data product, and provides recommendations on how to approach the work through a lens of accessibility, equity, and inclusion.\nAs one would expect, some figures, maps and visualizations may be too complex to meet all accessibility requirements. However, data practitioners should do their best to meet recommendations described below and follow best practices to make data products as accessible and inclusive as possible.\n\n\nAlt Text is descriptive text, usually associated with images or figures, that conveys the meaning and context of the item. When creating alt text, remember:\n\nIt should complement but not replicate the caption\nUse language to describe what the audience should take away from looking at the item\nIf there is text in the item, that text should be spelled out in the alt text.\n\nTest to see if your alt text is effective by asking a colleague or project partner to listen to you (or a screen reader - see below) read the text aloud to them BEFORE they look at the item. Then, have them look at the item and ask if there are other details about the item that stand out to them as contributing to the meaning behind the item. If there is, add it in!\nScreen readers are computer software that read the text aloud and include programs like NVDA, JAWS, Acrobat, Word. NVDA is free to download and is the Water Boards standard.\n\n\n\nColor can be a powerful communicator that should be selected purposefully. If we aren’t intentional with the colors we choose we may be unintentionally perpetuating stereotypes, creating hierarchies, or using color palettes that are not accessible to those with color vision differences.\nWhen selecting colors for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Data Visualization Style Guide:\n\nSelect the color palettes according to the message you’re trying to convey:\n\nCategorical palettes are best when you want to distinguish discrete chunks of data that do not have an inherent ordering. It’s best to use a palette that has bold, contrasting, non-gradient colors.\nSequential palettes are best when data range from relatively low or uninteresting values to relatively high or interesting values. It’s best to use a palette that has a relatively subtle shift in hue accompanied by a large shift in brightness and saturation.\n\nAvoid reinforcing stereotypes (e.g. using pink to represent female populations, and blue to represent male populations, using colors associated with skin tones or racial stereotypes).\nWhen thinking about demographic data:\n\nNot using color to differentiate between demographic groups helps avoid unintentionally creating a hierarchy between groups and reinforcing stereotypes, and instead enables color to instead be used to highlight group differences and assist with interpreting the data.\nIf color is determined to be necessary to differentiate between groups, the palette should be chosen carefully. Avoid using incremental color palettes (e.g., light to dark) to represent different demographic groups. Bold and contrasting, non-gradient color palettes are a best practice when displaying demographic data and incorporating color.\nConsider plotting data disaggregated by race or ethnicity in their own separate charts, instead of plotting all disaggregated groups together in a single chart.\n\nPlotting all groups on the same chart encourages comparison using a “deficit-based perspective” that focuses attention on what low-performing / worse off groups are lacking when compared with the high performing / better off groups.\nPlotting a set of small multiples faceted by each racial and ethnic group can better encourage viewers to think about the specific needs and challenges facing each group.\n\n\nDo not use color or shading alone to convey data or information. Instead pair color with patterns, textures, or shapes to convey data or information.\nAvoid using too many colors/shapes/patterns at the same time. Our brains can really only hold (or effectively understand) three to five complex concepts at once. If you need to use six or more colors to represent the data - it might feel cluttered or overwhelming to the viewers. Consider consolidating categories or breaking up the chart into multiple smaller charts as described above.\nA 3:1 contrast ratio is required for figures if a person must perceive a graphic in order to understand the content (e.g., colored lines in graphs, points on maps). However, text that is embedded in a graphic must meet a contrast ratio of 4.5:1 unless the text is incidental or there is no way of presenting the graphic with sufficient contrast without undermining the meaning. Use the Colour Contrast Analyser (CCA) or equivalent software to determine if the figure meets the ratio requirements.\n\nTools to help with color selection may already be integrated into your data visualization tools - if not, some other resources include:\n\nColor Brewer – tool to help create accessible color schemes for maps\nNon-exhaustive list of color vision difference simulators – Color Oracle, Coblis\nNon-exhaustive list of color palette packages for R – Base R Color Palette Guide, RColorBrewer, rcartocolor, viridis\nPaletton – tool to help explore colors and palettes\nWebAIM – color contract checker\n\n\n\n\nAs data practitioners, you decide how your audience will be able to view the data through your visualization or data product. While we might think that data are objective (spoiler - they’re not), how they are presented impacts how they are interpreted, preceived, and ultimately valued and acted upon. How we present the data impacts which stories we tell (or don’t tell) using that data.\nThe graphic below, from Data Feminism, illustrates how the same data can tell different stories depending on their presentation. The graphic on the left tells a story of how the unemployment has fallen substantially since it’s recent peak and might be interpreted as “good news”. The graphic on the right tells a story of how the unemployment rate has been consistently above 8% for a long time period and might be interpreted as “bad news”.\n\n\n\nTwo figures presenting the same data in different ways. Graphic from Data Feminism\n\n\nWhen selecting figure or chart types for your visualization or data product, consider:\n\nHow does the chart you use frame or present the data? What is the context and culture we are creating and perpetuating by presenting the data in that way? Are the charts we’re using positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nIs the chart used appropriate for the data you want to display? Think about your variables (string/categorical and numeric), the volume of data, and the question you are attempting to answer (or the story you’re trying to tell) through the visualization or data product.\nWill the chart type be easily interpreted by the audience? Sometimes data practitioners might be inclined to use a complex chart so we can display more data at once (or just because we think it’s cool!). In moments like these, remember the words of Leonardo da Vinci — “Simplicity is the ultimate sophistication.” Think about what will be the most easy to understand and impactful for your audience.\nHow can I use the textual components of the chart (i.e., title, axis and other labels, legend, notes, caption, tooltips) to communicate simply and with clarity? More is not always more here. If context is provided elsewhere in the visualization or data product, maybe the title, caption, or tooltips are redundant. If there is only one data category that can be described by the title or caption, maybe a legend is not necessary. Think about what is essential and what can be removed or omitted to improve your clarity and communication.\nHow can other chart components (i.e. gridlines, axis lines/ticks, scales, spacing) be used to communicate simply and with clarity? If you have a lot of data on the chart - do gridlines help make sense of things, or cause confusion and clutter? Are there enough empty or negative spaces in the chart so that viewers are given the “visual breathing room” needed to easily absorb the messages you’re hoping the data conveys?\n\nTools to help with chart selection include:\n\nCool Infographics Data Visualization Guides – collection of data visualization chart choosers, reference guides, cheat sheets, websites and infographics about data visualization design best practices.\n\n\n\n\nThe fonts and typography we use determine how easy (or not) it is for the audience to efficiently read and comprehend whatever it is you’re trying to communicate. Typography (the art and technique of arranging type to make written language legible, readable and appealing when displayed) within a chart and visualization us a way to establish a hierarchy among elements within the visualization or data product and guides the reader through the visual product.\nWhen selecting fonts and typography for your visualization or data product, consider:\n\nAre the fonts you’re using accessible? The Water Boards recommends that all fonts (including in tables and figures) must be sans serif (e.g. Arial, Calibri, Helvetica), 12 point or larger. Serif fonts (e.g. Times New Roman) and fonts sized 11 points or lower (even in captions, tables, figures, etc.) should be avoided.\nHow many different font & size combinations are being used at once? Similar to the principles outlined in the color section above, using too many different types of fonts and sizes can make the visualization or data product feel cluttered or overwhelming to the viewers. Consider consolidating your use of fonts by giving different types of information with the same level of importance the same font and size.\nDoes the text have enough space around it? Some users may have cognitive disabilities or visual impairments that results in them having trouble reading lines of text when they’re too closely spaced. By prioritizing types of cognitive accessibility through thoughtful spacing, you contribute to a more inclusive visualization or data product.\n\nFor paragraphs, give the reader at least 1.15 pt spacing within paragraphs and at least 6 pt spacing after (or between) each paragraph.\nFor lists - if the list contains only a few words on each line, 1.15 pt spacing may be sufficient. If each item in the list has multiple lines of text, treat it as you would a paragraph and use 1.15 pt spacing within each list and at least 3 pt spacing after (or between) each item in the list.\nFor items within a chart - make sure the default spacing is sufficient. If items feel cramped, see if you can add spacing among items or reduce the amount of text needed for that particular item (e.g. shorten the title so it’s only a few words on a single line rather that a full sentence)\nSpecific recommendations the World Wide Web Consortium Web Content Accessibility Guidelines (WCAG) 2.1 on Text Spacing include:\n\nLine height (line spacing) to at least 1.5 times the font size;\nSpacing following paragraphs to at least 2 times the font size;\nLetter spacing (tracking) to at least 0.12 times the font size;\nWord spacing to at least 0.16 times the font size.\n\n\n\nTools to help with fonts & typography include:\n\nUrban Institute Data Visualization Style Guide - Chart Typography Section\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidy\ndon’t use highlights as information\ncaptions",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html#resources",
    "href": "assure-analyze/vis.html#resources",
    "title": "Data Visualization",
    "section": "",
    "text": "Justinmind Blog (2020) User-centered design: a beginner’s guide\nUrban Institute Resources\n\nData Visualization Style Guide\nDo No Harm Guide: Applying Equity Awareness in Data Visualization\n\nWe All Count - Reverse Engineering Data Viz for Equity\nTEDx Talk (2014) Hardwired for story\n\n\n\n\nCalIndian’s California Public Domain Allotment (PDA) Water Rights Map",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html",
    "href": "assure-analyze/analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "By now, you’ve completed all of your planning and prep and are ready to get to the analysis part of the project. Congratulations!\n\n\n\n\n\n\nThe whole point of data analysis is to turn data into information, knowledge, and wisdom that we can use to make decisions and take action.\n\n\n\nIdeally, the data-informed decisions we make and the corresponding actions we take will help us advance and operationalize equity. Using data to inform decisions can be challenging, and is even more so when applying an equity lens to our data work.\n\n\n\n\n\nIllustrations of two perspectives on Data-Information-Knowledge-Wisdom (DIKW) In practice. Graphic adapted from Flood et al. (2016) and DataCamp (2023).\n\n\nAs the graphic above illustrates, the transition and transformation from data to wisdom requires adding context, meaning, insight to the original data, while gaining experience and understanding as we progress. In the above graphic we define data, information, knowledge, and wisdom as:\n\nData: individual measurements, facts, figures, and signals, without context.\nInformation: organized, structured, and contextualized data that can be used to identify concepts of “who”, “what”, “where”, and “when”.\nKnowledge: the result of analyzing, synthesizing, and interpreting information to identify patterns, trends, and relationships, which can be used to understand the “how” and “why” of what we’re observing.\nWisdom: the ability to integrate what we understand from the data with other context, reflection, and underlying knowledge of the broader question or topic at hand with the express purpose of making data and equity-informed decisions and to take effective and impactful action.\n\n\n\nTurning data into information in the context of racial equity involves navigating complex ethical considerations. The process requires an understanding of the potential impact on Black, Indigenous, and other People of Color (BIPOC) communities and the responsibility to mitigate perpetuating or reinforcing biases. Upholding ethical standards requires a commitment to maintaining privacy, accessibility, and fostering transparency throughout the data transformation process. Additionally, acknowledging the limitations of the data and being transparent about potential biases is essential for maintaining the integrity of the data and information generated and shared. The transformation of racial equity data into meaningful information requires a thoughtful and intentional approach which we will highlight in the next sections.\nFor example, many programs will rely on demographic and socioeconomic data, like those collected from the U.S. Census and the American Community Survey (ACS). Because the ACS is based on a sample, rather than all housing units and people, ACS estimates have a degree of uncertainty associated with them, called sampling error. In general, the larger the sample, the smaller the level of sampling error. To help users understand the impact of sampling error on data reliability, the Census Bureau provides a “margin of error” for each published ACS estimate. The margin of error, combined with the ACS estimate, gives users a range of values within which the actual “real-world” value is likely to fall. Also see: Using American Community Survey (ACS) Data: What All Data Users Need to Know Handbook.\nIt is important to acknowledge this uncertainty up front to be transparent with your audience about the data and conclusions you are drawing. \nExamples of how others have done this work include:\n\nCalif. Dept. of Finance Demographic Research Unit’s California Hard-to-Count Index Interactive Map, which measures the potential inaccuracies associated with relying on census data to enumerate demographic and socioeconomic characteristics in California\nOffice of Environmental Health and Hazard Assessment CalEnviroScreen 4.0 Race and Equity Analysis\n\n\n\nThere are five main data analysis method types, and each has a different process, purpose, and interpretation (see table below). As you embark on your analysis, it’s important to:\n\nunderstand which method(s) you can use given your data\nunderstand the limitations of your data and those method(s)\nselect the method(s) that are most appropriate for your project questions/objectives\n\n\n\n\n\n\n\n\n\nAnalysis Method\nPurpose\nLimitation\n\n\n\n\nDescriptive Analytics\nSummarize and describe data in clear and simple way\nDescriptive analyses cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified.\n\n\nInferential Analytics\nMake (infer) conclusions or generalizations about populations based on sample data\nConclusions can only be made on samples / data that are analysed, and the appropriate use and interpretation of the statistical method used depends on whether the data meets the assumptions of the method. If method assumptions are not met - results and conclusions are meaningless.\n\n\nDiagnostic Analytics\nTell us something about the “why” behind the results, patterns, or trends that are identified\nDepending on the data you have and the analysis you are able to use, your results may not be able to identify the root cause in the context of the lived experiences of the communities the data you’re using are meant to represent.\n\n\nPredictive Analytics\nHelp identify what might be likely to happen in the future given the previous trends or patterns found in the data\nYour ability to predict what may happen is highly dependent on (and limited by) the quantity, quality, and representativeness of the data that you use. If there are gaps in that data you feed your analytical methods, there will be similar gaps in your results.\n\n\nPrescriptive Analytics\nRecommend actions or decisions.\nThe potential analytical methods are complex and require a large amount of quality and relevant data and computing power to be implemented appropriately. One should also consider the implications of the potential actions and decisions being considered using an equity lens. Striking the balance between data-driven insights and equity considerations is essential for advancing equity outcomes.\n\n\nCausal Analytics\nUnderstand the cause and effect relationships between variables of interest.\nAll causal analytical tools require strong assumptions and can never fully capture all of the context of the relationships in questions (i.e. extraneous variables that cannot be measured or assessed). If these methods are used, be sure to ground-truth the results with the communities the data you’re using are meant to represent.\n\n\n\n\n\n\nAs with any analysis, we need to understand the limits of our data and the methods we use so we interpret the results we find appropriately. Below is an overview of common data fallacies to be aware of and avoid as you’re interpreting your results and making conclusions1:\n\n\n\nData Fallacy\nDescription\nAdditional Equity Considerations\n\n\n\n\nCherry Picking\n\nSelecting results that fit your hypothesis or claim, and excluding those that don’t.\nThis doesn’t mean you should always keep all data you have access to (see Data Processing).\nIt’s important to be aware of data or results you are excluding and to be honest and transparent about why you’re excluding it.\n\n\nCobra Effect\n(aka Perverse Incentive)\n\nSetting an incentive that accidentally produces the opposite result to the one intended.\nThis is all about unintended consequences of our decisions and actions.\nWhen you’re developing your project objectives, be sure to take the Planning step seriously and take the time needed to answer as many of the questions provided as possible.\n\n\nDanger of Summary Metrics\n\nOnly looking at summary metrics and missing big differences in the raw data.\nSummary metrics or descriptive analytics can (and should!) be used to help us understand out data. However it’s important to remember that they cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified. Be sure to use summary metrics in concert with context and other analyses so you can see your data holistically.\n\n\nData Dredging\n\nRepeatedly testing new hypotheses against the same set of data, failing to acknowledge that most correlations will be the result of chance.\nIt can be exciting to uncover meaningful and logical correlations when conducting your analyses.\nSimilar to cherry picking, it’s important to be aware of your analytical methods (and their limitations), and to be honest and transparent about your project objectives before you begin your analysis (see Plan and Prepare) as well as why you’re accepting or rejecting the results you get after analyses are conducted appropriately.\n\n\nFalse Causality\n\nFalsely assuming when two events appear related that one must have caused the other.\nThe classic: correlation does not imply causation. Adding context for which you don’t have data can help here. Discussing preliminary trends/results with partners and communities can help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\n\n\nGambler’s Fallacy\n(aka Monte Carlo Fallacy)\n\nMistakenly believing that because something has happened more frequently than usual, it’s now less likely to happen in future (and vice versa).\nTo combat this fallacy it will be important to understand the context and “why” behind the results you are seeing. There are a number methods designed to help identify the the “why” (and there may be more than one reason!).\nSee the Evaluation section for guidance and remember that you don’t need to wait until the project is complete to benefit from using these evaluation tools! In fact, it may be helpful to use these tools at multiple points during your project or process.\n\n\nGerrymandering\n\nManipulating the geographical boundaries used to group data in order to change the result.\nThis is closely related to how you use geographical boundaries to aggregate or group your data. During your Plan and Prepare phase, think through what geographic units are most appropriate for your project needs. Know that the data you have access to might be in one geographic unit (e.g. census tract, hydrologic unit code (HUC)) but your questions are related to a different unit (e.g. county, region, statewide) - so you may need to do some analyses to standardize units. Doing so has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nHawthorne Effect\n(aka Observer Effect)\n\nThe act of monitoring someone can affect their behavior, leading to spurious findings.\nThis may not be directly relevant in the usual data work you do. However, it is particularly important when delivering surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nMcNamara Fallacy\n\nRelying solely on metrics in complex situations and losing sight of the bigger picture.\nIt can be easy for some of us to get lost in the weeds of data analysis. What’s key here is to take a step back from time to time and continue to connect our analyses or product development back to the broader and contextualized project and equity goals you set for the project during your Plan and Prepare phase.\nWorking with and/or discussing your analytical process and/or preliminary trends/results with partners and communities can also help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project, and pull you out of the data weeds and back into context-informed data work.\n\n\nOverfitting\n\nCreating a model that’s overly tailored to the data you have and not representative of the general trend.\nUsing data to describe the environment and inform our decisions is complex, and is even more so when applying an equity lens to our data work. What’s important here is to keep this in mind as you develop, test, and interpret models for your analyses.\n\n\nPublication Bias\n\nInteresting research findings are more likely to be published, distorting our impression of reality.\nIt’s important to remember that academic systems - like our government systems and including publishing institutions - have racist origins and biases that have excluded the science, research, and ways of knowing that have come from of Black, Indigenous, and other People of Color (BIPOC) communities. This is why taking the time to develop relationships and partnerships with the communities our decisions and actions may impact is crucial.\nIncluding partners with the expertise that can only come from lived experiences in our projects from the beginnig, and giving that expertise the same weight as academic or research expertise can broaden our perspectives and help prevent publication (and other biases) from negatively impacting our project, process and advancement of equity.\nSee the Planning section on collecting expert input for more guidance.\n\n\nRegression Towards the Mean\n\nWhen something happens that’s unusually good or bad, it will revert back towards the average over time.\nA key component of this fallacy is that random chance influences the outcome. When looking at our data through an equity lens, we need to remember that racism and injustice are central to our collective history and can be traced back to before the founding of our country. We live and work in institutions and systems that have inherited those unjust decisions and processes. We know that, as government representatives, if we’re not clear and intentional about advancing racial equity in the work we do, it won’t happen and we will continue to perpetuate racial inequity. In other words, because of the unjust systems in which we work, we cannot depend on this fallacy and wait for the unusually “bad” or unjust results/trends to correct themselves or revert back to more equitable trends. Because government agencies created and perpetuated environmental racism, it is our responsibility to proactively advance racial equity and justice in all the work we do.\nIf our collective history and it’s deep connection to racism and injustice are not familiar to you - we recommend taking the Advancing Racial Equity training series offered by the Water Boards Training Academy and reviewing the GARE Framework: Normalize, Organize, and Operationalize. More details on both of these actions and others can be found on the Getting Started page.\n\n\nSampling Bias\n\nDrawing conclusions from a set of data that isn’t representative of the population you’re trying to understand.\nDuring your Plan and Prepare you will think through the type of data that’s needed to adequitely represent the populations related to your project objectives. If the data available is not adequately representative, then you may need to revise the types of questions you have of the data (and analytical methods). Or, it might be worth considering collecting the data yourself or with partners.\nNote that this principle and others can also be applied to surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nSimpson’s Paradox\n\nWhen a trend appears in different subsets of data but disappears or reverses when the groups are combined.\nThis is critical when trying to understand and quantify how environmental outcomes might impact different groups. During your Plan and Prepare phase, think through the different ways you might aggregate or disaggregate your data and consider what is appropriate for the questions you have.\nWithout disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nEach decision has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nSurvivorship Bias\n\nDrawing conclusions from an incomplete set of data, because that data has ‘survived’ some selection criteria.\nAs soon as we get our hands on a dataset it can be tempting to dive into analysis without taking the time to consider that data in the context of our project goals and objectives. Sometimes the data we need to understand every aspect of ideal our project objectives simply does not exist or is not accessible.\nIt’s important to take the time during the Plan and Prepare phase to understand what data are needed to achieve the objectives of your project, what data are actually available, and to understand what all of that means for your Collect and Process phase, and how that might impact your interpretation.",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html#data-analysis-with-an-equity-lens",
    "href": "assure-analyze/analysis.html#data-analysis-with-an-equity-lens",
    "title": "Data Analysis",
    "section": "",
    "text": "Turning data into information in the context of racial equity involves navigating complex ethical considerations. The process requires an understanding of the potential impact on Black, Indigenous, and other People of Color (BIPOC) communities and the responsibility to mitigate perpetuating or reinforcing biases. Upholding ethical standards requires a commitment to maintaining privacy, accessibility, and fostering transparency throughout the data transformation process. Additionally, acknowledging the limitations of the data and being transparent about potential biases is essential for maintaining the integrity of the data and information generated and shared. The transformation of racial equity data into meaningful information requires a thoughtful and intentional approach which we will highlight in the next sections.\nFor example, many programs will rely on demographic and socioeconomic data, like those collected from the U.S. Census and the American Community Survey (ACS). Because the ACS is based on a sample, rather than all housing units and people, ACS estimates have a degree of uncertainty associated with them, called sampling error. In general, the larger the sample, the smaller the level of sampling error. To help users understand the impact of sampling error on data reliability, the Census Bureau provides a “margin of error” for each published ACS estimate. The margin of error, combined with the ACS estimate, gives users a range of values within which the actual “real-world” value is likely to fall. Also see: Using American Community Survey (ACS) Data: What All Data Users Need to Know Handbook.\nIt is important to acknowledge this uncertainty up front to be transparent with your audience about the data and conclusions you are drawing. \nExamples of how others have done this work include:\n\nCalif. Dept. of Finance Demographic Research Unit’s California Hard-to-Count Index Interactive Map, which measures the potential inaccuracies associated with relying on census data to enumerate demographic and socioeconomic characteristics in California\nOffice of Environmental Health and Hazard Assessment CalEnviroScreen 4.0 Race and Equity Analysis\n\n\n\nThere are five main data analysis method types, and each has a different process, purpose, and interpretation (see table below). As you embark on your analysis, it’s important to:\n\nunderstand which method(s) you can use given your data\nunderstand the limitations of your data and those method(s)\nselect the method(s) that are most appropriate for your project questions/objectives\n\n\n\n\n\n\n\n\n\nAnalysis Method\nPurpose\nLimitation\n\n\n\n\nDescriptive Analytics\nSummarize and describe data in clear and simple way\nDescriptive analyses cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified.\n\n\nInferential Analytics\nMake (infer) conclusions or generalizations about populations based on sample data\nConclusions can only be made on samples / data that are analysed, and the appropriate use and interpretation of the statistical method used depends on whether the data meets the assumptions of the method. If method assumptions are not met - results and conclusions are meaningless.\n\n\nDiagnostic Analytics\nTell us something about the “why” behind the results, patterns, or trends that are identified\nDepending on the data you have and the analysis you are able to use, your results may not be able to identify the root cause in the context of the lived experiences of the communities the data you’re using are meant to represent.\n\n\nPredictive Analytics\nHelp identify what might be likely to happen in the future given the previous trends or patterns found in the data\nYour ability to predict what may happen is highly dependent on (and limited by) the quantity, quality, and representativeness of the data that you use. If there are gaps in that data you feed your analytical methods, there will be similar gaps in your results.\n\n\nPrescriptive Analytics\nRecommend actions or decisions.\nThe potential analytical methods are complex and require a large amount of quality and relevant data and computing power to be implemented appropriately. One should also consider the implications of the potential actions and decisions being considered using an equity lens. Striking the balance between data-driven insights and equity considerations is essential for advancing equity outcomes.\n\n\nCausal Analytics\nUnderstand the cause and effect relationships between variables of interest.\nAll causal analytical tools require strong assumptions and can never fully capture all of the context of the relationships in questions (i.e. extraneous variables that cannot be measured or assessed). If these methods are used, be sure to ground-truth the results with the communities the data you’re using are meant to represent.\n\n\n\n\n\n\nAs with any analysis, we need to understand the limits of our data and the methods we use so we interpret the results we find appropriately. Below is an overview of common data fallacies to be aware of and avoid as you’re interpreting your results and making conclusions1:\n\n\n\nData Fallacy\nDescription\nAdditional Equity Considerations\n\n\n\n\nCherry Picking\n\nSelecting results that fit your hypothesis or claim, and excluding those that don’t.\nThis doesn’t mean you should always keep all data you have access to (see Data Processing).\nIt’s important to be aware of data or results you are excluding and to be honest and transparent about why you’re excluding it.\n\n\nCobra Effect\n(aka Perverse Incentive)\n\nSetting an incentive that accidentally produces the opposite result to the one intended.\nThis is all about unintended consequences of our decisions and actions.\nWhen you’re developing your project objectives, be sure to take the Planning step seriously and take the time needed to answer as many of the questions provided as possible.\n\n\nDanger of Summary Metrics\n\nOnly looking at summary metrics and missing big differences in the raw data.\nSummary metrics or descriptive analytics can (and should!) be used to help us understand out data. However it’s important to remember that they cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified. Be sure to use summary metrics in concert with context and other analyses so you can see your data holistically.\n\n\nData Dredging\n\nRepeatedly testing new hypotheses against the same set of data, failing to acknowledge that most correlations will be the result of chance.\nIt can be exciting to uncover meaningful and logical correlations when conducting your analyses.\nSimilar to cherry picking, it’s important to be aware of your analytical methods (and their limitations), and to be honest and transparent about your project objectives before you begin your analysis (see Plan and Prepare) as well as why you’re accepting or rejecting the results you get after analyses are conducted appropriately.\n\n\nFalse Causality\n\nFalsely assuming when two events appear related that one must have caused the other.\nThe classic: correlation does not imply causation. Adding context for which you don’t have data can help here. Discussing preliminary trends/results with partners and communities can help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\n\n\nGambler’s Fallacy\n(aka Monte Carlo Fallacy)\n\nMistakenly believing that because something has happened more frequently than usual, it’s now less likely to happen in future (and vice versa).\nTo combat this fallacy it will be important to understand the context and “why” behind the results you are seeing. There are a number methods designed to help identify the the “why” (and there may be more than one reason!).\nSee the Evaluation section for guidance and remember that you don’t need to wait until the project is complete to benefit from using these evaluation tools! In fact, it may be helpful to use these tools at multiple points during your project or process.\n\n\nGerrymandering\n\nManipulating the geographical boundaries used to group data in order to change the result.\nThis is closely related to how you use geographical boundaries to aggregate or group your data. During your Plan and Prepare phase, think through what geographic units are most appropriate for your project needs. Know that the data you have access to might be in one geographic unit (e.g. census tract, hydrologic unit code (HUC)) but your questions are related to a different unit (e.g. county, region, statewide) - so you may need to do some analyses to standardize units. Doing so has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nHawthorne Effect\n(aka Observer Effect)\n\nThe act of monitoring someone can affect their behavior, leading to spurious findings.\nThis may not be directly relevant in the usual data work you do. However, it is particularly important when delivering surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nMcNamara Fallacy\n\nRelying solely on metrics in complex situations and losing sight of the bigger picture.\nIt can be easy for some of us to get lost in the weeds of data analysis. What’s key here is to take a step back from time to time and continue to connect our analyses or product development back to the broader and contextualized project and equity goals you set for the project during your Plan and Prepare phase.\nWorking with and/or discussing your analytical process and/or preliminary trends/results with partners and communities can also help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project, and pull you out of the data weeds and back into context-informed data work.\n\n\nOverfitting\n\nCreating a model that’s overly tailored to the data you have and not representative of the general trend.\nUsing data to describe the environment and inform our decisions is complex, and is even more so when applying an equity lens to our data work. What’s important here is to keep this in mind as you develop, test, and interpret models for your analyses.\n\n\nPublication Bias\n\nInteresting research findings are more likely to be published, distorting our impression of reality.\nIt’s important to remember that academic systems - like our government systems and including publishing institutions - have racist origins and biases that have excluded the science, research, and ways of knowing that have come from of Black, Indigenous, and other People of Color (BIPOC) communities. This is why taking the time to develop relationships and partnerships with the communities our decisions and actions may impact is crucial.\nIncluding partners with the expertise that can only come from lived experiences in our projects from the beginnig, and giving that expertise the same weight as academic or research expertise can broaden our perspectives and help prevent publication (and other biases) from negatively impacting our project, process and advancement of equity.\nSee the Planning section on collecting expert input for more guidance.\n\n\nRegression Towards the Mean\n\nWhen something happens that’s unusually good or bad, it will revert back towards the average over time.\nA key component of this fallacy is that random chance influences the outcome. When looking at our data through an equity lens, we need to remember that racism and injustice are central to our collective history and can be traced back to before the founding of our country. We live and work in institutions and systems that have inherited those unjust decisions and processes. We know that, as government representatives, if we’re not clear and intentional about advancing racial equity in the work we do, it won’t happen and we will continue to perpetuate racial inequity. In other words, because of the unjust systems in which we work, we cannot depend on this fallacy and wait for the unusually “bad” or unjust results/trends to correct themselves or revert back to more equitable trends. Because government agencies created and perpetuated environmental racism, it is our responsibility to proactively advance racial equity and justice in all the work we do.\nIf our collective history and it’s deep connection to racism and injustice are not familiar to you - we recommend taking the Advancing Racial Equity training series offered by the Water Boards Training Academy and reviewing the GARE Framework: Normalize, Organize, and Operationalize. More details on both of these actions and others can be found on the Getting Started page.\n\n\nSampling Bias\n\nDrawing conclusions from a set of data that isn’t representative of the population you’re trying to understand.\nDuring your Plan and Prepare you will think through the type of data that’s needed to adequitely represent the populations related to your project objectives. If the data available is not adequately representative, then you may need to revise the types of questions you have of the data (and analytical methods). Or, it might be worth considering collecting the data yourself or with partners.\nNote that this principle and others can also be applied to surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nSimpson’s Paradox\n\nWhen a trend appears in different subsets of data but disappears or reverses when the groups are combined.\nThis is critical when trying to understand and quantify how environmental outcomes might impact different groups. During your Plan and Prepare phase, think through the different ways you might aggregate or disaggregate your data and consider what is appropriate for the questions you have.\nWithout disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nEach decision has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nSurvivorship Bias\n\nDrawing conclusions from an incomplete set of data, because that data has ‘survived’ some selection criteria.\nAs soon as we get our hands on a dataset it can be tempting to dive into analysis without taking the time to consider that data in the context of our project goals and objectives. Sometimes the data we need to understand every aspect of ideal our project objectives simply does not exist or is not accessible.\nIt’s important to take the time during the Plan and Prepare phase to understand what data are needed to achieve the objectives of your project, what data are actually available, and to understand what all of that means for your Collect and Process phase, and how that might impact your interpretation.",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html#footnotes",
    "href": "assure-analyze/analysis.html#footnotes",
    "title": "Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content in this section has been informed by or pulled directly from: Geckoboard’s Data Fallacies↩︎",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "plan-prep/plan.html",
    "href": "plan-prep/plan.html",
    "title": "Planning",
    "section": "",
    "text": "When planning for your data project with an equity lens, it’s best to conduct a thorough equity assessment, or scoping process. Equity assessments1 are systematic examinations of available data and expert input on how various groups - especially those facing inequity or disparities - are or likely will be affected by a policy, program, or process. They aim to minimize unintended adverse outcomes and maximize opportunities and positive outcomes.\nThe steps to conducting an equity assessment are described below.\n\nStep 0: Project Scoping\nStep 1: Describe the selected program, policy, or process, and populations affected by it\nStep 2. Consider historical, societal, and policy context and drivers of disparities\nStep 3. Collect expert input, including from affected community members\nStep 4. Identify information sources and gaps\nStep 5. Analyze program/policy effects - potential or current - on people and communities\nStep 6. Plan for action and accountability\n\nAlthough these steps are numbered for clarity, teams should synthesize information from all steps rather than completing them in isolation or one at a time.\n\n\nTime frame and level of effort:\n\nPlan a detailed schedule that accounts for staff availability, budget, technical assistance needs, data availability, and the need to make decisions in a particular time frame.\nIdentify and document risks to the timeline, such as staff availability, or threats to the comprehensiveness of the assessment, such as lack of access to experts. Consider potential ways to mitigate risks.\n\nProject team:\n\nDefine roles for team members and assign responsibilities. Plan to share information at key milestones to synthesize information from different assessment steps.\nConsider how to meaningfully involve experts, including people with lived experience with relevant programs and topics; people in communities affected by the program, policy, or process; staff who work with program participants/beneficiaries; or representatives of other offices. Experts can contribute to the assessment process in several ways, such as suggesting data sources, providing multiple perspectives to inform and enhance the analysis, and developing recommendations for action.\n\n\n\n\nDescribe the focus of the assessment to provide a foundation for all members of the assessment team and external partners.\n\nWhat is the purpose of the selected program, policy, or process, and what are its goals?\nWhat are the known successes or challenges in meeting those goals?\nWhat types of actions or policy levers are involved in the selected program, policy, or process (such as grants, contracts, waivers, guidance to partners, technical assistance, or other actions)?\nWhich of these actions will be included in the assessment?\nWhat general descriptive or performance data can the organization use to describe the program, policy, or process (such as number served, total funds distributed, uptake estimates, or other key outcomes)?\nAre there existing quantifiable performance targets relevant to the focus of the assessment? Provide a brief summary.\n\nIdentify and describe populations of interest\n\nWhat populations are participating in the program, policy, or process, including program participants/beneficiaries? Consider which characteristics are relevant and of interest, such as race, ethnicity, gender identity, sexual orientation, disability status, income, religion, and rural geography.\nWhat populations are currently left out, or not participating or benefiting at desired rates or at the same rates as others? What are other disparities related to the selected program, policy, or process that are known at the outset of the assessment?\nWhat are the information sources for those inequities or disparities?  What is the comparison population or reference point for observed disparities? Reference point options include the total population in an area, the national population, the largest group, or a benchmark chosen through a planning process. Whenever possible, try to think critically about this population rather than simply defaulting to comparison populations used in the past.\nHow might population groups’ identifying characteristics overlap in ways that expose them to relatively greater inequities (known as intersectionality)? What implications does this overlap have for the impacts of the program, policy or process? For example, immigrants who are also LGBTQIA+ might face multiple barriers in accessing a particular program.\n\n\n\n\nDescribe the context for observed disparities and the program or policy itself.\n\nWhat is the social and cultural history of the populations listed in Step 1 and how does this history shape their current conditions? How does this context play a role in how these populations might perceive, access, or otherwise interact with the program or policy?\nWhat structural or social drivers of disparities might explain observed disparities? Structural drivers of disparities are governing processes and economic and social policies that distribute power and resources in unfair ways, such as an inequitable distribution of emergency funds to certain communities. Social drivers of disparities are differences in the conditions in which people are born, grow, live, work, and age, such as poverty, employment, housing, environment quality, transportation, food security, and community safety. Differences in these social conditions drive disparities. Although these conditions are also known as social determinants of health, this tool uses a broader term to encompass multiple outcomes, including both health outcomes and other outcomes (e.g., economic outcomes). Thinking through these drivers of disparities is important for placing focus on systems and institutions that need to be changed, and it helps to avoid blaming groups of people for poor outcomes.\nWhat is known about whether structural, systemic, or institutional racism or structural barriers affect the implementation and outcomes of previous programs or policies? Systemic or institutional racism refers to policies and practices that create or sustain disparate outcomes for persons of different races. An example is redlining, where financial services and other housing-related opportunities were restricted for individuals largely based on their race/ethnicity and originating neighborhoods (see this 2021 Memorandum for the Secretary of Housing and Urban Development regarding Redressing Our Nation’s and the Federal Government’s History of Discriminatory Housing Practices and Policies)\n\n\n\n\nExperts can include former or current program participants/beneficiaries, members of communities affected by the program, policy, or process, staff who work with program participants/beneficiaries or affected communities, subject matter experts such as researchers, or staff in other organizations, among others. Sources of expert input on programs, policies, and processes include listening sessions, surveys, interviews, focus groups, and position papers by experts in the field or advocacy groups.\n\nHow will the assessment team engage experts with lived experience with relevant programs, policies, processes, and/or issues? To what extent can these experts be part of the assessment team?\nHow will the assessment team engage other experts in the equity assessment (in addition to potentially involving them in the assessment team)? Which experts will be engaged?\nWhat individuals or communities have historically been excluded or disempowered in decision making? How can they be included and meaningfully engaged?\nHow can the assessment team ensure inclusivity when engaging experts, such as translation services or accommodations for people with disabilities? Will there be different options for sharing input for people with different communication preferences or time or transportation constraints?\nHow will the assessment team work to decrease power dynamics and ensure that experts are comfortable providing candid input? How can the team be transparent about how input will be shared and used?\nWhat methods can the assessment team use to collect input, such as focus groups on participants/beneficiaries’ experiences with programs? What are experts’ experiences with current programs and policies, and what are their views on the benefits and burdens involved in participating?\nWhat are experts’ perceptions about barriers to participation? Can experts help the assessment team understand whether there are current or potential burdens or barriers that are more severe for certain population groups?\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed guidance and resources regarding outreach and engagement, see the Practical Guidance Document developed by the Office of Public Participation.\n\n\n\n\n\nConsider a variety of qualitative and quantitative information sources to support the assessment, including gray and peer-reviewed literature, organization documents and administrative records, surveys, customer inquiry or complaint information, administrative data, program performance data, key informant interviews, and listening sessions or focus groups. Ideally, equity assessments often include both qualitative and quantitative data. Data sources can include, but should not be limited to, expert views.\n\nWhat are the quantitative data sources for the assessment process? Quantitative data such as program, administrative, or survey data shed light on the magnitude and prevalence of an inequity or an opportunity for improvement.\nAre available quantitative data disaggregated by relevant variables, such as race, ethnicity, income, and relevant geographic areas? If not, how can the assessment incorporate data that can help organizations understand or estimate the equity impacts of the program, policy, or process?\nWhat are the qualitative data sources for the assessment process? Qualitative data such as interview or focus group data increase understanding of context, as well as helping to interpret and understand quantitative data.\nAre there gaps or limitations in the information needed for the assessment? If either qualitative or quantitative data are not available, explain why. If there are gaps, how might the assessment team obtain new or better information, or highlight the need for investments in better data? It is important to describe gaps that might reflect historically overlooked inequities or point to the need for information sources that could be developed in future years.\n\n\n\n\nDrawing on all previous steps in the assessment process, analyze the available data and describe equity-related outcomes of the program, policy, or process. Describe findings with as much specificity as possible.\n\nWhat quantitative and qualitative analysis methods did the team use to analyze the available data? Did the team synthesize quantitative and qualitative data to develop a complete picture of current inequities or disparities related to the program, policy, or process?\nWhat are the assessment team’s findings on positive and negative equity-related outcomes of the program, policy, or process? What quantitative and qualitative evidence of inequities exists?\nWhat evidence is there of inequities in areas such as awareness of programs and benefits, processes and rules, administrative burden, access to services, participation, outcomes, quality, and engagement?\nHow do findings change the team’s understanding of disparities related to the selected program, policy, or process known at the outset of the assessment?\nWhat factors might be driving observed inequities or disparities? Are any of those factors potentially caused by the program or policy that is the focus of the assessment?\nHave experts helped the assessment team interpret the available data or validate or refine the initial findings?\nIn what ways might the findings be limited due to data gaps or analysis constraints? What findings point to the need for further research?\n\n\n\n\nDevelop a detailed plan to address inequities identified in Step 5 within the scope of your program.\n\nWhat solutions are needed to resolve observed inequities or disparities, or to address identified drivers of those inequities or disparities? Which solutions are in the program’s sphere of authority?\nWhat are the program’s short-term and long-term goals for improvement? Quantify those goals if possible.\nWhat steps will the program take to accomplish each goal? What coordination, training, information systems changes, business process changes, or other implementation actions are needed?\nHave subject matter experts—including those with lived experience—weighed in on needed solutions, proposed goals, or planned action steps? Are all components of the improvement plan responsive to the needs and cultures of different populations or communities?\nWhat resources will the program need to carry out the improvement plan?\nHas the program consulted or collaborated with key partners on potential improvement options and actions?\nIn what ways could the program coordinate with other partners to achieve equity improvements that are not solely within the control or influence of the program conducting the assessment?\n\nAdditional follow-up actions help programs learn about equity impacts and whether implementation should be adjusted to realize positive outcomes. In addition, equity assessments have the potential to generate many new lessons about equity that could be helpful for other partners. Articulating plans for these actions is part of the equity assessment even though these actions occur after the formal assessment is over.\n\nWould sharing the equity assessment with other partners support collaboration on other policies and programs intended to benefit priority populations?\nWould sharing the equity assessment or a summary of findings with experts who were not directly involved in the assessment further promote equity through transparency and accountability?\nWhat measures or indicators will the program use to track progress over time? Are these disaggregated individual-level or community-level measures? Monitoring can help the program assess whether patterns or trends are in the expected direction or require course corrections.\nHow and when will the organization evaluate the results of potential program changes? Evaluations focus on whether programs or policies reach their goals within a defined period. How can the organization design an equitable and inclusive evaluation?\nWho will be responsible for developing and executing monitoring and evaluation plans?  \nWill the program share monitoring and evaluation results with the experts involved in the assessment or other partners? If so, how?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-0-project-scoping",
    "href": "plan-prep/plan.html#step-0-project-scoping",
    "title": "Planning",
    "section": "",
    "text": "Time frame and level of effort:\n\nPlan a detailed schedule that accounts for staff availability, budget, technical assistance needs, data availability, and the need to make decisions in a particular time frame.\nIdentify and document risks to the timeline, such as staff availability, or threats to the comprehensiveness of the assessment, such as lack of access to experts. Consider potential ways to mitigate risks.\n\nProject team:\n\nDefine roles for team members and assign responsibilities. Plan to share information at key milestones to synthesize information from different assessment steps.\nConsider how to meaningfully involve experts, including people with lived experience with relevant programs and topics; people in communities affected by the program, policy, or process; staff who work with program participants/beneficiaries; or representatives of other offices. Experts can contribute to the assessment process in several ways, such as suggesting data sources, providing multiple perspectives to inform and enhance the analysis, and developing recommendations for action.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-1-describe-the-selected-program-policy-or-process-and-populations-affected-by-it",
    "href": "plan-prep/plan.html#step-1-describe-the-selected-program-policy-or-process-and-populations-affected-by-it",
    "title": "Planning",
    "section": "",
    "text": "Describe the focus of the assessment to provide a foundation for all members of the assessment team and external partners.\n\nWhat is the purpose of the selected program, policy, or process, and what are its goals?\nWhat are the known successes or challenges in meeting those goals?\nWhat types of actions or policy levers are involved in the selected program, policy, or process (such as grants, contracts, waivers, guidance to partners, technical assistance, or other actions)?\nWhich of these actions will be included in the assessment?\nWhat general descriptive or performance data can the organization use to describe the program, policy, or process (such as number served, total funds distributed, uptake estimates, or other key outcomes)?\nAre there existing quantifiable performance targets relevant to the focus of the assessment? Provide a brief summary.\n\nIdentify and describe populations of interest\n\nWhat populations are participating in the program, policy, or process, including program participants/beneficiaries? Consider which characteristics are relevant and of interest, such as race, ethnicity, gender identity, sexual orientation, disability status, income, religion, and rural geography.\nWhat populations are currently left out, or not participating or benefiting at desired rates or at the same rates as others? What are other disparities related to the selected program, policy, or process that are known at the outset of the assessment?\nWhat are the information sources for those inequities or disparities?  What is the comparison population or reference point for observed disparities? Reference point options include the total population in an area, the national population, the largest group, or a benchmark chosen through a planning process. Whenever possible, try to think critically about this population rather than simply defaulting to comparison populations used in the past.\nHow might population groups’ identifying characteristics overlap in ways that expose them to relatively greater inequities (known as intersectionality)? What implications does this overlap have for the impacts of the program, policy or process? For example, immigrants who are also LGBTQIA+ might face multiple barriers in accessing a particular program.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-2.-consider-historical-societal-and-policy-context-and-drivers-of-disparities",
    "href": "plan-prep/plan.html#step-2.-consider-historical-societal-and-policy-context-and-drivers-of-disparities",
    "title": "Planning",
    "section": "",
    "text": "Describe the context for observed disparities and the program or policy itself.\n\nWhat is the social and cultural history of the populations listed in Step 1 and how does this history shape their current conditions? How does this context play a role in how these populations might perceive, access, or otherwise interact with the program or policy?\nWhat structural or social drivers of disparities might explain observed disparities? Structural drivers of disparities are governing processes and economic and social policies that distribute power and resources in unfair ways, such as an inequitable distribution of emergency funds to certain communities. Social drivers of disparities are differences in the conditions in which people are born, grow, live, work, and age, such as poverty, employment, housing, environment quality, transportation, food security, and community safety. Differences in these social conditions drive disparities. Although these conditions are also known as social determinants of health, this tool uses a broader term to encompass multiple outcomes, including both health outcomes and other outcomes (e.g., economic outcomes). Thinking through these drivers of disparities is important for placing focus on systems and institutions that need to be changed, and it helps to avoid blaming groups of people for poor outcomes.\nWhat is known about whether structural, systemic, or institutional racism or structural barriers affect the implementation and outcomes of previous programs or policies? Systemic or institutional racism refers to policies and practices that create or sustain disparate outcomes for persons of different races. An example is redlining, where financial services and other housing-related opportunities were restricted for individuals largely based on their race/ethnicity and originating neighborhoods (see this 2021 Memorandum for the Secretary of Housing and Urban Development regarding Redressing Our Nation’s and the Federal Government’s History of Discriminatory Housing Practices and Policies)",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-3.-collect-expert-input-including-from-affected-community-members",
    "href": "plan-prep/plan.html#step-3.-collect-expert-input-including-from-affected-community-members",
    "title": "Planning",
    "section": "",
    "text": "Experts can include former or current program participants/beneficiaries, members of communities affected by the program, policy, or process, staff who work with program participants/beneficiaries or affected communities, subject matter experts such as researchers, or staff in other organizations, among others. Sources of expert input on programs, policies, and processes include listening sessions, surveys, interviews, focus groups, and position papers by experts in the field or advocacy groups.\n\nHow will the assessment team engage experts with lived experience with relevant programs, policies, processes, and/or issues? To what extent can these experts be part of the assessment team?\nHow will the assessment team engage other experts in the equity assessment (in addition to potentially involving them in the assessment team)? Which experts will be engaged?\nWhat individuals or communities have historically been excluded or disempowered in decision making? How can they be included and meaningfully engaged?\nHow can the assessment team ensure inclusivity when engaging experts, such as translation services or accommodations for people with disabilities? Will there be different options for sharing input for people with different communication preferences or time or transportation constraints?\nHow will the assessment team work to decrease power dynamics and ensure that experts are comfortable providing candid input? How can the team be transparent about how input will be shared and used?\nWhat methods can the assessment team use to collect input, such as focus groups on participants/beneficiaries’ experiences with programs? What are experts’ experiences with current programs and policies, and what are their views on the benefits and burdens involved in participating?\nWhat are experts’ perceptions about barriers to participation? Can experts help the assessment team understand whether there are current or potential burdens or barriers that are more severe for certain population groups?\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed guidance and resources regarding outreach and engagement, see the Practical Guidance Document developed by the Office of Public Participation.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-4.-identify-information-sources-and-gaps",
    "href": "plan-prep/plan.html#step-4.-identify-information-sources-and-gaps",
    "title": "Planning",
    "section": "",
    "text": "Consider a variety of qualitative and quantitative information sources to support the assessment, including gray and peer-reviewed literature, organization documents and administrative records, surveys, customer inquiry or complaint information, administrative data, program performance data, key informant interviews, and listening sessions or focus groups. Ideally, equity assessments often include both qualitative and quantitative data. Data sources can include, but should not be limited to, expert views.\n\nWhat are the quantitative data sources for the assessment process? Quantitative data such as program, administrative, or survey data shed light on the magnitude and prevalence of an inequity or an opportunity for improvement.\nAre available quantitative data disaggregated by relevant variables, such as race, ethnicity, income, and relevant geographic areas? If not, how can the assessment incorporate data that can help organizations understand or estimate the equity impacts of the program, policy, or process?\nWhat are the qualitative data sources for the assessment process? Qualitative data such as interview or focus group data increase understanding of context, as well as helping to interpret and understand quantitative data.\nAre there gaps or limitations in the information needed for the assessment? If either qualitative or quantitative data are not available, explain why. If there are gaps, how might the assessment team obtain new or better information, or highlight the need for investments in better data? It is important to describe gaps that might reflect historically overlooked inequities or point to the need for information sources that could be developed in future years.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-5.-analyze-programpolicy-effects---potential-or-current---on-people-and-communities",
    "href": "plan-prep/plan.html#step-5.-analyze-programpolicy-effects---potential-or-current---on-people-and-communities",
    "title": "Planning",
    "section": "",
    "text": "Drawing on all previous steps in the assessment process, analyze the available data and describe equity-related outcomes of the program, policy, or process. Describe findings with as much specificity as possible.\n\nWhat quantitative and qualitative analysis methods did the team use to analyze the available data? Did the team synthesize quantitative and qualitative data to develop a complete picture of current inequities or disparities related to the program, policy, or process?\nWhat are the assessment team’s findings on positive and negative equity-related outcomes of the program, policy, or process? What quantitative and qualitative evidence of inequities exists?\nWhat evidence is there of inequities in areas such as awareness of programs and benefits, processes and rules, administrative burden, access to services, participation, outcomes, quality, and engagement?\nHow do findings change the team’s understanding of disparities related to the selected program, policy, or process known at the outset of the assessment?\nWhat factors might be driving observed inequities or disparities? Are any of those factors potentially caused by the program or policy that is the focus of the assessment?\nHave experts helped the assessment team interpret the available data or validate or refine the initial findings?\nIn what ways might the findings be limited due to data gaps or analysis constraints? What findings point to the need for further research?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-6.-plan-for-action-and-accountability",
    "href": "plan-prep/plan.html#step-6.-plan-for-action-and-accountability",
    "title": "Planning",
    "section": "",
    "text": "Develop a detailed plan to address inequities identified in Step 5 within the scope of your program.\n\nWhat solutions are needed to resolve observed inequities or disparities, or to address identified drivers of those inequities or disparities? Which solutions are in the program’s sphere of authority?\nWhat are the program’s short-term and long-term goals for improvement? Quantify those goals if possible.\nWhat steps will the program take to accomplish each goal? What coordination, training, information systems changes, business process changes, or other implementation actions are needed?\nHave subject matter experts—including those with lived experience—weighed in on needed solutions, proposed goals, or planned action steps? Are all components of the improvement plan responsive to the needs and cultures of different populations or communities?\nWhat resources will the program need to carry out the improvement plan?\nHas the program consulted or collaborated with key partners on potential improvement options and actions?\nIn what ways could the program coordinate with other partners to achieve equity improvements that are not solely within the control or influence of the program conducting the assessment?\n\nAdditional follow-up actions help programs learn about equity impacts and whether implementation should be adjusted to realize positive outcomes. In addition, equity assessments have the potential to generate many new lessons about equity that could be helpful for other partners. Articulating plans for these actions is part of the equity assessment even though these actions occur after the formal assessment is over.\n\nWould sharing the equity assessment with other partners support collaboration on other policies and programs intended to benefit priority populations?\nWould sharing the equity assessment or a summary of findings with experts who were not directly involved in the assessment further promote equity through transparency and accountability?\nWhat measures or indicators will the program use to track progress over time? Are these disaggregated individual-level or community-level measures? Monitoring can help the program assess whether patterns or trends are in the expected direction or require course corrections.\nHow and when will the organization evaluate the results of potential program changes? Evaluations focus on whether programs or policies reach their goals within a defined period. How can the organization design an equitable and inclusive evaluation?\nWho will be responsible for developing and executing monitoring and evaluation plans?  \nWill the program share monitoring and evaluation results with the experts involved in the assessment or other partners? If so, how?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#footnotes",
    "href": "plan-prep/plan.html#footnotes",
    "title": "Planning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content on this page has been informed by: Conducting Intensive Equity Assessments of Existing Programs, Policies, and Processes (hhs.gov)↩︎",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/index.html",
    "href": "plan-prep/index.html",
    "title": "Plan & Prepare",
    "section": "",
    "text": "Plan & Prepare\n\n\n\n\n\n\nDon’t forget to publish your data!\n\n\n\nIf you are a steward of Water Boards data - and it is not yet made appropriately open and accessible (including thorough documentation and complete metadata!) - you should complete the guidance on the Publish Your Data page before you begin thinking about planning for your project.\n\n\nBefore data collection begins, it is paramount that you think through why you want to collect the data you think you need and how you intend to use it once it is collected.\nThis phase of the data life cycle involves conducting an equity assessment (Planning) and developing your data management plan using an equity lens (Data Preparation).\nAlso see a short list of best practices for this phase.",
    "crumbs": [
      "Plan & Prepare"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "Background\nDuring its August 18, 2020 meeting, the State Water Resources Control Board (State Water Board) publicly acknowledged that the historical effects of institutional racism must be confronted throughout government, and it directed staff to develop a priority plan of action (Aug 18, 2020 Meeting Agenda, Recording). The State Water Board’s Racial Equity Team held public and employee listening sessions to help develop a draft resolution. After a public comment period on the draft resolution in spring 2021, the Racial Equity Team made significant updates to the resolution. On November 16, 2021, the State Water Board adopted Resolution No. 2021-0050, “Condemning Racism, Xenophobia, Bigotry, and Racial Injustice and Strengthening Commitment to Racial Equity, Diversity, Inclusion, Access, and Anti-Racism” (Racial Equity Resolution) which affirms the State Water Board’s commitment to racial equity and directs staff to undertake a variety of actions to achieve racial equity throughout all State Water Board programs and activities (Nov 16, 2021 Meeting Agenda, Recording).\nThe Racial Equity Resolution is one milestone on our ongoing journey to operationalize equity throughout our organization. The next step is to implement the Racial Equity Action Plan, which includes specific actions the State Water Board will take to address racial inequities, as well as metrics to measure our progress. With this Action Plan, we envision a sustainable California where race no longer predicts where clean water is available or who has access to it. \n\n\n\n\n\n\nEquity is an outcome\n\n\n\nIt’s important to note that racial equity and equity in general is an outcome and there is no such thing as a “racial equity data-set” or “racial equity data”.\nInstead we should think of data as a tool to help us achieve the overall outcome of equity - and how we view, use, and act on data and related tools is what will determine whether we operationalize equity or perpetuate injustice.\n\n\nDevelopment of the Racial Equity Action Plan began in Spring 2022 and involved public and employee engagement and tribal consultations. The Water Boards’ Racial Equity Team presented the Racial Equity Action Plan (2023-2025) to the State Water Board as an informational item at the Board Meeting on January 18, 2023 (Agenda, Slides, Recording).\nGoal 1a of the Racial Equity Action Plan is to ensure Water Boards data are accessible, equitable, and culturally relevant. One action captured under that goal is the development of a Racial Equity Data Action Plan which must: \n\nDevelop training and best practices guidance for Water Boards staff on incorporating racial equity concepts into the planning and design of data collection methods and visualizations (e.g., maps, factsheets, etc.) projects.\nIdentify and expand existing opportunities for public participation in science and community data gathering programs to develop new data collection methods, support existing programs, and incorporate community datasets into the database. \nCreate a publicly accessible data catalog tool / interface that includes existing demographic data, Water Boards program data, and other available data (such as heat maps or flood hazard maps) to inform the implementation of the Racial Equity Action Plan.\n\nThe Racial Equity Data Action Plan is being developed by the Water Board’s Racial Equity Data Subcommittee of the Environmental Justice Roundtable which is a group of volunteer staff from across the Water Board led by the Office of Information Management and Analysis and the Office of Public Participation. This Handbook is intended to address Item 1 above by providing staff a best practices guide for incorporating racial equity concepts into the Water Board programs using data and information. Item 2 above will be iterative and grow as staff and programs begin to utilize the guidance and tools found within this handbook. To fulfill Item 3 above staff have created a California Water Boards Racial Equity Data Resource Hub which will grow as more programs create and publish racial equity based tools and visualizations.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "eval.html",
    "href": "eval.html",
    "title": "Evaluation",
    "section": "",
    "text": "After a project is complete (or has gone through a complete iteration) - it’s important to assess the project and evaluate the extent to which it has achieved the goals you set for it and advanced and improved equity outcomes. There are several ways to achieve this. Below is a list of methods one might consider utilizing for this phase of the data life cycle.\nDepending on what you discover during the evaluation process, you may need to undergo a second phase or iteration of the project to integrate lessons and get closer to achieving the original goals and desired equity outcomes.\n\n\n\n\n\n\nTip\n\n\n\nYou don’t need to wait until the project is complete to benefit from using these tools! It may be helpful to use these tools at multiple points during your project or process.\nWhen in doubt - make time to use these tools early and often!\n\n\n\n\nA root cause analysisis a technique that helps idenify the what caused a problem. Identifying the root cause(s) is the most effective way to prevent future problems and ultimately remedy the exisiting ones. The Water Boards Advancing Racial Equity training course provides some strong content touches on root cause analysis and it’s importance when considering racial inequities which is based on the common framework of asking why at least five times. Some online resources and templates are available below:\n\nhttps://slidemodel.com/templates/tag/root-cause-analysis/\nhttps://www.pvpc.org/content/root-cause-solutions-exchange\nhttps://www.mchevidence.org/learning/framework/tools.php",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#root-cause-analysis",
    "href": "eval.html#root-cause-analysis",
    "title": "Evaluation",
    "section": "",
    "text": "A root cause analysisis a technique that helps idenify the what caused a problem. Identifying the root cause(s) is the most effective way to prevent future problems and ultimately remedy the exisiting ones. The Water Boards Advancing Racial Equity training course provides some strong content touches on root cause analysis and it’s importance when considering racial inequities which is based on the common framework of asking why at least five times. Some online resources and templates are available below:\n\nhttps://slidemodel.com/templates/tag/root-cause-analysis/\nhttps://www.pvpc.org/content/root-cause-solutions-exchange\nhttps://www.mchevidence.org/learning/framework/tools.php",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Add guidance on documentation, reproducibility & transparency, oh my!\nThis is intended to be a living document and providing documentation is extremely important.\n\n\n\n\n\n\nImportant\n\n\n\nRemember - if your project or process is not well-documented to the point of being largely reproducible - it’s incomplete!\n\n\n\n\nDocumentation is a critical to improving transparency, ensuring when roles or positions change data and information is not lost. Detailed documentation of data sources, analysis methods, and code is also critical to equity because it helps ensure that data is meaningful, accessible, and actionable for communities. Without proper documentation, it can be difficult for others to validate or build on existing analyses. Clear documentation also allows us to pick up where we left off, or easily transition data projects from one team to another. Documentation in our data workflow can also help identify areas of improvement in workflows and paths forward. Projects are more manageable, with clear documentation and over the long-term, documentation can increase transparency and equity.\n\n\n\n\nDetailing where data or projects live and whether they’re posted to internal or external websites.\nNaming files deliberately\nCreating and maintaining a Data Dictionary\nDocument code through comments and #tidydata\n\n\n\n\n\nSchema.org – Promoting common data structures on the internet\nAccessibility tips using Markdown (Smashing Magazine)\nWritethedocs.org, plus a piece on documenting for beginners\nTemplates for documenting on GitHub (e.g., on a ReadMe)\nGitHub projects best practices\nNOAA NMFS: Open Science Resources\nNOAA Fisheries: Code documentation\nOpenscapes Approach Guide",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#why-document",
    "href": "documentation.html#why-document",
    "title": "Documentation",
    "section": "",
    "text": "Documentation is a critical to improving transparency, ensuring when roles or positions change data and information is not lost. Detailed documentation of data sources, analysis methods, and code is also critical to equity because it helps ensure that data is meaningful, accessible, and actionable for communities. Without proper documentation, it can be difficult for others to validate or build on existing analyses. Clear documentation also allows us to pick up where we left off, or easily transition data projects from one team to another. Documentation in our data workflow can also help identify areas of improvement in workflows and paths forward. Projects are more manageable, with clear documentation and over the long-term, documentation can increase transparency and equity.",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#what-can-we-document",
    "href": "documentation.html#what-can-we-document",
    "title": "Documentation",
    "section": "",
    "text": "Detailing where data or projects live and whether they’re posted to internal or external websites.\nNaming files deliberately\nCreating and maintaining a Data Dictionary\nDocument code through comments and #tidydata",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#tools-available-to-support-documentation",
    "href": "documentation.html#tools-available-to-support-documentation",
    "title": "Documentation",
    "section": "",
    "text": "Schema.org – Promoting common data structures on the internet\nAccessibility tips using Markdown (Smashing Magazine)\nWritethedocs.org, plus a piece on documenting for beginners\nTemplates for documenting on GitHub (e.g., on a ReadMe)\nGitHub projects best practices\nNOAA NMFS: Open Science Resources\nNOAA Fisheries: Code documentation\nOpenscapes Approach Guide",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "collect-process/collection.html",
    "href": "collect-process/collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "Important\n\n\n\nYou should have addressed this during the Plan & Prepare phase of your process…but just in case you haven’t (yet) or you need a refresher - we’re restating here:\nAchieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you begin to collect the data for your project, be sure it includes:\n\nData that can represent your management question(s) or project objectives.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start!\n\n\n\nThe data related to your project may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and/or compilations of data from other sources. Below we focus on data that can be downloaded from open data sources, and survey guidance.\n\n\nIt’s important to remember that ALL data have limits in what they can actually tell us, constraints on how they should be used appropriately, and biases related to initial data collection or generation - and it’s crucial to be aware of and account for them during your project.\nIn most cases, the Water Boards programs were not developed or designed to collect the types of data needed to conduct analyses with an equity lens as a matter of process which means that most will rely on a few key data sources.\n\n\n\nData Benefit vs Risk\n\n\nIncorporating a racial equity lens during data analysis includes incorporating individual, community, political, and historical contexts of race to inform analysis, conclusions, and recommendations. Solely relying on statistical outputs will not necessarily lead to insights without careful consideration during the analytic process, such as ensuring data quality is sufficient and determining appropriate statistical power. Disaggregation of data is also a series of tradeoffs. Without disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nCentering racial equity means paying attention to which data are highlighted and how they are framed, as well as the readability and accessibility of the communication method. This involves strategic consideration of the audience and the mode of dissemination that most effectively conveys the information. There are many ways to communicate information. These include briefs, interactive documents, websites, dashboards, social media content, data walks, posters, briefs, and infographics. Regardless of the form, content geared toward the public should avoid jargon that may be otherwise appropriate for internal program staff or academic audiences, while also using person-centered language and translating materials into languages most applicable to your community context. (Centering Racial Equity Throughout Data Integration)  \nFurthermore, good quality data regarding marginalized communities is often lacking, but it is still important to discuss impacts to BIPOC communities. It may be appropriate in some cases to still present or analyze this data and also present caveats for the data limitations. In other cases, it may be more appropriate to rely only on qualitative discussion based on information derived from background research and feedback from affected communities.\n\n\n\nBelow we have provided a list of common data sources that can tell us something about the extent to which we are achieving equity outcomes.\n\n\n\n\n\nPort over relevant water boards data and databases content?\n\n\n\nUnited States Census Data\nAmerican Community Survey Data\nInternal Administrative Data (i.e. Human Resources data)\n\n\n\n\n\n\nThe key to an equity analysis is to compare it to a baseline where race and ethnicity were not factors. Adding demographics data to your data project can help increase understanding of potential correlations or relationships between your data and demographic and socioeconomic characteristics of locations of interest.\nDepending on what demographics data sources you decide to use, the methods needed to combine, overlay, or compare with the data you are interested in may vary. Below we outline methods of comparing demographics data to point, line, and polygon data types.\n\n\nIt’s important to remember that you can always benefit from setting context before trying to communicate demographic or specific racial equity answers to questions posed by our Board, a member of the public, or a partner in this work. What your program is about, what does it do, how well does it do these things (aka Performance Report), etc. This may take a few different visualizations to help frame the context of the program mission but will help viewers understand how you are approaching the racial equity data work within the scope of your program. \nOften the go-to resources for making inferences to demographic and socioeconomic characteristics is the National Census dataset and the associated American Community Survey dataset. While we are fortunate to have just updated this dataset in 2020 there are limitations and potential inaccuracies associated with relying solely on census data to enumerate demographic characteristics within a given census tract. This tool from the Department of Finance exists to measure this limitation.\nA detailed example of using R programming to estimate demographics and other characteristics with U.S. census data to be used for custom spatial features is available and can be tailored to programs with the help of a data scientist proficient in R and staff familiar with the program.  https://daltare.github.io/example-census-race-ethnicity-calculation/example_census_race_ethnicity_calculation.html \nExample - SAFER Fund Expenditure Plan\nSection VIII.G. Racial Equity and Environmental Justice provides several tables with data that incorporates demographics data. The tables and the text lack clarity and indepth analysis on why the data is telling the story it is. Race and ethnicity of the populations served by the systems likely isn’t the only only difference between the systems. Are there other factors that are associated with different populations that could be driving the imbalance in failing systems and funding? Perhaps, the majority-Hispanic population systems are much larger, or older, or have more severe problems. An apples to apples comparison would look at the major factors that determine cost and compare the racial difference between those subgroups. For example, comparing the racial and ethnic differences between systems of medium sized cities with a water treatment plant built within the past 30 years. Making these proportions more explicit and adding a section where Hispanics population systems get more funding, proportionally and list some additional explanatory factors that should be explored.\nTIP: It’s ok to note data gaps, and in the case of racial equity data, gaps will be the norm. It is a key part of a good analysis to identify data gaps and set a course for filling those gaps.\n\n\n\n\n\n\nThe California Water Boards Racial Equity Data Resource Hub provides a list of Water Boards Developed Racial Equity Data Tools\n\n\n\nCalEnviroScreen 4.0\nCalEnviroScreen can be a helpful tool in creating visualizations and performing analysis as it provides a number of index, as well as a “rolled-up” score that combines environmental and demographic data together. However, there can be things to consider, a couple of which are discussed below.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores. These missing values are distinct from zeros, which are also in the CES dataset. For more information about the missing (and zero) values, see the data dictionary (calenviroscreen40resultsdatadictionary_F_2021.pdf) that accompanies the CalEnviroScreen 4.0 results Excel workbook, available for download as a zip file here.\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores (available here) encodes these missing values as negative numbers (-999 for most variables, and -1998 for one variable). The Excel workbook containing CES 4.0 scores (available here) encodes these missing values as NA. Users should account for these missing values – and their different encodings – as needed when doing any analysis using CES data. Also, note that the CalEnviroScreen 3 shapefile (June 2018 update version) encoded missing values as 0, so users should be aware of this change if/when updating an analysis from CES 3 to CES 4.0 data.\n\n\n\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores (available here) uses a simplified version of the polygons that represent 2010 census tracts. The boundaries of the census tracts defined by these simplified polygons do not always align with the boundaries of neighboring census tracts, resulting in slight gaps or overlaps between some neighboring census tracts. These inconsistencies are not likely to have a significant impact on most uses of the CES data, but they could impact some types of analysis based on CES data. For example, when assessing sites or facilities based on the CES score of the census tract they are located in, sites located near a census tract boundary could be associated with more than one census tract (and more than one CES score) in areas where there are overlapping census tract polygons, or not associated with any census tract (and no CES score) in areas where there are gaps between census tract polygons. This issue may be addressed in a future release of the CES dataset; in the meantime, a possible workaround is to use the official 2010 census tract boundaries from the US Census Bureau for any calculations, then use census tract IDs to tie this information to the associated CES score for each tract.\nCalEnviroScreen vs. EnviroFacts vs. EnviroMapper vs. EJ Screen\n\n\n\n\n\n\nThere may be instances where you need to collect new data using survey(s).\n\n\nCreating surveys that yield actionable insights is all about the details - and writing effective survey questions is the first step. You do not have to be an expert to build and distribute an effective online survey, but by checking your survey against tried-and-tested benchmarks, you can help ensure you are collecting the best data possible.  \nTips for Building an Effective Survey:\n\nMake Sure That Every Question Is Necessary.\nKeep it Short and Simple.\nAsk Direct Questions.\nAsk One Question at a Time.\nAvoid Leading and Biased Questions.\nSpeak Your Respondent’s Language.\nUse Response Scales Whenever Possible.\nAvoid Using Grids or Matrices for Responses.\nRephrase Yes/No Questions if Possible\nTake Your Survey for a Test Drive\n\nA good comprehensive guide for survey design can be found here:  https://files.eric.ed.gov/fulltext/ED619797.pdf \nhttps://www.qualtrics.com/blog/10-tips-for-building-effective-surveys/\n\n\n\n\n\n\nMost Water Board staff will use Microsoft Forms which is available to all staff through the Microsoft 365 suite of applications.  Microsoft Forms has a lot of advantages because of its integration with other Microsoft tools like Excel and PowerBi which allow for the survey results to be analyzed and visualized.  Here is video on how to make that connection between Forms and PowerBi via Sharepoint that allows for consistent updating of results:  https://youtu.be/XBFVDedwLiY?si=O161oYja-FBhG1W7 \nOne issue with Microsoft Forms and other free software like Google Forms is that they produce wide data in Excel of Google Sheets.  This type of data is more difficult to transform.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#data-limitations",
    "href": "collect-process/collection.html#data-limitations",
    "title": "Data Collection",
    "section": "",
    "text": "It’s important to remember that ALL data have limits in what they can actually tell us, constraints on how they should be used appropriately, and biases related to initial data collection or generation - and it’s crucial to be aware of and account for them during your project.\nIn most cases, the Water Boards programs were not developed or designed to collect the types of data needed to conduct analyses with an equity lens as a matter of process which means that most will rely on a few key data sources.\n\n\n\nData Benefit vs Risk\n\n\nIncorporating a racial equity lens during data analysis includes incorporating individual, community, political, and historical contexts of race to inform analysis, conclusions, and recommendations. Solely relying on statistical outputs will not necessarily lead to insights without careful consideration during the analytic process, such as ensuring data quality is sufficient and determining appropriate statistical power. Disaggregation of data is also a series of tradeoffs. Without disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nCentering racial equity means paying attention to which data are highlighted and how they are framed, as well as the readability and accessibility of the communication method. This involves strategic consideration of the audience and the mode of dissemination that most effectively conveys the information. There are many ways to communicate information. These include briefs, interactive documents, websites, dashboards, social media content, data walks, posters, briefs, and infographics. Regardless of the form, content geared toward the public should avoid jargon that may be otherwise appropriate for internal program staff or academic audiences, while also using person-centered language and translating materials into languages most applicable to your community context. (Centering Racial Equity Throughout Data Integration)  \nFurthermore, good quality data regarding marginalized communities is often lacking, but it is still important to discuss impacts to BIPOC communities. It may be appropriate in some cases to still present or analyze this data and also present caveats for the data limitations. In other cases, it may be more appropriate to rely only on qualitative discussion based on information derived from background research and feedback from affected communities.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#common-data-sources",
    "href": "collect-process/collection.html#common-data-sources",
    "title": "Data Collection",
    "section": "",
    "text": "Below we have provided a list of common data sources that can tell us something about the extent to which we are achieving equity outcomes.\n\n\n\n\n\nPort over relevant water boards data and databases content?\n\n\n\nUnited States Census Data\nAmerican Community Survey Data\nInternal Administrative Data (i.e. Human Resources data)\n\n\n\n\n\n\nThe key to an equity analysis is to compare it to a baseline where race and ethnicity were not factors. Adding demographics data to your data project can help increase understanding of potential correlations or relationships between your data and demographic and socioeconomic characteristics of locations of interest.\nDepending on what demographics data sources you decide to use, the methods needed to combine, overlay, or compare with the data you are interested in may vary. Below we outline methods of comparing demographics data to point, line, and polygon data types.\n\n\nIt’s important to remember that you can always benefit from setting context before trying to communicate demographic or specific racial equity answers to questions posed by our Board, a member of the public, or a partner in this work. What your program is about, what does it do, how well does it do these things (aka Performance Report), etc. This may take a few different visualizations to help frame the context of the program mission but will help viewers understand how you are approaching the racial equity data work within the scope of your program. \nOften the go-to resources for making inferences to demographic and socioeconomic characteristics is the National Census dataset and the associated American Community Survey dataset. While we are fortunate to have just updated this dataset in 2020 there are limitations and potential inaccuracies associated with relying solely on census data to enumerate demographic characteristics within a given census tract. This tool from the Department of Finance exists to measure this limitation.\nA detailed example of using R programming to estimate demographics and other characteristics with U.S. census data to be used for custom spatial features is available and can be tailored to programs with the help of a data scientist proficient in R and staff familiar with the program.  https://daltare.github.io/example-census-race-ethnicity-calculation/example_census_race_ethnicity_calculation.html \nExample - SAFER Fund Expenditure Plan\nSection VIII.G. Racial Equity and Environmental Justice provides several tables with data that incorporates demographics data. The tables and the text lack clarity and indepth analysis on why the data is telling the story it is. Race and ethnicity of the populations served by the systems likely isn’t the only only difference between the systems. Are there other factors that are associated with different populations that could be driving the imbalance in failing systems and funding? Perhaps, the majority-Hispanic population systems are much larger, or older, or have more severe problems. An apples to apples comparison would look at the major factors that determine cost and compare the racial difference between those subgroups. For example, comparing the racial and ethnic differences between systems of medium sized cities with a water treatment plant built within the past 30 years. Making these proportions more explicit and adding a section where Hispanics population systems get more funding, proportionally and list some additional explanatory factors that should be explored.\nTIP: It’s ok to note data gaps, and in the case of racial equity data, gaps will be the norm. It is a key part of a good analysis to identify data gaps and set a course for filling those gaps.\n\n\n\n\n\n\nThe California Water Boards Racial Equity Data Resource Hub provides a list of Water Boards Developed Racial Equity Data Tools\n\n\n\nCalEnviroScreen 4.0\nCalEnviroScreen can be a helpful tool in creating visualizations and performing analysis as it provides a number of index, as well as a “rolled-up” score that combines environmental and demographic data together. However, there can be things to consider, a couple of which are discussed below.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores. These missing values are distinct from zeros, which are also in the CES dataset. For more information about the missing (and zero) values, see the data dictionary (calenviroscreen40resultsdatadictionary_F_2021.pdf) that accompanies the CalEnviroScreen 4.0 results Excel workbook, available for download as a zip file here.\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores (available here) encodes these missing values as negative numbers (-999 for most variables, and -1998 for one variable). The Excel workbook containing CES 4.0 scores (available here) encodes these missing values as NA. Users should account for these missing values – and their different encodings – as needed when doing any analysis using CES data. Also, note that the CalEnviroScreen 3 shapefile (June 2018 update version) encoded missing values as 0, so users should be aware of this change if/when updating an analysis from CES 3 to CES 4.0 data.\n\n\n\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores (available here) uses a simplified version of the polygons that represent 2010 census tracts. The boundaries of the census tracts defined by these simplified polygons do not always align with the boundaries of neighboring census tracts, resulting in slight gaps or overlaps between some neighboring census tracts. These inconsistencies are not likely to have a significant impact on most uses of the CES data, but they could impact some types of analysis based on CES data. For example, when assessing sites or facilities based on the CES score of the census tract they are located in, sites located near a census tract boundary could be associated with more than one census tract (and more than one CES score) in areas where there are overlapping census tract polygons, or not associated with any census tract (and no CES score) in areas where there are gaps between census tract polygons. This issue may be addressed in a future release of the CES dataset; in the meantime, a possible workaround is to use the official 2010 census tract boundaries from the US Census Bureau for any calculations, then use census tract IDs to tie this information to the associated CES score for each tract.\nCalEnviroScreen vs. EnviroFacts vs. EnviroMapper vs. EJ Screen",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#surveys",
    "href": "collect-process/collection.html#surveys",
    "title": "Data Collection",
    "section": "",
    "text": "There may be instances where you need to collect new data using survey(s).\n\n\nCreating surveys that yield actionable insights is all about the details - and writing effective survey questions is the first step. You do not have to be an expert to build and distribute an effective online survey, but by checking your survey against tried-and-tested benchmarks, you can help ensure you are collecting the best data possible.  \nTips for Building an Effective Survey:\n\nMake Sure That Every Question Is Necessary.\nKeep it Short and Simple.\nAsk Direct Questions.\nAsk One Question at a Time.\nAvoid Leading and Biased Questions.\nSpeak Your Respondent’s Language.\nUse Response Scales Whenever Possible.\nAvoid Using Grids or Matrices for Responses.\nRephrase Yes/No Questions if Possible\nTake Your Survey for a Test Drive\n\nA good comprehensive guide for survey design can be found here:  https://files.eric.ed.gov/fulltext/ED619797.pdf \nhttps://www.qualtrics.com/blog/10-tips-for-building-effective-surveys/\n\n\n\n\n\n\nMost Water Board staff will use Microsoft Forms which is available to all staff through the Microsoft 365 suite of applications.  Microsoft Forms has a lot of advantages because of its integration with other Microsoft tools like Excel and PowerBi which allow for the survey results to be analyzed and visualized.  Here is video on how to make that connection between Forms and PowerBi via Sharepoint that allows for consistent updating of results:  https://youtu.be/XBFVDedwLiY?si=O161oYja-FBhG1W7 \nOne issue with Microsoft Forms and other free software like Google Forms is that they produce wide data in Excel of Google Sheets.  This type of data is more difficult to transform.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/process.html",
    "href": "collect-process/process.html",
    "title": "Data Processing",
    "section": "",
    "text": "“Raw data, like raw potatoes, usually require cleaning before use.”\n— Ronald A. Thisted, Professor Emeritus - Departments of Statistics and Public Health Sciences, The University of Chicago\n\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy, Russian author known as one of the world’s greatest novelists.\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham, Chief Scientist at Posit (formerly RStudio)\n\nGood data organization and formatting is the foundation of any data project. Once the data are collected, you will need to process them so they can be used in your analyses or product development steps.\n\n\n\n\n\n\nData processing takes time - plan accordingly!\n\n\n\nIt’s commonly understood in the data science field that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003) - expect the same will be true for your project and prepare to invest time and resources accordingly.\n\n\nThe Resources section below links to resources that provide detailed technical guidance on how to process data using data science methods. Here, we discuss the why and how we go about this step with an equity lens.\nWhen we talk about data processing here, we’re referring to the steps required to organize, format, and clean the data so it’s more efficient to use in future steps, and so others can consistently reproduce your steps - - this is commonly referred to as tidying data.\n\n\nFrom a data science lens - Investing the time to properly tidy your data upfront makes the analyses or product development steps much more efficient and reproducible. This will cut down the time it takes in the long term to complete your analysis and product development steps (even if they need to evolve/change over time!), enabling you to spend more time on the actual data, science, management, and equity questions you have for your project (aka the good stuff!).\nFrom an equity lens - having consistent and reproducible data makes it easy for others to replicate your work, which can increase transparency of your process and ultimately help build trust with your partners, the communities impacted by the management decisions associated with your project, and the public at large.\nAdditionally, working from a foundation of tidy data (and using consistent tools) can make it easier for others to collaborate with and contribute to your data project, which generally results in a better product in the end.\n\n\n\nKeep your “raw data raw”: It’s important to keep (and backup) a version of your data that is untouched by your process (i.e. raw). We also recommend backing up a version of your data that is tidied and cleaned (i.e. what you will use for analysis). Not only does doing so protect you if your working version of the data are lost for some reason (e.g., computer crash, freak system failure) - but it also helps with reproducibility and transparency of your process.\nRemove duplicate or irrelevant data: When you’re processing your data and making it tidy, think through which data from your sources are applicable to your question(s). Sometimes we don’t have the technical or logistical capacity to keep ALL of the data we are able to collect - so if you are going to remove data at this step, be sure to document WHY you’re removing it in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making. Some reasons might include:\n\nData are outside of the geographic scope of the analysis (e.g. we’re focused on a particular region for the project, and we are removing data outside of that region so we can increase loading/analysis speeds)\nFields/columns from source data are not relevant to our analysis for XYZ reasons.\n\nTransform your data into a tidy structure: Here you will reformat or restructure your dataset so it can be used in your future analyses or product development steps in a way that is efficient, effective, and meaningful for your project’s objectives. A common data transformation at this phase is converting a dataset from wide to long formats.\n\n\n\nGraphic illustrating tidy data as a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, each observation has its own row, and each cell is a single measurement. Artwork by Allison Horst from the Openscapes Blog: Tidy data for efficiency, reproducibility, and collaboration by Julie Lowndes and Allison Horst.\n\n\nExplore your data - but don’t analyze it: Once duplicate and irrelevant data are removed from the dataset, you may want to explore your data using simple visualizations or statistics (see the Data Exploration Checklist for ideas). This will help you find extreme outliers in your data, or otherwise incorrect or missing data that should be removed, separated, or corrected before you begin your data analysis phase. Similar to when you removed duplicate or irrelevant data, you will want to document WHY you’re removing/correcting these data in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making.\nAny aggregation or preliminary analysis of your data should be completed during the analysis phase of the project. Keeping the values and aggregation of the data as they are at this phase increases reproducibility and transparency now and during future steps. Note that this is different from transforming data as described above (e.g. restructuring from wide to long format), which is a common and very useful part of the data processing phase.\nYou might be tempted to begin your data quality assessments as you’re looking at the data during this phase, but you’ll actually want to hold off on that for now. You’ll complete your data quality and assurance steps after all of your data have been tidied and you can look at all of your data holistically.\n\n\n\n\nJulia Lowndes and Allison Horst (2020) Tidy Data for reproducibility, efficiency, and collaboration. Openscapes blog.\nCollege of Water Informatics Data Management Handbook - Collect and Process Section\nHadley Wickham. Tidy Data. Journal of Statistical Software\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science (2e) - Data Tidying Chapter\nThe Py4DS Community. Python for Data Science - Tidy Data Chapter\nKarl W. Broman, and Kara H Woo (2018) Data Organization in Spreadsheets. The American Statistician 72 (1). Available open access as a PeerJ preprint.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#why-tidy-data",
    "href": "collect-process/process.html#why-tidy-data",
    "title": "Data Processing",
    "section": "",
    "text": "From a data science lens - Investing the time to properly tidy your data upfront makes the analyses or product development steps much more efficient and reproducible. This will cut down the time it takes in the long term to complete your analysis and product development steps (even if they need to evolve/change over time!), enabling you to spend more time on the actual data, science, management, and equity questions you have for your project (aka the good stuff!).\nFrom an equity lens - having consistent and reproducible data makes it easy for others to replicate your work, which can increase transparency of your process and ultimately help build trust with your partners, the communities impacted by the management decisions associated with your project, and the public at large.\nAdditionally, working from a foundation of tidy data (and using consistent tools) can make it easier for others to collaborate with and contribute to your data project, which generally results in a better product in the end.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#processing-data-with-an-equity-lens",
    "href": "collect-process/process.html#processing-data-with-an-equity-lens",
    "title": "Data Processing",
    "section": "",
    "text": "Keep your “raw data raw”: It’s important to keep (and backup) a version of your data that is untouched by your process (i.e. raw). We also recommend backing up a version of your data that is tidied and cleaned (i.e. what you will use for analysis). Not only does doing so protect you if your working version of the data are lost for some reason (e.g., computer crash, freak system failure) - but it also helps with reproducibility and transparency of your process.\nRemove duplicate or irrelevant data: When you’re processing your data and making it tidy, think through which data from your sources are applicable to your question(s). Sometimes we don’t have the technical or logistical capacity to keep ALL of the data we are able to collect - so if you are going to remove data at this step, be sure to document WHY you’re removing it in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making. Some reasons might include:\n\nData are outside of the geographic scope of the analysis (e.g. we’re focused on a particular region for the project, and we are removing data outside of that region so we can increase loading/analysis speeds)\nFields/columns from source data are not relevant to our analysis for XYZ reasons.\n\nTransform your data into a tidy structure: Here you will reformat or restructure your dataset so it can be used in your future analyses or product development steps in a way that is efficient, effective, and meaningful for your project’s objectives. A common data transformation at this phase is converting a dataset from wide to long formats.\n\n\n\nGraphic illustrating tidy data as a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, each observation has its own row, and each cell is a single measurement. Artwork by Allison Horst from the Openscapes Blog: Tidy data for efficiency, reproducibility, and collaboration by Julie Lowndes and Allison Horst.\n\n\nExplore your data - but don’t analyze it: Once duplicate and irrelevant data are removed from the dataset, you may want to explore your data using simple visualizations or statistics (see the Data Exploration Checklist for ideas). This will help you find extreme outliers in your data, or otherwise incorrect or missing data that should be removed, separated, or corrected before you begin your data analysis phase. Similar to when you removed duplicate or irrelevant data, you will want to document WHY you’re removing/correcting these data in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making.\nAny aggregation or preliminary analysis of your data should be completed during the analysis phase of the project. Keeping the values and aggregation of the data as they are at this phase increases reproducibility and transparency now and during future steps. Note that this is different from transforming data as described above (e.g. restructuring from wide to long format), which is a common and very useful part of the data processing phase.\nYou might be tempted to begin your data quality assessments as you’re looking at the data during this phase, but you’ll actually want to hold off on that for now. You’ll complete your data quality and assurance steps after all of your data have been tidied and you can look at all of your data holistically.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#resources",
    "href": "collect-process/process.html#resources",
    "title": "Data Processing",
    "section": "",
    "text": "Julia Lowndes and Allison Horst (2020) Tidy Data for reproducibility, efficiency, and collaboration. Openscapes blog.\nCollege of Water Informatics Data Management Handbook - Collect and Process Section\nHadley Wickham. Tidy Data. Journal of Statistical Software\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science (2e) - Data Tidying Chapter\nThe Py4DS Community. Python for Data Science - Tidy Data Chapter\nKarl W. Broman, and Kara H Woo (2018) Data Organization in Spreadsheets. The American Statistician 72 (1). Available open access as a PeerJ preprint.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "use-cases/demographics.html",
    "href": "use-cases/demographics.html",
    "title": "Demographics Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.",
    "crumbs": [
      "Use Cases",
      "Demographics Data"
    ]
  },
  {
    "objectID": "use-cases/demographics.html#point-data",
    "href": "use-cases/demographics.html#point-data",
    "title": "Demographics Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.",
    "crumbs": [
      "Use Cases",
      "Demographics Data"
    ]
  },
  {
    "objectID": "use-cases/uc_demographics.html",
    "href": "use-cases/uc_demographics.html",
    "title": "Demographics",
    "section": "",
    "text": "Demographics"
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Tip\n\n\n\nBefore embarking on your data project using a racial equity lens you should:\n\nMake your Water Boards data open and accessible to the public! See the Open Data Handbook for guidance.\nConsult the Racial Equity Data Subcommittee by emailing equitydatahelp@waterboards.ca.gov to help identify audience and potential questions or information your teams’ analysis may answer and provide guidance.\nReview the guidance and best practices described in this Equity Data Handbook and complete the Racial Equity Data Project Form\nReview the GARE Framework: Normalize, Organize, and Operationalize  \n\nOther Optional Recommendations\n\nTake the Advancing Racial Equity training series offered by the Water Boards Training Academy\nPut a team together and join an Openscapes Champions Cohort at the Water Boards\n\n\n\n\n\nIn accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nGuidance on open data publishing at the Water Boards is available in the Open Data Handbook.\n\n\n\nProgram staff that are beginning data project using a racial equity lens should meet with the Racial Equity Data Team to discuss key aspects of the project including who the audience is and what questions or information you are trying to convey using the available racial equity data. This engagement should occur prior to beginning any data collection or analysis.\nTo initiate a consultation with the Racial Equity Data Team, please send an email to equitydatahelp@waterboards.ca.gov. \n\n\n\nThis Equity Data Handbook is a curated compilation of emerging and comprehensive (but not exhaustive) guidance on:  \n\nHow to break down the management questions where racial equity information is being posed against administrative data; and \nHow to apply equity best practices during each phase of the data life cycle to begin an iterative process into advancing racial equity. \n\nThis Handbook is specifically structured to support Water Boards staff on incorporating racial equity concepts into all phases of their data projects. The guidance and best practices provided serves as a strategic guide emphasizing the importance of collection, analysis and utilization of racial equity data.\nThese intended users of this Handbook include, but are not limited to:  \n\nProgram Staff\nProgram managers\nExecutives\nAgency Partners\nTribal Governments\nThe Public \n\nAfter reviewing this Handbook, please complete the Racial Equity Data Project Form so that the Racial Equity Data Subcommittee is better equipped for your project consultation.\n\n\n\nIt’s strongly suggested staff review the framework outlined by the Government Alliance for Racial Equity (GARE) to normalize, organize, and operationalize racial equity throughout data integration (see image below).\n\n\n\nGARE model of change. Source: GARE Communications Guide, May 2018\n\n\nIn addition staff should also review why it is important to lead with race:\n\n“with the recognition that the creation and perpetuation of racial inequities has been baked into government, and that racial inequities across all indicators for success are deep and pervasive. We also know that other groups of people are still marginalized, including based on gender, sexual orientation, ability and age, to name but a few. Focusing on racial equity provides the opportunity to introduce a framework, tools and resources that can also be applied to other areas of marginalization. It is critical to address all areas of marginalization, and an institutional approach is necessary across the board. As the local and regional government deepens its ability to eliminate racial inequity, it will be better equipped to transform systems and institutions impacting other marginalized groups.”\n\n\n\n\n\n\nWhen possible, staff should take the Advancing Racial Equity training series offered by the Water Boards Training Academy to foster a consistent baseline knowledge of racial equity work and the importance of applying a racial equity lens to our work.\n\n\n\nOpenscapes through their Champions Program, provides a framework for education, integration, and operationalization of open science, equity, communication, and kindness into individual and team collaborations and workflows. Teams that participate in the Champions Program are empowered to evolve and invest in their culture, processes, and workflows so that they can embody the better science for future us mindset.\nWater Boards teams are able to join Openscapes Champions Cohorts that are created for and led by Water Boards staff.\nFor more information and to learn how to join an upcoming Openscapes Champions Cohort at the Water Boards, visit the Openscapes at the Water Boards webpage.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#make-your-water-boards-data-open-and-accessible",
    "href": "get-started/index.html#make-your-water-boards-data-open-and-accessible",
    "title": "Getting Started",
    "section": "",
    "text": "In accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nGuidance on open data publishing at the Water Boards is available in the Open Data Handbook.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#consult-the-racial-equity-data-subcommittee",
    "href": "get-started/index.html#consult-the-racial-equity-data-subcommittee",
    "title": "Getting Started",
    "section": "",
    "text": "Program staff that are beginning data project using a racial equity lens should meet with the Racial Equity Data Team to discuss key aspects of the project including who the audience is and what questions or information you are trying to convey using the available racial equity data. This engagement should occur prior to beginning any data collection or analysis.\nTo initiate a consultation with the Racial Equity Data Team, please send an email to equitydatahelp@waterboards.ca.gov.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#review-the-equity-data-handbook-and-complete-the-racial-equity-data-project-form",
    "href": "get-started/index.html#review-the-equity-data-handbook-and-complete-the-racial-equity-data-project-form",
    "title": "Getting Started",
    "section": "",
    "text": "This Equity Data Handbook is a curated compilation of emerging and comprehensive (but not exhaustive) guidance on:  \n\nHow to break down the management questions where racial equity information is being posed against administrative data; and \nHow to apply equity best practices during each phase of the data life cycle to begin an iterative process into advancing racial equity. \n\nThis Handbook is specifically structured to support Water Boards staff on incorporating racial equity concepts into all phases of their data projects. The guidance and best practices provided serves as a strategic guide emphasizing the importance of collection, analysis and utilization of racial equity data.\nThese intended users of this Handbook include, but are not limited to:  \n\nProgram Staff\nProgram managers\nExecutives\nAgency Partners\nTribal Governments\nThe Public \n\nAfter reviewing this Handbook, please complete the Racial Equity Data Project Form so that the Racial Equity Data Subcommittee is better equipped for your project consultation.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#review-the-gare-framework",
    "href": "get-started/index.html#review-the-gare-framework",
    "title": "Getting Started",
    "section": "",
    "text": "It’s strongly suggested staff review the framework outlined by the Government Alliance for Racial Equity (GARE) to normalize, organize, and operationalize racial equity throughout data integration (see image below).\n\n\n\nGARE model of change. Source: GARE Communications Guide, May 2018\n\n\nIn addition staff should also review why it is important to lead with race:\n\n“with the recognition that the creation and perpetuation of racial inequities has been baked into government, and that racial inequities across all indicators for success are deep and pervasive. We also know that other groups of people are still marginalized, including based on gender, sexual orientation, ability and age, to name but a few. Focusing on racial equity provides the opportunity to introduce a framework, tools and resources that can also be applied to other areas of marginalization. It is critical to address all areas of marginalization, and an institutional approach is necessary across the board. As the local and regional government deepens its ability to eliminate racial inequity, it will be better equipped to transform systems and institutions impacting other marginalized groups.”",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#other-optional-recommendations",
    "href": "get-started/index.html#other-optional-recommendations",
    "title": "Getting Started",
    "section": "",
    "text": "When possible, staff should take the Advancing Racial Equity training series offered by the Water Boards Training Academy to foster a consistent baseline knowledge of racial equity work and the importance of applying a racial equity lens to our work.\n\n\n\nOpenscapes through their Champions Program, provides a framework for education, integration, and operationalization of open science, equity, communication, and kindness into individual and team collaborations and workflows. Teams that participate in the Champions Program are empowered to evolve and invest in their culture, processes, and workflows so that they can embody the better science for future us mindset.\nWater Boards teams are able to join Openscapes Champions Cohorts that are created for and led by Water Boards staff.\nFor more information and to learn how to join an upcoming Openscapes Champions Cohort at the Water Boards, visit the Openscapes at the Water Boards webpage.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nThis Equity Data Handbook is an online resource written by the The State Water Resources Control Board (State Water Board) and the nine Regional Water Quality Control Boards (Regional Water Boards), collectively known as the California Water Boards (Water Boards).\nContent in this Handbook includes best practices and guidance for Water Boards staff on incorporating racial equity concepts into their data-related work. Specifically, this Handbook will provide guidance and resources to help Water Boards staff conduct each phase of the data life cycle through a racial equity lens - from the planning and design of a project to data collection methods, visualization development (e.g., maps, fact sheets, etc.) and more!\nA depiction of the Data Life Cycle is provided below for context. We must have meaningful engagement and partnership with our diverse communities during each phase of the Data Life Cycle, especially those that have been historically underserved namely Black, Indigenous, and other People of Color (BIPOC). If we focus on uplifting those most highly impacted we will inevitably improve the the data products and services we develop and, most importantly, the experiences and outcomes for all communities.\n\n\n\nGraphic of the data life cycle.\n\n\nThis Quarto book is an open, living, and continuously iterating resource. If you have suggestions for additions or revisions you think should be incorporated into this book, please email equitydatahelp@waterboards.ca.gov.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "publish-data.html",
    "href": "publish-data.html",
    "title": "Publish Water Boards Data",
    "section": "",
    "text": "Publish Water Boards Data\nIn accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nWhen it comes to doing racial equity data work, a critical first step that should occur before we consider taking on additional data related projects (e.g. making maps or other visualizations) is to make the data that already exits and for which we steward and/or are responsible, open, transparent, and accessible to the public (as appropriate).\nNot only does making data open help improve transparency, but doing so before you build your data interpretation tools (e.g. visualizations, reports) makes it easier for the communities we serve to access the data behind our interpretations and can ultimately help build relationships and trust.\nMaking data open is more than adding a spreadsheet or flat file to a webpage. Truly making your data open involves investing the time and resources required to follow the guidance on open data publishing at the Water Boards is available in the Open Data Handbook, including:\n\nmaking data machine readable\nformatting data in a way that aligns with common data standards\nensuring the quality of data is known and documented\ngoing through data security process\ngenerating and ensuring all metadata and documentation related to the data is comprehensive and complete\npublishing data and all documentation in an open data platform / format\n\nContact OIMA (OIMA-Helpdesk@waterboards.ca.gov) for specific guidance or support with navigating through the open data publication process at the Water Boards.",
    "crumbs": [
      "Publish Your Data"
    ]
  },
  {
    "objectID": "get-started/best-practice.html",
    "href": "get-started/best-practice.html",
    "title": "Best Practices",
    "section": "",
    "text": "We strongly encourage staff to read this Handbook in its entirety. However, for those who want to jump to best practices, or who just need a refresher, we have provided them below, with links to pages that have more details, guidance, and resources.\n\n\n\n\n\nPractice\nDescription\nSource\nNotes\n\n\n\n\nIt is helpful to survey a range of existing tools when considering the mapping of “disadvantaged” populations.\nEach presents strengths and weaknesses to consider while keeping in mind that ultimately decisions will be driven by:\n\nStatutory mandates and priorities of each unique program\nData availability, currency, longevity\nLevel of spatial analysis appropriate for the program\nCensus boundary (Tract, Block Group, or Block)\nProject radius (such as a half-mile)\nWatershed\nWalkshed\n\nWeb-based Tools for Identifying Underserved Communities, CNRA\nThis guidance discusses things to consider when evaluating tools and is applicable to work at the water boards too.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores.\nThese missing values are distinct from zeros, which are also in the CES dataset. For more information about the missing (and zero) values, see the data dictionary (calenviroscreen40resultsdatadictionary_F_2021.pdf) that accompanies the CalEnviroScreen 4.0 results Excel workbook, available for download as a zip file here.\nRacial Equity Data Hub, Tools REDAP Resource Hub Draft (ca.gov), 6/2/2023.\nThings to consider when using CalEnviroScreen Scores.\n\n\nUtilizing quantitative and qualitative analyses to identify inequities and response system  effectiveness.\nVarious national level quantitative and quantitative tools exist to develop a continuum of care focusing on racial inequities and homelessness. Quantitative tools examine population data on race and ethnicity and program outcomes data. Qualitative tool include interviewing people with lived experience.\nRacial Equity Data Guidebook from the United States Interagency Council on Homelessness: https://www.usich.gov/resources/uploads/asset_library/Racial_Equity_Data_Guidebook.pdf\nInnovative tools when looking at homelessness and race. The tools could be used for other programs.\n\n\nUsing F.A.I.R. data principles\nFour foundational principles—Findability, Accessibility, Interoperability, and Reusability—that serve to guide data producers and publishers in data management and stewarship.\nOriginal principles from 2016 are available here: https://www.nature.com/articles/sdata201618%C2%A0some good documentation on how to operationalize FAIR principles for equity data is available here: https://www.nature.com/articles/s41597-022-01606-w\nData management principles.\n\n\nUsing CARE principles for Indigenous Data governance.\nThe ‘CARE Principles for Indigenous Data Governance’ address concerns related to the people and purpose of data; Collective benefit, Authority to control, Responsibility, and Ethics, and their respective sub-principles. The CARE Principles detail that the use of Indigenous data should result in tangible benefits for Indigenous collectives through inclusive development and innovation, improved governance and citizen engagement, and result in equitable outcomes.\nA good source of information on CARE including the original publication is available here: https://www.gida-global.org/care CARE and FAIR principles can go hand and hand and there is good information on that available here: https://www.nature.com/articles/s41597-021-00892-0\nData governance with a specific lens on tribal data and is applicable to broader groups of underrepresented peoples.\n\n\n\n\n\n\nIf you are a steward of Water Boards data - and it is not yet made appropriately open and accessible (including thorough documentation and complete metadata!) - you should complete the guidance on the Publish Your Data page before you begin thinking about planning for your project.\n\nBuilding a map (or other data visualization/interpretation tool) before making our own data open and accessible is not the right order. Share the we have first, then build a map or other resource showing what we think is interesting/important using that (and other) data. \n\n\n\n\nThis phase involves conducting an equity assessment (Planning) and developing your data management plan using an equity lens (Data Preparation). Best practices for this phase include:\n\nNot rushing through this phase! We often want to dive straight into “doing something” and taking the time to plan can feel like a waste of time. If done quickly and without using an equity lens, it will be time wasted. Instead, invest the time needed to do this phase well, rather than fast.\nUsing this phase to begin the process of trust and relationship building with Tribes, communities, and other expert partners interested in your project. Maybe it involves some early outreach, putting together an email list, establishing a technical advisory committee, or otherwise co-creating your planning documents with the communities that will be most impacted by the project’s implementation.\nMake the products developed during this phase as open, transparent, and accessible as possible and appropriate. This doesn’t necessarily mean that every single thing should be made public - but it does mean seriously considering what can be made open, to whom, and when.\nUse the best available science while relying on current, generally accepted Agency procedures for conducting risk assessments, economic or other technical analysis. But also understand that just because something has “always been done this way” does not automatically make it the best science, method, or process available. Be open to integrating other ways of knowing that might be different from “western science” or our business as usual methods or processes, such as the traditional knowledge landscape, and non-tribal expertise that stems from lived experiences.\n\n\n\n\nThis phase involves collecting the data and information you need for your project (Data Collection) and making it tidy so that is ready to be used in your analyses or product development steps (Data Processing). Best practices for this phase include:\n\nUse already existing and available frameworks and data, supplementing as appropriate.\nCarefully select and justify the choice of the geographic unit you will use in your analysis and discuss any particular challenges or potential aggregation issues related to the choice of spatial scale.\nKeep data disaggregated so you are able to reveal important spatial differences during your analysis phase (e.g., demographic information for each facility/place) when feasible and appropriate.\nPrepare to invest the time required to tidy the data needed for your project. It’s commonly understood in the data science field that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003) - expect the same will be true for your project and prepare to invest time and resources accordingly.\n\n\n\n\nRemember - this phase is all about…. Best practices for this phase include:\n\nUse the highest quality and most recent data available.\nCarefully select and justify the choice of a comparison population group.\nAnalyze and compare effects in baseline and across policy scenarios to show differences in effects.\nWhen data allow, characterize the distribution of risks, exposures, or outcomes within each population group, instead of presenting only average effects.\nPresent summary metrics for relevant population groups of concern as well as the comparison population group. \nBe consistent with the basic assumptions underlying other parts of the analysis, such as using the same baseline and option scenarios.\n\n\n\n\nRemember - this phase is all about…. Best practices for this phase include:\n\n\n\nRemember - this phase is all about…. Best practices for this phase include:\n\n\n\nRemember - this phase is all about…. Best practices for this phase include:\n\n\n\nRemember - this phase is all about…. Best practices for this phase include:\n\nDiscuss the overall quality and main limitations of the data (e.g., completeness, accuracy, validation). \nDiscuss available evidence of factors that may make population groups of concern more vulnerable to adverse effects (e.g., unique pathways; cumulative exposure from multiple stressors; and behavioral, biological, or environmental factors that increase susceptibility). \nIdentify unique considerations for subsistence populations when relevant.\nDiscuss the severity and nature of the health consequences for which differences between population groups have been analyzed. \nClearly describe data sources, assumptions, analytic techniques, and results. \nDiscuss key sources of uncertainty or potential biases in the data (e.g., sample size, using proximity as a surrogate for exposure) and how they may influence results.\nWhen possible, conduct sensitivity analysis for key assumptions or parameters that may affect findings. \nMake elements of Environmental Justice (EJ) assessments as straightforward and easy for the public to understand as possible.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#best-practices-table",
    "href": "get-started/best-practice.html#best-practices-table",
    "title": "Best Practices",
    "section": "",
    "text": "Practice\nDescription\nSource\nNotes\n\n\n\n\nIt is helpful to survey a range of existing tools when considering the mapping of “disadvantaged” populations.\nEach presents strengths and weaknesses to consider while keeping in mind that ultimately decisions will be driven by:\n\nStatutory mandates and priorities of each unique program\nData availability, currency, longevity\nLevel of spatial analysis appropriate for the program\nCensus boundary (Tract, Block Group, or Block)\nProject radius (such as a half-mile)\nWatershed\nWalkshed\n\nWeb-based Tools for Identifying Underserved Communities, CNRA\nThis guidance discusses things to consider when evaluating tools and is applicable to work at the water boards too.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores.\nThese missing values are distinct from zeros, which are also in the CES dataset. For more information about the missing (and zero) values, see the data dictionary (calenviroscreen40resultsdatadictionary_F_2021.pdf) that accompanies the CalEnviroScreen 4.0 results Excel workbook, available for download as a zip file here.\nRacial Equity Data Hub, Tools REDAP Resource Hub Draft (ca.gov), 6/2/2023.\nThings to consider when using CalEnviroScreen Scores.\n\n\nUtilizing quantitative and qualitative analyses to identify inequities and response system  effectiveness.\nVarious national level quantitative and quantitative tools exist to develop a continuum of care focusing on racial inequities and homelessness. Quantitative tools examine population data on race and ethnicity and program outcomes data. Qualitative tool include interviewing people with lived experience.\nRacial Equity Data Guidebook from the United States Interagency Council on Homelessness: https://www.usich.gov/resources/uploads/asset_library/Racial_Equity_Data_Guidebook.pdf\nInnovative tools when looking at homelessness and race. The tools could be used for other programs.\n\n\nUsing F.A.I.R. data principles\nFour foundational principles—Findability, Accessibility, Interoperability, and Reusability—that serve to guide data producers and publishers in data management and stewarship.\nOriginal principles from 2016 are available here: https://www.nature.com/articles/sdata201618%C2%A0some good documentation on how to operationalize FAIR principles for equity data is available here: https://www.nature.com/articles/s41597-022-01606-w\nData management principles.\n\n\nUsing CARE principles for Indigenous Data governance.\nThe ‘CARE Principles for Indigenous Data Governance’ address concerns related to the people and purpose of data; Collective benefit, Authority to control, Responsibility, and Ethics, and their respective sub-principles. The CARE Principles detail that the use of Indigenous data should result in tangible benefits for Indigenous collectives through inclusive development and innovation, improved governance and citizen engagement, and result in equitable outcomes.\nA good source of information on CARE including the original publication is available here: https://www.gida-global.org/care CARE and FAIR principles can go hand and hand and there is good information on that available here: https://www.nature.com/articles/s41597-021-00892-0\nData governance with a specific lens on tribal data and is applicable to broader groups of underrepresented peoples.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#publish-water-boards-data",
    "href": "get-started/best-practice.html#publish-water-boards-data",
    "title": "Best Practices",
    "section": "",
    "text": "If you are a steward of Water Boards data - and it is not yet made appropriately open and accessible (including thorough documentation and complete metadata!) - you should complete the guidance on the Publish Your Data page before you begin thinking about planning for your project.\n\nBuilding a map (or other data visualization/interpretation tool) before making our own data open and accessible is not the right order. Share the we have first, then build a map or other resource showing what we think is interesting/important using that (and other) data.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#plan-prepare",
    "href": "get-started/best-practice.html#plan-prepare",
    "title": "Best Practices",
    "section": "",
    "text": "This phase involves conducting an equity assessment (Planning) and developing your data management plan using an equity lens (Data Preparation). Best practices for this phase include:\n\nNot rushing through this phase! We often want to dive straight into “doing something” and taking the time to plan can feel like a waste of time. If done quickly and without using an equity lens, it will be time wasted. Instead, invest the time needed to do this phase well, rather than fast.\nUsing this phase to begin the process of trust and relationship building with Tribes, communities, and other expert partners interested in your project. Maybe it involves some early outreach, putting together an email list, establishing a technical advisory committee, or otherwise co-creating your planning documents with the communities that will be most impacted by the project’s implementation.\nMake the products developed during this phase as open, transparent, and accessible as possible and appropriate. This doesn’t necessarily mean that every single thing should be made public - but it does mean seriously considering what can be made open, to whom, and when.\nUse the best available science while relying on current, generally accepted Agency procedures for conducting risk assessments, economic or other technical analysis. But also understand that just because something has “always been done this way” does not automatically make it the best science, method, or process available. Be open to integrating other ways of knowing that might be different from “western science” or our business as usual methods or processes, such as the traditional knowledge landscape, and non-tribal expertise that stems from lived experiences.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#collect-process",
    "href": "get-started/best-practice.html#collect-process",
    "title": "Best Practices",
    "section": "",
    "text": "This phase involves collecting the data and information you need for your project (Data Collection) and making it tidy so that is ready to be used in your analyses or product development steps (Data Processing). Best practices for this phase include:\n\nUse already existing and available frameworks and data, supplementing as appropriate.\nCarefully select and justify the choice of the geographic unit you will use in your analysis and discuss any particular challenges or potential aggregation issues related to the choice of spatial scale.\nKeep data disaggregated so you are able to reveal important spatial differences during your analysis phase (e.g., demographic information for each facility/place) when feasible and appropriate.\nPrepare to invest the time required to tidy the data needed for your project. It’s commonly understood in the data science field that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003) - expect the same will be true for your project and prepare to invest time and resources accordingly.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#assure-analyze",
    "href": "get-started/best-practice.html#assure-analyze",
    "title": "Best Practices",
    "section": "",
    "text": "Remember - this phase is all about…. Best practices for this phase include:\n\nUse the highest quality and most recent data available.\nCarefully select and justify the choice of a comparison population group.\nAnalyze and compare effects in baseline and across policy scenarios to show differences in effects.\nWhen data allow, characterize the distribution of risks, exposures, or outcomes within each population group, instead of presenting only average effects.\nPresent summary metrics for relevant population groups of concern as well as the comparison population group. \nBe consistent with the basic assumptions underlying other parts of the analysis, such as using the same baseline and option scenarios.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#preserve-store",
    "href": "get-started/best-practice.html#preserve-store",
    "title": "Best Practices",
    "section": "",
    "text": "Remember - this phase is all about…. Best practices for this phase include:",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#publish-share",
    "href": "get-started/best-practice.html#publish-share",
    "title": "Best Practices",
    "section": "",
    "text": "Remember - this phase is all about…. Best practices for this phase include:",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#discover-integrate",
    "href": "get-started/best-practice.html#discover-integrate",
    "title": "Best Practices",
    "section": "",
    "text": "Remember - this phase is all about…. Best practices for this phase include:",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/best-practice.html#describe",
    "href": "get-started/best-practice.html#describe",
    "title": "Best Practices",
    "section": "",
    "text": "Remember - this phase is all about…. Best practices for this phase include:\n\nDiscuss the overall quality and main limitations of the data (e.g., completeness, accuracy, validation). \nDiscuss available evidence of factors that may make population groups of concern more vulnerable to adverse effects (e.g., unique pathways; cumulative exposure from multiple stressors; and behavioral, biological, or environmental factors that increase susceptibility). \nIdentify unique considerations for subsistence populations when relevant.\nDiscuss the severity and nature of the health consequences for which differences between population groups have been analyzed. \nClearly describe data sources, assumptions, analytic techniques, and results. \nDiscuss key sources of uncertainty or potential biases in the data (e.g., sample size, using proximity as a surrogate for exposure) and how they may influence results.\nWhen possible, conduct sensitivity analysis for key assumptions or parameters that may affect findings. \nMake elements of Environmental Justice (EJ) assessments as straightforward and easy for the public to understand as possible.",
    "crumbs": [
      "Getting Started",
      "Best Practices"
    ]
  },
  {
    "objectID": "get-started/common-language.html",
    "href": "get-started/common-language.html",
    "title": "Establishing Common Language",
    "section": "",
    "text": "When working with a racial equity lens we suggest establishing a common language and definitions to cultivate a collective understanding of underlying concepts and historical context. Creating and agreeing upon a common language can help foster transparency, challenge assumptions, and center the voices of marginalized communities; yet the efficiency of these efforts hinges on a shared language that facilitates understanding and collaboration. By grounding discussions in a common language, we can build trust and empower our team and community.\nEstablishing a common language and definitions are critical to creating a shared understanding, however we acknowledge that language can be used deliberately to engage and support community anti-racism coalitions and initiatives, or to inflame and divide them. It is important to note that although the language in this Handbook may be commonly used, the list of terms herein is not exhaustive, and may not be the sole definition of a term, and some may disagree with the definitions and their use. More specifically, in this resource we intentionally use the acronym BIPoC (Black, Indigenous, People of Color) as a term that seeks to recognize the unique experience of Black and Indigenous People within the United States. We recognize that naming is power, and we remain committed to using language that supports pro-Blackness and Native visibility, while dismantling white supremacy. \n\n\nBelow are a set of key terms and definitions provided by the Water Board Racial Equity Team in the development of the Racial Equity Resolution and Racial Equity Action Plan and are those that we adhere to in this document. (citations can be found here). For a more comprehensive list of equity-related terms, see the Racial Equity Tools Glossary\n\nDisadvantaged Community (DAC) is defined by the California Water Code as a community in which the median household income (MHI) is less than 80 percent of the statewide annual MHI (CA Water Code Section 13149.2 and 79505.5(a)). At the time this document was developed, the statewide annual MHI from the U.S. Census Bureau from 2017-2021 was $84,097 (U.S. Census Bureau QuickFacts: California). Based on this data, a community with a household income of less than $67,278 qualifies as a DAC based on the Water Code definition.\nEquality describes circumstances in which each individual or group is given the same or equal treatment, including the same resources, opportunities, and support. However, because different individuals or groups have different histories, needs, and circumstances, they do not have equal positions in society or starting points. Providing the same resources, support, or treatment does not guarantee that everyone will have fair or equal outcomes.\nEthnicity is a term used to describe subgroups of a population that share characteristics such as language, values, behavioral patterns, history, and ancestral geographical base. Social scientists often use the terms ethnicity and ethnic group to avoid the perception of biological significance associated with race.\nIntersectionality is a term used to describe the complex, cumulative way in which the effects of multiple forms of discrimination (such as racism, sexism, and classism) combine, overlap and intersect especially in the experiences of marginalized individuals or groups.\nInstitutional racism describes the ways in which policies and practices perpetuated by institutions, including governments and private groups, produce different outcomes for different racial groups in a manner that benefits the dominant group. In the United States, institutional racism includes policies that may not mention race but still result in benefiting white people over people of color.\nRace is a social construct used to categorize humans into groups based on combinations of shared physical traits such as skin color, hair texture, nose shape, eye shape, or head shape. Although most scientists agree that such groupings lack biological meaning, racial groups continue to have a strong influence over contemporary social relations. Historically in the United States, race has frequently been used to concentrate power with white people and legitimize dominance over non-white people.\nRacial equity means Race can no longer be used to predict life outcomes and outcomes for all groups are improved. For example, when we hold income constant, there are still large inequities based on race across multiple indicators for success, including the environment, education, jobs, incarceration, health and housing. \nRacism is any prejudice against someone because of their race when systems of power reinforce those views. \nStructural racism is the normalization and legitimization of an array of historical, cultural, institutional, and interpersonal dynamics that routinely advantage whites while producing cumulative and chronic adverse outcomes for people of color. Structural racism encompasses the entire system of white domination, diffused, and infused in all aspects of society, including its history, culture, politics, economics, and whole social fabric. Structural racism is more difficult to locate in a particular institution because it involves the reinforcing effects of multiple institutions and cultural norms, past and present, continually reproducing old and producing new forms of racism. Structural racism is the most profound and pervasive form of racism; all other forms of racism emerge from structural racism.\nSystemic racism can be said to encompass both institutional and structural racism. Glenn Harris, president of Race Forward, defines systemic racism as “the complex interaction of culture, policy and institutions that holds in place the outcomes we see in our lives.”  The legacy of systemic racism can be seen in a variety of outcomes affecting people of color, such as housing insecurity, a ten-fold wealth gap between white and Black or Latinx households, a dramatic over-representation of people of color in prison, and disparities in education, health, and exposure to environmental pollution.\n\n\n\n\nBelow are a set of common terms that are non-inclusive particularly to Black and Indigenous People and are commonly used in the workplace that have . This is not an exhaustive list but we recommend that research be conducted to review non-inclusive terms that may be used and have a history prior to engaging with communities. A good resource to start with is the University of Arizona’s Antiracist language guide.\n\nBlacklist - Blacklist typically refers to the ostracizing of a person, group, or organization that prevents them from participating in specific activities or spaces. This issue with the word blacklist is the association of the color with negative, evil, or wrong, and the racist undertones associated with it, and can be harmful.\n\nAlternatives: blocked, closed off, inaccessible, blocked list, banned, or closed list\n\nBrown Bag - The term “brown bag” has a historical connotation with creating an exclusive gathering that required attendees to have a lighter skin tone than a brown paper bag to participate and gain access.\n\nAlternatives: lunch in, lunch and learn, presentation, seminar\n\nChief - this term is used throughout the Water Boards to indicate positions and job titles.  This term is appropriated from the Indigenous Peoples of North America and should be avoided wherever possible.\n\nAlternatives: manager, lead, head\n\nGrandfathered in - The American South created absurd voting requirements that targeted Black people and made it almost impossible to vote. The name for these requirements is the “Grandfather Clause.” They wrote the Amendment in a way to imply the practice was not discriminatory. They created stringent new voter requirements such as literacy tests. These requirements did not apply to people who had voted before 1867. Slaves did not know they were free until June 19, 1865. However, slavery was abolished on January 1, 1863, making it nearly impossible for a person formally kept in captivity to be legally allowed to vote.\n\nAlternatives:  legacied, exempted, preapproved\n\nMaster ____ - using the term “master” to describe something that is the main or centralized source of information is inappropriate due to the connotations associated with slavery.\n\nAlternatives: primary, main\n\nPow wow - Social gatherings for ceremonial and celebratory purposes conducted under strict protocols. Avoid using the phrase to refer to a quick business meeting or informal social gathering as this is a form of cultural appropriation.\n\nAlternatives: meeting, gathering, or huddle\n\nSpirit Animal - These are spiritual guides that take the form of animals often viewed as sacred in tribal cultures. Non-native people appropriate the term to relate themselves to an animal, inanimate object, or person and draw parallels between the person and object’s characteristics. For example, saying that a sloth is your spirit animal because you are slow, lazy, and/or sleepy.\n\nAlternatives: patronus, kindred spirit, reason for living, muse, guide, or familiar.\n\nTotem Pole - Pieces of wood carved with a person’s totems. It is a tradition particular to Native and Indigenous people on the Northwest Coast. They tend to convey a family or tribe’s history. Avoid using phrases like “low on the totem pole” or “climbing the totem pole” as these are forms of cultural appropriation. These phrases are also inaccurate because in some First Nation communities being lower on the totem pole is a higher honor.\n\nAlternatives: climbing the corporate ladder, the lowest rung on the latter, least significant, or promotion.\n\nWhite Paper - while this term is widely used to describe an authoritative document, the term has historical implications that evoke negative associations especially with Tribes. \n\nAlternatives: Issue paper, briefing document, prospectus",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "get-started/common-language.html#glossary",
    "href": "get-started/common-language.html#glossary",
    "title": "Establishing Common Language",
    "section": "",
    "text": "Below are a set of key terms and definitions provided by the Water Board Racial Equity Team in the development of the Racial Equity Resolution and Racial Equity Action Plan and are those that we adhere to in this document. (citations can be found here). For a more comprehensive list of equity-related terms, see the Racial Equity Tools Glossary\n\nDisadvantaged Community (DAC) is defined by the California Water Code as a community in which the median household income (MHI) is less than 80 percent of the statewide annual MHI (CA Water Code Section 13149.2 and 79505.5(a)). At the time this document was developed, the statewide annual MHI from the U.S. Census Bureau from 2017-2021 was $84,097 (U.S. Census Bureau QuickFacts: California). Based on this data, a community with a household income of less than $67,278 qualifies as a DAC based on the Water Code definition.\nEquality describes circumstances in which each individual or group is given the same or equal treatment, including the same resources, opportunities, and support. However, because different individuals or groups have different histories, needs, and circumstances, they do not have equal positions in society or starting points. Providing the same resources, support, or treatment does not guarantee that everyone will have fair or equal outcomes.\nEthnicity is a term used to describe subgroups of a population that share characteristics such as language, values, behavioral patterns, history, and ancestral geographical base. Social scientists often use the terms ethnicity and ethnic group to avoid the perception of biological significance associated with race.\nIntersectionality is a term used to describe the complex, cumulative way in which the effects of multiple forms of discrimination (such as racism, sexism, and classism) combine, overlap and intersect especially in the experiences of marginalized individuals or groups.\nInstitutional racism describes the ways in which policies and practices perpetuated by institutions, including governments and private groups, produce different outcomes for different racial groups in a manner that benefits the dominant group. In the United States, institutional racism includes policies that may not mention race but still result in benefiting white people over people of color.\nRace is a social construct used to categorize humans into groups based on combinations of shared physical traits such as skin color, hair texture, nose shape, eye shape, or head shape. Although most scientists agree that such groupings lack biological meaning, racial groups continue to have a strong influence over contemporary social relations. Historically in the United States, race has frequently been used to concentrate power with white people and legitimize dominance over non-white people.\nRacial equity means Race can no longer be used to predict life outcomes and outcomes for all groups are improved. For example, when we hold income constant, there are still large inequities based on race across multiple indicators for success, including the environment, education, jobs, incarceration, health and housing. \nRacism is any prejudice against someone because of their race when systems of power reinforce those views. \nStructural racism is the normalization and legitimization of an array of historical, cultural, institutional, and interpersonal dynamics that routinely advantage whites while producing cumulative and chronic adverse outcomes for people of color. Structural racism encompasses the entire system of white domination, diffused, and infused in all aspects of society, including its history, culture, politics, economics, and whole social fabric. Structural racism is more difficult to locate in a particular institution because it involves the reinforcing effects of multiple institutions and cultural norms, past and present, continually reproducing old and producing new forms of racism. Structural racism is the most profound and pervasive form of racism; all other forms of racism emerge from structural racism.\nSystemic racism can be said to encompass both institutional and structural racism. Glenn Harris, president of Race Forward, defines systemic racism as “the complex interaction of culture, policy and institutions that holds in place the outcomes we see in our lives.”  The legacy of systemic racism can be seen in a variety of outcomes affecting people of color, such as housing insecurity, a ten-fold wealth gap between white and Black or Latinx households, a dramatic over-representation of people of color in prison, and disparities in education, health, and exposure to environmental pollution.",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "get-started/common-language.html#non-inclusive-terms-to-avoid",
    "href": "get-started/common-language.html#non-inclusive-terms-to-avoid",
    "title": "Establishing Common Language",
    "section": "",
    "text": "Below are a set of common terms that are non-inclusive particularly to Black and Indigenous People and are commonly used in the workplace that have . This is not an exhaustive list but we recommend that research be conducted to review non-inclusive terms that may be used and have a history prior to engaging with communities. A good resource to start with is the University of Arizona’s Antiracist language guide.\n\nBlacklist - Blacklist typically refers to the ostracizing of a person, group, or organization that prevents them from participating in specific activities or spaces. This issue with the word blacklist is the association of the color with negative, evil, or wrong, and the racist undertones associated with it, and can be harmful.\n\nAlternatives: blocked, closed off, inaccessible, blocked list, banned, or closed list\n\nBrown Bag - The term “brown bag” has a historical connotation with creating an exclusive gathering that required attendees to have a lighter skin tone than a brown paper bag to participate and gain access.\n\nAlternatives: lunch in, lunch and learn, presentation, seminar\n\nChief - this term is used throughout the Water Boards to indicate positions and job titles.  This term is appropriated from the Indigenous Peoples of North America and should be avoided wherever possible.\n\nAlternatives: manager, lead, head\n\nGrandfathered in - The American South created absurd voting requirements that targeted Black people and made it almost impossible to vote. The name for these requirements is the “Grandfather Clause.” They wrote the Amendment in a way to imply the practice was not discriminatory. They created stringent new voter requirements such as literacy tests. These requirements did not apply to people who had voted before 1867. Slaves did not know they were free until June 19, 1865. However, slavery was abolished on January 1, 1863, making it nearly impossible for a person formally kept in captivity to be legally allowed to vote.\n\nAlternatives:  legacied, exempted, preapproved\n\nMaster ____ - using the term “master” to describe something that is the main or centralized source of information is inappropriate due to the connotations associated with slavery.\n\nAlternatives: primary, main\n\nPow wow - Social gatherings for ceremonial and celebratory purposes conducted under strict protocols. Avoid using the phrase to refer to a quick business meeting or informal social gathering as this is a form of cultural appropriation.\n\nAlternatives: meeting, gathering, or huddle\n\nSpirit Animal - These are spiritual guides that take the form of animals often viewed as sacred in tribal cultures. Non-native people appropriate the term to relate themselves to an animal, inanimate object, or person and draw parallels between the person and object’s characteristics. For example, saying that a sloth is your spirit animal because you are slow, lazy, and/or sleepy.\n\nAlternatives: patronus, kindred spirit, reason for living, muse, guide, or familiar.\n\nTotem Pole - Pieces of wood carved with a person’s totems. It is a tradition particular to Native and Indigenous people on the Northwest Coast. They tend to convey a family or tribe’s history. Avoid using phrases like “low on the totem pole” or “climbing the totem pole” as these are forms of cultural appropriation. These phrases are also inaccurate because in some First Nation communities being lower on the totem pole is a higher honor.\n\nAlternatives: climbing the corporate ladder, the lowest rung on the latter, least significant, or promotion.\n\nWhite Paper - while this term is widely used to describe an authoritative document, the term has historical implications that evoke negative associations especially with Tribes. \n\nAlternatives: Issue paper, briefing document, prospectus",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "use-cases/index.html",
    "href": "use-cases/index.html",
    "title": "Use Cases",
    "section": "",
    "text": "There is no single correct way to operationalize equity in a data project. Each project has it’s own objectives, data, and audience - and truly operationalizing equity in your data work means dedicating the time, attention, and resources needed to meet the needs of the project (i.e. avoid one-size fits all approaches).\nIn this Use Cases section, you will find some examples of how some Water Boards staff have approached operationalizing equity into their data-intensive work, including use cases from:\n\nThe Statewide Surface Water Ambient Monitoring Program (SWAMP)\n&lt;ADD OTHERS HERE!&gt;\n\n\n\n\n\n\n\nHelp us highlight your data equity work!\n\n\n\nIf you are the lead of a data-intensive project that has worked to operationalize equity into some or all phases of your project’s data life cycle, and would like to add a page to this Handbook to highlight the work - let us know!\nPlease email equitydatahelp@waterboards.ca.gov with a short description of your project and what you’d like to highlight!",
    "crumbs": [
      "Use Cases"
    ]
  },
  {
    "objectID": "use-cases/swamp.html",
    "href": "use-cases/swamp.html",
    "title": "SWAMP",
    "section": "",
    "text": "SWAMP\nchange page title to short description of use case",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "collect-process/index.html",
    "href": "collect-process/index.html",
    "title": "Collect & Process",
    "section": "",
    "text": "Collect & Process\nAt this phase, you should have already completed your equity assessment (Planning) and developed your data management plan using an equity lens (Data Preparation). Now you’re ready to begin the data work!\nThis phase of the data life cycle involves gathering the data you identified during the Plan & Prep phase (Data Collection) and getting it ready so it can be used in your analyses or product development steps (Data Processing).\nAlso see a short list of best practices for this phase.",
    "crumbs": [
      "Collect & Process"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "Data Sharing",
    "section": "",
    "text": "Data Sharing\nadd guidance on open/sharing practices & locations - GitHub, Open Data Portals, etc.",
    "crumbs": [
      "Publish & Share"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here you will find a curated list of presentations, webpages and other resources related to the development, implementation and scaling of the principles and practices outlined in this Data Equity Handbook.\nAll Water Boards authors are bolded below.\n\n\n\nWater Boards Developed Racial Equity Data Tools\nWater Boards Environmental Justice Roundtable Resource Catalog\nGARE Racial Equity Toolkit: An Opportunity to Operationalize Equity\nBeyond Compliance Network Advocacy Toolkit\nAcademic Data Science Alliance Data Science Ethos\nCA Water Boards College of Water Informatics Data Toolkit\n500 Women Scientists Guide for Inclusive Scientific Meetings\nCity of Oakland | OakDOT Geographic Equity Toolbox (oaklandca.gov)\nCity of Portland Racial Equity Toolkit: https://www.portlandoregon.gov/oehr/71685\nPierce County (Washington State) utilizes an Equity Index mapping tool to help inform diversity, equity, and inclusion considerations throughout County decision-making.\nThe City of Seattle utilizes a Racial and Social Equity Index that many departments use for prioritization and evaluation/analysis. A user guide is also available.\nNew York City government’s online interactive equity data tool EquityNYC, including a Mapping Equity tool.\nThe Water Boards Office of Research Planning and Performance Water Conservation and Urban Water folks tool\nThe Water Boards Office of Research Planning and Performance Vulnerable Communities Platform\nRacial Equity Workforce Planning (CalEPA) - Practices to Advance Racial Equity in Workforce Planning (ca.gov)\nCreate a social equity index to improve public health using ArcGIS\nCaltrans Beta Equity Index Map\nWater Boards Division of Drinking Water SAFER Outreach Tool\n\n\n\n\n\nhttps://learn.arcgis.com/en/projects/create-a-social-equity-index-to-improve-public-health/\nExtend_the_Reach_of_Your_GIS (1).pdf\nAdditional best practices from Esri on improving the quality of content within a Hub site.\nAlso worth noting that ArcGIS Online comes some general categories that are derived from ISO and INSPIRE. Those could be good starting points, but my hunch is that you’re going to want to add additional categories to better support filtering and finding the right datasets.\n\n\n\n\n\nGovOps medium article on Data Ethics\nFederal data ethics framework\nOperationalizing the CARE and FAIR Principles for Indigenous data futures\nThe Data Ethics Canvas is designed to help identify potential ethical issues associated with a data project or activity. It promotes understanding and debate around the foundation, intention and potential impact of any piece of work, and helps identify the steps needed to act ethically\nEthics Framework (CalData); Framework at a Glance\nFAIR data principles for community centered data formats. Research practices documents related to using data for equity - working with communities to find data to use in equity work, and how to best collect and use demographic (ethnic and racial equity, sexual orientation, and gender identity data, specifically).\nArticle - Mapping for Whom? Communities of Color and the Citizen Science Gap\nGlobal Indigenous Digital Alliance guidance on using CARE principles for Indigenous data governance\nAt the most basic level, taking an equity lens in Data Wise involves having a team ask itself probing questions at each step of the improvement process. This document provides a look at how to use a data wise techniques with a racial equity lens.\nCollecting/using demographic data\n\n\n\n\nAnalyzing Water Boards and Demographic Data for Equity. Jun 2024. Hannah Cushman Garland. State Water Board Racial Equity Data Subcommittee Webinar. Recording | Download and Use the Code | View Code\n\n\n\n\n\n\n\n\nGovernor Executive Order on Racial Equity: https://www.gov.ca.gov/wp-content/uploads/2022/09/9.13.22-EO-N-16-22-Equity.pdf?emrc=c11513\nEquity in the State Revolving Fund use for infrastructure: https://www.policyinnovation.org/publications/a-fairer-funding-stream\n\n\n\n\n\nRacial Equity Resolution (2021) - English, Español\nRacial Equity Action Plan (2023) - English, Español\nRacial Equity Resolution Annotated References (2021)\n\n\n\n\n\nCalifornia Data Strategy Report (2024)\nCalifornia Data Strategy (2020)\n\n\n\n\n\nConsidering Equity in Community Impact Analysis for Projects https://dot.ca.gov/-/media/dot-media/programs/environmental-analysis/documents/ser/equity-guidance-a11y.pdf\n\n\n\n\n\nCalEnviroScreen 4.0 and how race and ethnicity is analyzed within that tool and the CES score correlations: https://oehha.ca.gov/media/downloads/calenviroscreen/document/calenviroscreen40raceanalysisf2021.pdf\n\n\n\n\n\n\nCity of Oakland | OakDOT Geographic Equity Toolbox (oaklandca.gov)\nCity of Portland Racial Equity Toolkit: https://www.portlandoregon.gov/oehr/71685\nPierce County (Washington State) utilizes an Equity Index mapping tool to help inform diversity, equity, and inclusion considerations throughout County decision-making.\nThe City of Seattle utilizes a Racial and Social Equity Index that many departments use for prioritization and evaluation/analysis. A user guide is also available.\nNew York City government’s online interactive equity data tool EquityNYC, including a Mapping Equity tool.\n\n\n\n\n\nMN DOT Governance and Best Practices: A tailored document like this would be useful to encourage contributors to follow the same guidelines and understand which groups or tags need to be added for the data to be ingested properly. - MnDOT Governance and Best Practices and User Requirements (1).docx\n\n\n\n\n\nThe Environmental Justice Science, Data, and Research Plan (2024)\nYear of Open Science Fact Sheet (2023)\nhttps://www.census.gov/programs-surveys/acs/data.html - The American Community Survey (ACS) releases new data every year through a variety of data tables that you can access with different data tools.\nhttps://www.census.gov/programs-surveys/acs/library/handbook - You can use American Community Survey (ACS) data in different ways and for different reasons. Each one of our downloadable PDF handbooks helps a particular group with specific how-to instructions and/or case studies.\nUS EPA guidance for Assessing Environmental Justice in Regulatory Analysis https://www.epa.gov/system/files/documents/2023-11/ejtg_revision_110823_508compliant_0.pdf\nGuide from the U.S. Department of Health and Human Services on how to conduct an equity assessment of a policy or program: In-Depth Equity Assessment Guide | ASPE (hhs.gov)\nFACT SHEET: Biden-⁠Harris Administration Releases Annual Agency Equity Action Plans to Further Advance Racial Equity and Support for Underserved Communities Through the Federal Government\nhttps://www.usich.gov/resources/uploads/asset_library/Racial_Equity_Data_Guidebook.pdf (link broken?) - The U.S. Interagency Council on Homelessness (USICH) is the only federal agency with a sole mission focused on preventing and ending homelessness in America. The council consists of 19 federal agencies that help create and catalyze implementation of the Federal Strategic Plan to Prevent and End Homelessness.\nhttps://www.whitehouse.gov/wp-content/uploads/2022/04/eo13985-vision-for-equitable-data.pdf - From the White House Equitable Data Working Group\nhttps://cssp.org/wp-content/uploads/2021/09/Our-Identities-Ourselves-Project-Infographic-FINAL.pdf - Center for the Study of Social Policy. This is the first phase of a larger project whose goal is to share best practices for collecting data about race, ethnicity, and other demographic information including sexual orientation, gender identity and expression (SOGIE); national origin; language spoken; disability; religion; and tribal affiliation in child welfare agencies nationwide.\nCDC guidance: Principles for Using Public Health Data to Drive Equity A guide to embedding equitable practices throughout the data life cycle\nFederal Register :: Initial Proposals For Updating OMB’s Race and Ethnicity Statistical Standards\nActionable Intelligence for Social Policy (AISP) Centering for Equity Toolkit: https://aisp.upenn.edu/resource-article/a-toolkit-for-centering-racial-equity-throughout-data-integration/\nUrban Institute’s Principles for Advancing Equitable Data Practice: https://www.urban.org/sites/default/files/publication/102346/principles-for-advancing-equitable-data-practice.pdf\nGeneral Data Management resource hub from the National Institute of Health that includes FAIR principles, metadata practices etc. https://sharing.nih.gov/data-management-and-sharing-policy/data-management\nHero Database with Racial Equity Resources collated from the National : https://hero.epa.gov/hero/index.cfm/project/page/project_id/3611\nhttps://nationalequityatlas.org/ The National Equity Atlas is America’s most detailed report card on racial and economic equity.\nUS EPA has guidance for content metadata in ArcGIS Online.\nInitial Proposals for Revising the Federal Race and Ethnicity Standards | OMB | The White House\n\n\n\n\n\nO’Brien, M., Duerr, R., Taitingfong, R., Martinez, A., Vera, L., Jennings, L.L., Downs, R.R., Antognoli, E., ten Brink, T., Halmai, N.B., David-Chavez, D., Carroll, S.R., Hudson, M. and Buttigieg, P.L. (2024) ‘Earth Science Data Repositories: Implementing the CARE Principles’, Data Science Journal, 23(1), p. 37.\n\n\n\nBaron N. (2010) Escape from the Ivory Tower: A Guide to Making Your Science Matter. Island Press\nD’Ignazio C., Klein L. F. (2020) Data Feminism. The MIT Press\nLeonardi P., Neeley T. (2022) The Digital Mindset: What It Really Takes to Thrive in the Age of Data, Algorithms, and AI. Harvard Business Review Press\nPahlka J. (2023) Recoding America : why government is failing in the digital age and how we can do better. Metropolitan Books\n\n\n\n\nRacial Equity at the Water Boards\nOpenscapes at the Water Boards\nCalifornia Office of Data and Innovation\nGovernment Alliance on Race and Equity (GARE)\nCalifornia Indian Water Rights Study",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#other-equity-data-handbooks-toolkits-guides",
    "href": "resources.html#other-equity-data-handbooks-toolkits-guides",
    "title": "Resources",
    "section": "",
    "text": "Water Boards Developed Racial Equity Data Tools\nWater Boards Environmental Justice Roundtable Resource Catalog\nGARE Racial Equity Toolkit: An Opportunity to Operationalize Equity\nBeyond Compliance Network Advocacy Toolkit\nAcademic Data Science Alliance Data Science Ethos\nCA Water Boards College of Water Informatics Data Toolkit\n500 Women Scientists Guide for Inclusive Scientific Meetings\nCity of Oakland | OakDOT Geographic Equity Toolbox (oaklandca.gov)\nCity of Portland Racial Equity Toolkit: https://www.portlandoregon.gov/oehr/71685\nPierce County (Washington State) utilizes an Equity Index mapping tool to help inform diversity, equity, and inclusion considerations throughout County decision-making.\nThe City of Seattle utilizes a Racial and Social Equity Index that many departments use for prioritization and evaluation/analysis. A user guide is also available.\nNew York City government’s online interactive equity data tool EquityNYC, including a Mapping Equity tool.\nThe Water Boards Office of Research Planning and Performance Water Conservation and Urban Water folks tool\nThe Water Boards Office of Research Planning and Performance Vulnerable Communities Platform\nRacial Equity Workforce Planning (CalEPA) - Practices to Advance Racial Equity in Workforce Planning (ca.gov)\nCreate a social equity index to improve public health using ArcGIS\nCaltrans Beta Equity Index Map\nWater Boards Division of Drinking Water SAFER Outreach Tool",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#gis-resources",
    "href": "resources.html#gis-resources",
    "title": "Resources",
    "section": "",
    "text": "https://learn.arcgis.com/en/projects/create-a-social-equity-index-to-improve-public-health/\nExtend_the_Reach_of_Your_GIS (1).pdf\nAdditional best practices from Esri on improving the quality of content within a Hub site.\nAlso worth noting that ArcGIS Online comes some general categories that are derived from ISO and INSPIRE. Those could be good starting points, but my hunch is that you’re going to want to add additional categories to better support filtering and finding the right datasets.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#data-ethics-and-data-sovereignty-resources",
    "href": "resources.html#data-ethics-and-data-sovereignty-resources",
    "title": "Resources",
    "section": "",
    "text": "GovOps medium article on Data Ethics\nFederal data ethics framework\nOperationalizing the CARE and FAIR Principles for Indigenous data futures\nThe Data Ethics Canvas is designed to help identify potential ethical issues associated with a data project or activity. It promotes understanding and debate around the foundation, intention and potential impact of any piece of work, and helps identify the steps needed to act ethically\nEthics Framework (CalData); Framework at a Glance\nFAIR data principles for community centered data formats. Research practices documents related to using data for equity - working with communities to find data to use in equity work, and how to best collect and use demographic (ethnic and racial equity, sexual orientation, and gender identity data, specifically).\nArticle - Mapping for Whom? Communities of Color and the Citizen Science Gap\nGlobal Indigenous Digital Alliance guidance on using CARE principles for Indigenous data governance\nAt the most basic level, taking an equity lens in Data Wise involves having a team ask itself probing questions at each step of the improvement process. This document provides a look at how to use a data wise techniques with a racial equity lens.\nCollecting/using demographic data",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#presentations",
    "href": "resources.html#presentations",
    "title": "Resources",
    "section": "",
    "text": "Analyzing Water Boards and Demographic Data for Equity. Jun 2024. Hannah Cushman Garland. State Water Board Racial Equity Data Subcommittee Webinar. Recording | Download and Use the Code | View Code",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#relevant-government-documents",
    "href": "resources.html#relevant-government-documents",
    "title": "Resources",
    "section": "",
    "text": "Governor Executive Order on Racial Equity: https://www.gov.ca.gov/wp-content/uploads/2022/09/9.13.22-EO-N-16-22-Equity.pdf?emrc=c11513\nEquity in the State Revolving Fund use for infrastructure: https://www.policyinnovation.org/publications/a-fairer-funding-stream\n\n\n\n\n\nRacial Equity Resolution (2021) - English, Español\nRacial Equity Action Plan (2023) - English, Español\nRacial Equity Resolution Annotated References (2021)\n\n\n\n\n\nCalifornia Data Strategy Report (2024)\nCalifornia Data Strategy (2020)\n\n\n\n\n\nConsidering Equity in Community Impact Analysis for Projects https://dot.ca.gov/-/media/dot-media/programs/environmental-analysis/documents/ser/equity-guidance-a11y.pdf\n\n\n\n\n\nCalEnviroScreen 4.0 and how race and ethnicity is analyzed within that tool and the CES score correlations: https://oehha.ca.gov/media/downloads/calenviroscreen/document/calenviroscreen40raceanalysisf2021.pdf\n\n\n\n\n\n\nCity of Oakland | OakDOT Geographic Equity Toolbox (oaklandca.gov)\nCity of Portland Racial Equity Toolkit: https://www.portlandoregon.gov/oehr/71685\nPierce County (Washington State) utilizes an Equity Index mapping tool to help inform diversity, equity, and inclusion considerations throughout County decision-making.\nThe City of Seattle utilizes a Racial and Social Equity Index that many departments use for prioritization and evaluation/analysis. A user guide is also available.\nNew York City government’s online interactive equity data tool EquityNYC, including a Mapping Equity tool.\n\n\n\n\n\nMN DOT Governance and Best Practices: A tailored document like this would be useful to encourage contributors to follow the same guidelines and understand which groups or tags need to be added for the data to be ingested properly. - MnDOT Governance and Best Practices and User Requirements (1).docx\n\n\n\n\n\nThe Environmental Justice Science, Data, and Research Plan (2024)\nYear of Open Science Fact Sheet (2023)\nhttps://www.census.gov/programs-surveys/acs/data.html - The American Community Survey (ACS) releases new data every year through a variety of data tables that you can access with different data tools.\nhttps://www.census.gov/programs-surveys/acs/library/handbook - You can use American Community Survey (ACS) data in different ways and for different reasons. Each one of our downloadable PDF handbooks helps a particular group with specific how-to instructions and/or case studies.\nUS EPA guidance for Assessing Environmental Justice in Regulatory Analysis https://www.epa.gov/system/files/documents/2023-11/ejtg_revision_110823_508compliant_0.pdf\nGuide from the U.S. Department of Health and Human Services on how to conduct an equity assessment of a policy or program: In-Depth Equity Assessment Guide | ASPE (hhs.gov)\nFACT SHEET: Biden-⁠Harris Administration Releases Annual Agency Equity Action Plans to Further Advance Racial Equity and Support for Underserved Communities Through the Federal Government\nhttps://www.usich.gov/resources/uploads/asset_library/Racial_Equity_Data_Guidebook.pdf (link broken?) - The U.S. Interagency Council on Homelessness (USICH) is the only federal agency with a sole mission focused on preventing and ending homelessness in America. The council consists of 19 federal agencies that help create and catalyze implementation of the Federal Strategic Plan to Prevent and End Homelessness.\nhttps://www.whitehouse.gov/wp-content/uploads/2022/04/eo13985-vision-for-equitable-data.pdf - From the White House Equitable Data Working Group\nhttps://cssp.org/wp-content/uploads/2021/09/Our-Identities-Ourselves-Project-Infographic-FINAL.pdf - Center for the Study of Social Policy. This is the first phase of a larger project whose goal is to share best practices for collecting data about race, ethnicity, and other demographic information including sexual orientation, gender identity and expression (SOGIE); national origin; language spoken; disability; religion; and tribal affiliation in child welfare agencies nationwide.\nCDC guidance: Principles for Using Public Health Data to Drive Equity A guide to embedding equitable practices throughout the data life cycle\nFederal Register :: Initial Proposals For Updating OMB’s Race and Ethnicity Statistical Standards\nActionable Intelligence for Social Policy (AISP) Centering for Equity Toolkit: https://aisp.upenn.edu/resource-article/a-toolkit-for-centering-racial-equity-throughout-data-integration/\nUrban Institute’s Principles for Advancing Equitable Data Practice: https://www.urban.org/sites/default/files/publication/102346/principles-for-advancing-equitable-data-practice.pdf\nGeneral Data Management resource hub from the National Institute of Health that includes FAIR principles, metadata practices etc. https://sharing.nih.gov/data-management-and-sharing-policy/data-management\nHero Database with Racial Equity Resources collated from the National : https://hero.epa.gov/hero/index.cfm/project/page/project_id/3611\nhttps://nationalequityatlas.org/ The National Equity Atlas is America’s most detailed report card on racial and economic equity.\nUS EPA has guidance for content metadata in ArcGIS Online.\nInitial Proposals for Revising the Federal Race and Ethnicity Standards | OMB | The White House",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#literature",
    "href": "resources.html#literature",
    "title": "Resources",
    "section": "",
    "text": "O’Brien, M., Duerr, R., Taitingfong, R., Martinez, A., Vera, L., Jennings, L.L., Downs, R.R., Antognoli, E., ten Brink, T., Halmai, N.B., David-Chavez, D., Carroll, S.R., Hudson, M. and Buttigieg, P.L. (2024) ‘Earth Science Data Repositories: Implementing the CARE Principles’, Data Science Journal, 23(1), p. 37.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "",
    "text": "Baron N. (2010) Escape from the Ivory Tower: A Guide to Making Your Science Matter. Island Press\nD’Ignazio C., Klein L. F. (2020) Data Feminism. The MIT Press\nLeonardi P., Neeley T. (2022) The Digital Mindset: What It Really Takes to Thrive in the Age of Data, Algorithms, and AI. Harvard Business Review Press\nPahlka J. (2023) Recoding America : why government is failing in the digital age and how we can do better. Metropolitan Books",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Resources",
    "section": "",
    "text": "Racial Equity at the Water Boards\nOpenscapes at the Water Boards\nCalifornia Office of Data and Innovation\nGovernment Alliance on Race and Equity (GARE)\nCalifornia Indian Water Rights Study",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "store.html",
    "href": "store.html",
    "title": "Preservation & Storage",
    "section": "",
    "text": "Preservation & Storage",
    "crumbs": [
      "Preserve & Store"
    ]
  },
  {
    "objectID": "inspo.html",
    "href": "inspo.html",
    "title": "Inspiration",
    "section": "",
    "text": "Inspiration\nThe impetus for developing this Data Equity Handbook began in August 2020 when the State Water Resources Control Board (State Water Board) publicly acknowledged that the historical effects of institutional racism must be confronted throughout government, and it directed staff to develop a priority plan of action.\nSince then, the State Water Board, it’s Office of Information Management and Analysis (OIMA) and OIMA’s many internal and external partners have been developing and compiling material, and adding to this Data Equity Handbook as time and bandwidth allow.\nThis Data Equity Handbook is inspired by many sources, including:\n\nWater Boards Racial Equity - Resolution and Related Actions\nGovernment Alliance on Race and Equity (GARE)\nOpenscapes and their Approach Guide\nNOAA Fisheries (NMFS) Open Science Resource Book",
    "crumbs": [
      "Inspiration"
    ]
  },
  {
    "objectID": "plan-prep/prep.html",
    "href": "plan-prep/prep.html",
    "title": "Data Preparation",
    "section": "",
    "text": "As you’re in the Planning phase of conducting your Equity Assessment, we recommend documenting what you are doing for the remaining phases of the data life cycle and making that documentation as open and transparent as possible. This is best accomplished through the use of a Data Management Plan.\nA Data Management Plan describes what data will be used, how the data will be collected, processed, and managed to conduct your analysis, visualization or other products, and how those data products will be stored, shared, and maintained over the long-term. In other words, the Data Management Plan describes how the project team intends to address and manage each phase of the data life cycle.\nDepending on the complexity of the project, your data management plan can be relatively short, and it can be used as a way to begin to engage with experts and partners that are interested in your project!\n\n\n\n\n\n\nTip\n\n\n\nTry using the development of your Data Management Plan as a way to build relationships and trust with Tribal and community experts!\nSome ideas for how to do this include:\n\nYou can create a Technical Advisory group composed of Tribal and community experts that helps co-create the Data Management Plan with the Project Team.\nThe Project Team develops the Data Management Plan, but solicits feedback on early versions from Tribal and community experts and makes revisions according to their feedback.\n\n\n\nA Data Management Plan should include the following sections:\n\nProject Introduction & Context\nData Collection & Processing\nData Analysis & Product Development\nData & Product Preservation & Storage\nData & Product Publication & Sharing\nData & Product Documentation\nData & Product Evaluation\nOther Potential Sections or Potential Appendices, including: Acknowledgements, Timeline, Project Roles and Responsibilities, Dataset Details, Survey Details, Future work\n\nSince the development the Data Management Plan takes place before data are actually collected, some details, like specific analytical methods, may not be completely worked out. However, the Data Management Plan should include a clear vision and general plan for each section and include as much detail as possible.\n\n\nHere you want to briefly describe the project and the mechanism(s) driving the data collection and product development. Much of this will likely be worked out during the Planning phase, including:\n\nWhat is the purpose of the selected program, policy, or process, related to this project?\nWhat are the objectives of the project?\nWho is the intended audience of the project?\nHow do you envision the project’s resulting data and products contribute to the advancement and operationalization of equity for your the program, policy, or process, related to this project?\n\n\n\n\nIn this section, you will identify the data you plan on collecting, how you will collect it, and how you organize, manage, and process said data once it is collected. More detailed guidance on collection and processing of data and resultant products is outlined on the Data Collection (including information on Surveys), and Data Processing pages.\n\n\n\n\n\n\nBe sure to pause and think about what you actually need to answer the questions/objectives you have using an equity lens.\n\n\n\nAs a reminder - achieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you create the list of data you want to collect for your project, it should contain:\n\nData that can represent your management question(s) or project objectives. See the Planning page for more guidance.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start! See the Data Collection page for more guidance.\n\n\n\n\n\nA good plan will include information that is sufficient to understand the nature of the data that is collected, including:\n\nTypes. A good first step is to list the various types of data that you expect to collect or create. This may include text, spreadsheets, software and algorithms, models, images and movies, audio files, and patient records. \nSources. Data may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and compilations of data from other studies.\nVolume. Both the total volume of data and the total number of files that are expected to be collected can affect all other data management activities.\nData and file formats. Technology changes and formats that are acceptable today may soon be obsolete. Good choices include those formats that are nonproprietary, based upon open standards, and widely adopted and preferred by the larger data consuming community (e.g., Comma Separated Values [CSV] over Excel [.xls, xlsx]). Data are more accessible for the long term if they are uncompressed, unencrypted, and stored using standard character encodings.\n\nSome questions to help guide the development of this section include:\n\nWhat data will we be collecting and/or generating?\nHow and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?\nWhat file formats will be used? Do these formats conform to an open standard and/or are they proprietary?\nHow much data will be generated for this project?\nAre you using data that someone else produced? If so, where is it from?\nHow long will the data be collected/generated and how often will it change?\nTo what extent do the data and methods of collection and use for this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes data collection and use to better meet them?\n\n\n\n\nFAIR Principles (Findable, Accessible, Interoperable, Reusable) within the open data movement primarily focus on characteristics of data that will facilitate increased data sharing among entities while ignoring power differentials and historical contexts. CARE Principles (Collective Benefit, Authority to Control, Responsibility, Ethics) for Indigenous Data Governance are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. Image credit: Global Indigenous Data Allianc\n\n\n\n\n\nDefine how and where the data will be organized and managed.\nFor example, your effort may require a small number of data tables and these can be effectively managed with spreadsheet programs like Excel. Larger data volumes and usage constraints may require the use of relational database management systems for linked data tables like ORACLE or mySQL, or a Geographic Information System (GIS) for geospatial data layers like ArcGIS, or computer programming languages like R or Python for large datasets that cannot be contained within standard database or GIS systems.\nThis section should contain just enough detail to identify basic data organization needs and plan, not the level of detail needed to build a comprehensive system or information technology project plan.\nSome questions to help guide the development of this section include:\n\nHow and where will your data be organized?\nWhat tools or software are required to read or view the data?\nWhat directory and file naming convention will be used?\nWhat are your local storage and backup procedures?\nWill this data require secure storage?\nWho is responsible for managing the data? Who will ensure that the data management plan is carried out?\nWhat steps will be taken to protect privacy, security, confidentiality, intellectual property or other rights?\n\n\n\n\nHere you will define the processes you will use to clean and prepare your data once it is collected - also known as tidying data. Tidy data are structured such that the data are easy to manipulate, model, and visualize - and getting data to the point of being tidy is often the most time consuming step of any data-intensive work. More detailed data processing/tidying steps are outlined on the Data Processing page.\nSimilar to the analysis step - you might not know all of the details of how the data will need to be tidied - what’s important for this section is that you think through the potential methods you will need to use to make disparate datasets interoperable and tidy so that they’re of acceptable quality and easy to use for your analysis and product development steps.\nSome questions to help guide the development of this section include:\n\nWhich datasets (if any) will need to be merged/combined to be made useful for your project? If this needs to be done, how will you plan on doing it?\nWhat are your data quality objectives/standards?\nHow will you assess and establish the quality of the data you use?\nWhat rubric will you use to decide which data are kept and which are excluded from future steps?\n\n\n\n\n\n\n\nConsider how your project aligns with the Water Boards Quality Management System!\n\n\n\nThe Water Boards have a quality management system and overarching Quality Assurance Management Plan. Be sure to review this material to consider how your project falls in that framework, and if more is needed to ensure your all aspects of your project are of sufficient quality. For example, many programs do NOT have Quality Assurance Program Plans, so this may be a step needed to occur to establish data quality objectives, etc.\nIn some cases the Quality Assurance and Data Management plans can be integrated (see this USGS Quality-Assurance and Data-Management Plan as an example)\n\n\n\n\n\n\nIn this section, you will describe how you intend on using the data and the general plan or intended workflow you envision for your data analysis and/or product development phase. If you know you will use certain formulas, methods, or software for this step, you will identify them here. More detailed guidance on data analysis and product development steps are outlined on the Data Analysis and Data Visualization pages.\nSome questions to help guide the development of this section include:\n\nWhat management questions are you planning on answering or informing with this data and project?\n\nIf operational decision making is the use please identify which performance measures or existing resource allocation planning processes that will be using the data (e.g., assigning inspections to staff, determining priority for compliance assurance work, etc.).\nPlease also identify any business interests that will need to be alerted to the data / products and may have concerns over its quality, etc. For example, invoicing for fees will need to have updated information, etc.\n\nWhat workflow will you use to analyse the data and/or develop the resultant data product?\nWhat data analysis or visualization methods or software will you use?\nWhat product(s) will be developed (e.g. analyses, visualizations, applications, reports, etc.)\nWhat opportunities will Tribal and community partners have to review and provide feedback on the data analysis or product development before it is finalized?\n\n\n\n\nIn this section, you will describe how and where you plan on preserving and storing data and products once they are developed. More detailed guidance on preservation and storage of data and resultant products is outlined on the Preservation & Storage page.\nSome questions to help guide the development of this section include:\n\nHow and where will you store and secure your data and resultant products (code, results, products, visualizations, applications, etc.)?\nWhat privacy and confidentiality issues must you address?\nWhat are your plans for preserving the data/products after the project is completed?\nWhat procedures will you use to ensure long-term archiving and preservation of your data?\nAt what point will data, code/scripts, and resultant products/applications be archived or deleted?\n\n\n\n\nIn this section, you will describe how and where you plan on publishing, sharing and otherwise making accessibly the project’s data and products once they are developed. More detailed guidance on publishing and sharing data and resultant products is outlined on the Data Sharing page.\nThe Water Boards typically make virtually all of the data we collect available to the public. The exceptions are confidential information (e.g., part of ongoing enforcement actions and/or formal Tribal consultations) and some personally identifiable information (PII). This section should describe any policies that will filter out data from the step of making the data publicly available and, more importantly, how the project plans to provide access to the data.\n\n\n\n\n\n\nMake your project as open and accessible as possible and appropriate!\n\n\n\nPublishing and sharing Water Boards data and resultant products is critical for collaboration and transparency of our data, products, and workflows. Your project should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.”\nThis means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software (e.g. Python, R) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\n\n\nSome questions to help guide the development of this section include:\n\nWhat data and products will be shared, and when?\nWhere and how will data and products be made open and/or accessible?\n\nDatasets that are of high value should, at minimum, be published to the California Open Data Portal in the form of machine readable, well documented, maintained data.\nGeospatial products of high value should, at minimum, be published to the California State Geoportal.\nCode and similar products (scripts, analysis packages) should, at minimum, be published on the Water Boards GitHub in it’s own, well documented, project repository.\nFor all other data or products, please indicate how the data/product will be made accessible (e.g., via search forms at SMARTS public reports page, etc.).\n\nDoes sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?\nIf you collected data directly from Tribes or communities -\n\nHow will permission be obtained to use and disseminate the data?\nHow is informed consent being handled and how is privacy being protected?\nHow and when will you communicate what will or will not be shared?\n\nTo what extent do the methods of publication and sharing of data, products developed through this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes of publication and sharing to better meet them?\n\n\n\n\nIn this section, you will describe how and where every aspect of the project will be well documented. More detailed guidance on describing the project’s data and products is outlined on the Documentation page.\nMetadata - the details about what, where, when, why, and how the data were collected, processed, and interpreted - provide the information that enables data and files to be discovered, used, and properly cited. Metadata and other project documentation include descriptions of how data and files are named, physically structured, and stored as well as details about the experiments, analytical/visualization methods, project context, and names long-term data/product/project managers/stewards.\n\n\n\n\n\n\nThe utility and longevity of data and products relate directly to how complete and comprehensive the metadata and documentation are.\n\n\n\nThe amount of effort devoted to creating comprehensive metadata and documentation may vary substantially based on the complexity, types, and volume of data/products developed throughout the life of a project - but it’s safe to assume (and plan for) a substantial amount of time and energy will be required to develop adequate metadata and documentation.\n\n\nSome questions to help guide the development of this section include:\n\nWhat types of metadata will be produced alongside the data?\nWhat metadata standards will be used? Are you using metadata that is standard to your field?\nHow will the metadata be managed and stored?\nWhat other documentation will be developed for the project and associated products (e.g. workflows, standard operating procedures, data or product use or interpretation guidance, etc.)? Where will that be stored? How will it be made accessible and shared?\nIf you collected data or partnered directly from Tribes or communities -\n\nDoes it make sense to have these same partners review and provide feedback on your metadata and documentation materials? Doing so would help ensure that documentation is clear, simple, and accessible to a wide array of audiences.\nWhen and how will you share the aforementioned documentation with your partners?\n\n\n\n\n\nIn this section, you will describe how you will evaluate the data, products, and outcomes of the project after it is complete, to assess the extent to which the project has achieved the goals you set for it and advanced and improved equity outcomes. More detailed guidance on describing the project’s data and products is outlined on the Evaluation page.\nSome questions to help guide the development of this section include:\n\nAt what point(s) during the project’s life cycle will you conduct your evaluation? (You don’t need to wait until the project is complete to benefit from this phase!)\nWhat evaluation method(s) will you use?\nHow can the project design an equitable and inclusive evaluation?\nWill the project team share evaluation findings with the experts or other partners involved in the project? If so, with whom will you share it, and how?\nWould sharing the project’s evaluation findings with the experts or other partners who were NOT directly involved in the project further promote equity through transparency and accountability? If so, with whom will you share it, and how?\n\n\n\n\n\n\nIf the Data Management Plan was developed by a group that included external partners, we recommend including an acknowledgements section to acknowledge, express appreciation, and give credit to those efforts.\n\n\n\nIncluding a timeline for project implementation is always recommended (even though it is more of a project management tool than a data management tool) and even if specific dates are not yet known. Including a timeline helps keep ourselves accountable and makes it easier for potential partners see when their contributions, feedback, and partnership might be needed so they can plan ahead and be ready for when its their time to engage.\n\n\n\nIf there are multiple people on the team that will be involved with project implementation, it might be a good idea to define who will be responsible for which parts of the project/data life cycle so that everyone is clear on their roles and responsibilities to this project ( even though it is more of a project management tool than a data management tool).\n\n\n\n\n\n\nTip\n\n\n\nSpelling out project roles and responsibilities during this phase can help identify gaps and resource needs early!\nThis will enable the project team, management, and/or project partners to understand the limitations and dedicate time and resources to find more team members that can help fill those gaps before the project is underway. Doing this will ultimately prevent the project from being delayed, stalled, or put on hold after time, energy, and resources have already been expended (or even wasted).\n\n\nYou might include a Project Roles and Responsibilities table that includes:\n\nData Life Cycle Phase\nRole Title (e.g. project manager, data collection coordinator, data manager, data analysis lead, data product developer, project engagement lead, etc.)\nName (Affiliation)\nResponsibilities (with a short list of responsibilities associated with that role)\n\n\n\n\n\n\n\nThe goal of this section is making it easy for readers to see and understand the content of your data sources without having to view data directly. You might include a data schema for the datasets of interest. A data schema shows what the “guts” of your data will look like, including the identification of tables, columns/fields, data types, constraints, and relationships. This could be provided as a single table that includes your column/field names, and data types or something much more complex that better suits the needs of your project. For a simple example, see Appendix 1 of the SWAMP Bioassessment Reporting Module Data Management Plan.\n\n\n\nIf your project involves collecting data through a survey, you might use this section to document your intended survey questions and possible responses or response types.\n\n\n\nHere you might describe next steps or project ideas that are outside the scope and timelines of the current project, but that you see as being directly related to or building upon the current project.\n\n\n\n\n\nDMP Tool (a web-based tool that helps you construct data management plans using templates that address specific funder requirements) \nCA Healthy Watersheds Partnership Assessment Guidance: Data Acquisition, Monitoring and Management\nUSGS - Data Management Plans\nMIT Libraries - Write a Data Management Plan &\nHarvard Medical School - Data Management Plans\nUniversity of Arizona - Data Management Plans Overview",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#project-introduction-context",
    "href": "plan-prep/prep.html#project-introduction-context",
    "title": "Data Preparation",
    "section": "",
    "text": "Here you want to briefly describe the project and the mechanism(s) driving the data collection and product development. Much of this will likely be worked out during the Planning phase, including:\n\nWhat is the purpose of the selected program, policy, or process, related to this project?\nWhat are the objectives of the project?\nWho is the intended audience of the project?\nHow do you envision the project’s resulting data and products contribute to the advancement and operationalization of equity for your the program, policy, or process, related to this project?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-collection-processing",
    "href": "plan-prep/prep.html#data-collection-processing",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will identify the data you plan on collecting, how you will collect it, and how you organize, manage, and process said data once it is collected. More detailed guidance on collection and processing of data and resultant products is outlined on the Data Collection (including information on Surveys), and Data Processing pages.\n\n\n\n\n\n\nBe sure to pause and think about what you actually need to answer the questions/objectives you have using an equity lens.\n\n\n\nAs a reminder - achieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you create the list of data you want to collect for your project, it should contain:\n\nData that can represent your management question(s) or project objectives. See the Planning page for more guidance.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start! See the Data Collection page for more guidance.\n\n\n\n\n\nA good plan will include information that is sufficient to understand the nature of the data that is collected, including:\n\nTypes. A good first step is to list the various types of data that you expect to collect or create. This may include text, spreadsheets, software and algorithms, models, images and movies, audio files, and patient records. \nSources. Data may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and compilations of data from other studies.\nVolume. Both the total volume of data and the total number of files that are expected to be collected can affect all other data management activities.\nData and file formats. Technology changes and formats that are acceptable today may soon be obsolete. Good choices include those formats that are nonproprietary, based upon open standards, and widely adopted and preferred by the larger data consuming community (e.g., Comma Separated Values [CSV] over Excel [.xls, xlsx]). Data are more accessible for the long term if they are uncompressed, unencrypted, and stored using standard character encodings.\n\nSome questions to help guide the development of this section include:\n\nWhat data will we be collecting and/or generating?\nHow and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?\nWhat file formats will be used? Do these formats conform to an open standard and/or are they proprietary?\nHow much data will be generated for this project?\nAre you using data that someone else produced? If so, where is it from?\nHow long will the data be collected/generated and how often will it change?\nTo what extent do the data and methods of collection and use for this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes data collection and use to better meet them?\n\n\n\n\nFAIR Principles (Findable, Accessible, Interoperable, Reusable) within the open data movement primarily focus on characteristics of data that will facilitate increased data sharing among entities while ignoring power differentials and historical contexts. CARE Principles (Collective Benefit, Authority to Control, Responsibility, Ethics) for Indigenous Data Governance are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. Image credit: Global Indigenous Data Allianc\n\n\n\n\n\nDefine how and where the data will be organized and managed.\nFor example, your effort may require a small number of data tables and these can be effectively managed with spreadsheet programs like Excel. Larger data volumes and usage constraints may require the use of relational database management systems for linked data tables like ORACLE or mySQL, or a Geographic Information System (GIS) for geospatial data layers like ArcGIS, or computer programming languages like R or Python for large datasets that cannot be contained within standard database or GIS systems.\nThis section should contain just enough detail to identify basic data organization needs and plan, not the level of detail needed to build a comprehensive system or information technology project plan.\nSome questions to help guide the development of this section include:\n\nHow and where will your data be organized?\nWhat tools or software are required to read or view the data?\nWhat directory and file naming convention will be used?\nWhat are your local storage and backup procedures?\nWill this data require secure storage?\nWho is responsible for managing the data? Who will ensure that the data management plan is carried out?\nWhat steps will be taken to protect privacy, security, confidentiality, intellectual property or other rights?\n\n\n\n\nHere you will define the processes you will use to clean and prepare your data once it is collected - also known as tidying data. Tidy data are structured such that the data are easy to manipulate, model, and visualize - and getting data to the point of being tidy is often the most time consuming step of any data-intensive work. More detailed data processing/tidying steps are outlined on the Data Processing page.\nSimilar to the analysis step - you might not know all of the details of how the data will need to be tidied - what’s important for this section is that you think through the potential methods you will need to use to make disparate datasets interoperable and tidy so that they’re of acceptable quality and easy to use for your analysis and product development steps.\nSome questions to help guide the development of this section include:\n\nWhich datasets (if any) will need to be merged/combined to be made useful for your project? If this needs to be done, how will you plan on doing it?\nWhat are your data quality objectives/standards?\nHow will you assess and establish the quality of the data you use?\nWhat rubric will you use to decide which data are kept and which are excluded from future steps?\n\n\n\n\n\n\n\nConsider how your project aligns with the Water Boards Quality Management System!\n\n\n\nThe Water Boards have a quality management system and overarching Quality Assurance Management Plan. Be sure to review this material to consider how your project falls in that framework, and if more is needed to ensure your all aspects of your project are of sufficient quality. For example, many programs do NOT have Quality Assurance Program Plans, so this may be a step needed to occur to establish data quality objectives, etc.\nIn some cases the Quality Assurance and Data Management plans can be integrated (see this USGS Quality-Assurance and Data-Management Plan as an example)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-analysis-product-development",
    "href": "plan-prep/prep.html#data-analysis-product-development",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how you intend on using the data and the general plan or intended workflow you envision for your data analysis and/or product development phase. If you know you will use certain formulas, methods, or software for this step, you will identify them here. More detailed guidance on data analysis and product development steps are outlined on the Data Analysis and Data Visualization pages.\nSome questions to help guide the development of this section include:\n\nWhat management questions are you planning on answering or informing with this data and project?\n\nIf operational decision making is the use please identify which performance measures or existing resource allocation planning processes that will be using the data (e.g., assigning inspections to staff, determining priority for compliance assurance work, etc.).\nPlease also identify any business interests that will need to be alerted to the data / products and may have concerns over its quality, etc. For example, invoicing for fees will need to have updated information, etc.\n\nWhat workflow will you use to analyse the data and/or develop the resultant data product?\nWhat data analysis or visualization methods or software will you use?\nWhat product(s) will be developed (e.g. analyses, visualizations, applications, reports, etc.)\nWhat opportunities will Tribal and community partners have to review and provide feedback on the data analysis or product development before it is finalized?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-preservation-storage",
    "href": "plan-prep/prep.html#data-product-preservation-storage",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where you plan on preserving and storing data and products once they are developed. More detailed guidance on preservation and storage of data and resultant products is outlined on the Preservation & Storage page.\nSome questions to help guide the development of this section include:\n\nHow and where will you store and secure your data and resultant products (code, results, products, visualizations, applications, etc.)?\nWhat privacy and confidentiality issues must you address?\nWhat are your plans for preserving the data/products after the project is completed?\nWhat procedures will you use to ensure long-term archiving and preservation of your data?\nAt what point will data, code/scripts, and resultant products/applications be archived or deleted?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-publication-sharing",
    "href": "plan-prep/prep.html#data-product-publication-sharing",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where you plan on publishing, sharing and otherwise making accessibly the project’s data and products once they are developed. More detailed guidance on publishing and sharing data and resultant products is outlined on the Data Sharing page.\nThe Water Boards typically make virtually all of the data we collect available to the public. The exceptions are confidential information (e.g., part of ongoing enforcement actions and/or formal Tribal consultations) and some personally identifiable information (PII). This section should describe any policies that will filter out data from the step of making the data publicly available and, more importantly, how the project plans to provide access to the data.\n\n\n\n\n\n\nMake your project as open and accessible as possible and appropriate!\n\n\n\nPublishing and sharing Water Boards data and resultant products is critical for collaboration and transparency of our data, products, and workflows. Your project should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.”\nThis means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software (e.g. Python, R) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\n\n\nSome questions to help guide the development of this section include:\n\nWhat data and products will be shared, and when?\nWhere and how will data and products be made open and/or accessible?\n\nDatasets that are of high value should, at minimum, be published to the California Open Data Portal in the form of machine readable, well documented, maintained data.\nGeospatial products of high value should, at minimum, be published to the California State Geoportal.\nCode and similar products (scripts, analysis packages) should, at minimum, be published on the Water Boards GitHub in it’s own, well documented, project repository.\nFor all other data or products, please indicate how the data/product will be made accessible (e.g., via search forms at SMARTS public reports page, etc.).\n\nDoes sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?\nIf you collected data directly from Tribes or communities -\n\nHow will permission be obtained to use and disseminate the data?\nHow is informed consent being handled and how is privacy being protected?\nHow and when will you communicate what will or will not be shared?\n\nTo what extent do the methods of publication and sharing of data, products developed through this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes of publication and sharing to better meet them?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-documentation",
    "href": "plan-prep/prep.html#data-product-documentation",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where every aspect of the project will be well documented. More detailed guidance on describing the project’s data and products is outlined on the Documentation page.\nMetadata - the details about what, where, when, why, and how the data were collected, processed, and interpreted - provide the information that enables data and files to be discovered, used, and properly cited. Metadata and other project documentation include descriptions of how data and files are named, physically structured, and stored as well as details about the experiments, analytical/visualization methods, project context, and names long-term data/product/project managers/stewards.\n\n\n\n\n\n\nThe utility and longevity of data and products relate directly to how complete and comprehensive the metadata and documentation are.\n\n\n\nThe amount of effort devoted to creating comprehensive metadata and documentation may vary substantially based on the complexity, types, and volume of data/products developed throughout the life of a project - but it’s safe to assume (and plan for) a substantial amount of time and energy will be required to develop adequate metadata and documentation.\n\n\nSome questions to help guide the development of this section include:\n\nWhat types of metadata will be produced alongside the data?\nWhat metadata standards will be used? Are you using metadata that is standard to your field?\nHow will the metadata be managed and stored?\nWhat other documentation will be developed for the project and associated products (e.g. workflows, standard operating procedures, data or product use or interpretation guidance, etc.)? Where will that be stored? How will it be made accessible and shared?\nIf you collected data or partnered directly from Tribes or communities -\n\nDoes it make sense to have these same partners review and provide feedback on your metadata and documentation materials? Doing so would help ensure that documentation is clear, simple, and accessible to a wide array of audiences.\nWhen and how will you share the aforementioned documentation with your partners?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-evaluation",
    "href": "plan-prep/prep.html#data-product-evaluation",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how you will evaluate the data, products, and outcomes of the project after it is complete, to assess the extent to which the project has achieved the goals you set for it and advanced and improved equity outcomes. More detailed guidance on describing the project’s data and products is outlined on the Evaluation page.\nSome questions to help guide the development of this section include:\n\nAt what point(s) during the project’s life cycle will you conduct your evaluation? (You don’t need to wait until the project is complete to benefit from this phase!)\nWhat evaluation method(s) will you use?\nHow can the project design an equitable and inclusive evaluation?\nWill the project team share evaluation findings with the experts or other partners involved in the project? If so, with whom will you share it, and how?\nWould sharing the project’s evaluation findings with the experts or other partners who were NOT directly involved in the project further promote equity through transparency and accountability? If so, with whom will you share it, and how?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#other-potential-sections",
    "href": "plan-prep/prep.html#other-potential-sections",
    "title": "Data Preparation",
    "section": "",
    "text": "If the Data Management Plan was developed by a group that included external partners, we recommend including an acknowledgements section to acknowledge, express appreciation, and give credit to those efforts.\n\n\n\nIncluding a timeline for project implementation is always recommended (even though it is more of a project management tool than a data management tool) and even if specific dates are not yet known. Including a timeline helps keep ourselves accountable and makes it easier for potential partners see when their contributions, feedback, and partnership might be needed so they can plan ahead and be ready for when its their time to engage.\n\n\n\nIf there are multiple people on the team that will be involved with project implementation, it might be a good idea to define who will be responsible for which parts of the project/data life cycle so that everyone is clear on their roles and responsibilities to this project ( even though it is more of a project management tool than a data management tool).\n\n\n\n\n\n\nTip\n\n\n\nSpelling out project roles and responsibilities during this phase can help identify gaps and resource needs early!\nThis will enable the project team, management, and/or project partners to understand the limitations and dedicate time and resources to find more team members that can help fill those gaps before the project is underway. Doing this will ultimately prevent the project from being delayed, stalled, or put on hold after time, energy, and resources have already been expended (or even wasted).\n\n\nYou might include a Project Roles and Responsibilities table that includes:\n\nData Life Cycle Phase\nRole Title (e.g. project manager, data collection coordinator, data manager, data analysis lead, data product developer, project engagement lead, etc.)\nName (Affiliation)\nResponsibilities (with a short list of responsibilities associated with that role)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#potential-appendices",
    "href": "plan-prep/prep.html#potential-appendices",
    "title": "Data Preparation",
    "section": "",
    "text": "The goal of this section is making it easy for readers to see and understand the content of your data sources without having to view data directly. You might include a data schema for the datasets of interest. A data schema shows what the “guts” of your data will look like, including the identification of tables, columns/fields, data types, constraints, and relationships. This could be provided as a single table that includes your column/field names, and data types or something much more complex that better suits the needs of your project. For a simple example, see Appendix 1 of the SWAMP Bioassessment Reporting Module Data Management Plan.\n\n\n\nIf your project involves collecting data through a survey, you might use this section to document your intended survey questions and possible responses or response types.\n\n\n\nHere you might describe next steps or project ideas that are outside the scope and timelines of the current project, but that you see as being directly related to or building upon the current project.",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#additional-resources",
    "href": "plan-prep/prep.html#additional-resources",
    "title": "Data Preparation",
    "section": "",
    "text": "DMP Tool (a web-based tool that helps you construct data management plans using templates that address specific funder requirements) \nCA Healthy Watersheds Partnership Assessment Guidance: Data Acquisition, Monitoring and Management\nUSGS - Data Management Plans\nMIT Libraries - Write a Data Management Plan &\nHarvard Medical School - Data Management Plans\nUniversity of Arizona - Data Management Plans Overview",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "assure-analyze/index.html",
    "href": "assure-analyze/index.html",
    "title": "Assure & Analyze",
    "section": "",
    "text": "Assure & Analyze\nBy now, you’ve established your project equity and data management plans (Plan & Prepare) and you’ve finished assembling and tidying the data you plan on using for your project (Collect & Process).\nThis phase of the data life cycle involves conducing your final data quality checks (Quality Assurance) and beginning your analyses or product development steps (Data Analysis, Data Visualization).\nAlso see a short list of best practices for this phase.",
    "crumbs": [
      "Assure & Analyze"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html",
    "href": "assure-analyze/qaqc.html",
    "title": "Quality Assurance",
    "section": "",
    "text": "Before diving into your analyses or product development steps - you’ll want to assess the quality of your data according to quality assurance and quality control processes and standards that are relevant to your project.\n\n\n\n\n\n\nBe sure to set up your quality system early!\n\n\n\nImpactful and effective quality assurance requires setting up systems and thinking about quality long before the data analysis phase - be sure to start thinking about project and data quality early and take the time to get effective systems in place. See the sections below for more guidance and resources on how to do so with equity in mind.\n\n\n\n\nAccording to the Water Boards Quality Assurance/Quality Control Website:\n\nQuality assurance and quality control are components of a quality system. A quality system is a structured system that describes the policies and procedures for ensuring that work processes, products, or services satisfy the user’s specifications and expectations. A quality system is the means by which an organization manages its quality aspects. These aspects are Project Management, Data Generation and Acquisition, and Assessment and Oversight.\n\nQuality Assurance (QA) involves an integrated system of management activities that involves planning, implementation, documentation, assessment, reporting, and quality improvement to ensure that a process, item, or service is of the type and quality required for the project needs.\nQuality Control (QC) involves a system of technical activities that measures the attributes and performance of a process, item, or service against defined standards to verify that they meet the stated requirements established by the customer. QC also involves analytical frequency requirements and control limits.\n\n\n\nSetting up your QA system with an equity lens requires thinking through the systems that are already in place and assessing the extent to which they are supporting the advancement of equity objectives associated with your project. Consider reviewing the pages associated with the aforementioned QA activities to see if your systems are on track:\n\nPlanning: Planning, Data Preparation\nImplementation: Data Collection, Data Processing, Data Analysis, Data Visualization, Preservation & Storage\nDocumentation: Describe (aka Documentation)\nAssessment: Discover & Integrate (aka Evaluation)\nReporting: Data Sharing\nQuality Improvement: If some or all of your QA systems are not in support of your equity objectives, think through what changes you/your team can make to your process NOW to better operationalize equity.\n\nSetting up your QC system with an equity lens requires thinking through how we’re measuring quality/performance and the standards we’re using to assess quality.\n\nHow do our current performance measures help us assess the extent to which we’re meeting project objectives?\nDo any of our current performance measures make it difficult for us to achieve our equity objectives? If so, how might we modify or eliminate them?\nWhat additional performance measures would better help us assess the extent to which this data-intensive project is meeting the project’s equity objectives?\nDo the standards, thresholds, or limits we’re using align with the needs of those facing inequity or disparities - are or likely will be affected by a policy, program, or process? If not, how might we modify the them to better align with those needs?\n\nFrom the data science perspective, conducting QA/QC of the data requires checking values of the dataset against the aforementioned standards to understand the quality of the data.\n\nIs the metadata and vocabulary associated with each dataset complete, easy to access, and simple enough to be understood by a variety of audiences?\nAre the standards we’re using to check the quality of our data sufficient and relevant to our project objectives and/or in support of advancing equity outcomes? If not, why not? How should our project standards be revised so they can be more supportive of equity outcomes?\nIf data is of insufficient quality and needs to be excluded from future steps - where and how will documentation summarize what measures/standards were used to make such determination and which data were excluded?\n\n\n\n\n\nWater Boards QAQC Webpage\nSWAMP Bioaccumulation Monitoring Program Training Series\n\nData Quality and Data Management - Slides 67-80 | Recording Part 3\nQuality Assurance & Quality Control - Slides 5 - 38 | Recording Part 1",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#quality-assurancequality-control-qaqc-grounding",
    "href": "assure-analyze/qaqc.html#quality-assurancequality-control-qaqc-grounding",
    "title": "Quality Assurance",
    "section": "",
    "text": "According to the Water Boards Quality Assurance/Quality Control Website:\n\nQuality assurance and quality control are components of a quality system. A quality system is a structured system that describes the policies and procedures for ensuring that work processes, products, or services satisfy the user’s specifications and expectations. A quality system is the means by which an organization manages its quality aspects. These aspects are Project Management, Data Generation and Acquisition, and Assessment and Oversight.\n\nQuality Assurance (QA) involves an integrated system of management activities that involves planning, implementation, documentation, assessment, reporting, and quality improvement to ensure that a process, item, or service is of the type and quality required for the project needs.\nQuality Control (QC) involves a system of technical activities that measures the attributes and performance of a process, item, or service against defined standards to verify that they meet the stated requirements established by the customer. QC also involves analytical frequency requirements and control limits.",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#qaqc-with-an-equity-lens",
    "href": "assure-analyze/qaqc.html#qaqc-with-an-equity-lens",
    "title": "Quality Assurance",
    "section": "",
    "text": "Setting up your QA system with an equity lens requires thinking through the systems that are already in place and assessing the extent to which they are supporting the advancement of equity objectives associated with your project. Consider reviewing the pages associated with the aforementioned QA activities to see if your systems are on track:\n\nPlanning: Planning, Data Preparation\nImplementation: Data Collection, Data Processing, Data Analysis, Data Visualization, Preservation & Storage\nDocumentation: Describe (aka Documentation)\nAssessment: Discover & Integrate (aka Evaluation)\nReporting: Data Sharing\nQuality Improvement: If some or all of your QA systems are not in support of your equity objectives, think through what changes you/your team can make to your process NOW to better operationalize equity.\n\nSetting up your QC system with an equity lens requires thinking through how we’re measuring quality/performance and the standards we’re using to assess quality.\n\nHow do our current performance measures help us assess the extent to which we’re meeting project objectives?\nDo any of our current performance measures make it difficult for us to achieve our equity objectives? If so, how might we modify or eliminate them?\nWhat additional performance measures would better help us assess the extent to which this data-intensive project is meeting the project’s equity objectives?\nDo the standards, thresholds, or limits we’re using align with the needs of those facing inequity or disparities - are or likely will be affected by a policy, program, or process? If not, how might we modify the them to better align with those needs?\n\nFrom the data science perspective, conducting QA/QC of the data requires checking values of the dataset against the aforementioned standards to understand the quality of the data.\n\nIs the metadata and vocabulary associated with each dataset complete, easy to access, and simple enough to be understood by a variety of audiences?\nAre the standards we’re using to check the quality of our data sufficient and relevant to our project objectives and/or in support of advancing equity outcomes? If not, why not? How should our project standards be revised so they can be more supportive of equity outcomes?\nIf data is of insufficient quality and needs to be excluded from future steps - where and how will documentation summarize what measures/standards were used to make such determination and which data were excluded?",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#resources",
    "href": "assure-analyze/qaqc.html#resources",
    "title": "Quality Assurance",
    "section": "",
    "text": "Water Boards QAQC Webpage\nSWAMP Bioaccumulation Monitoring Program Training Series\n\nData Quality and Data Management - Slides 67-80 | Recording Part 3\nQuality Assurance & Quality Control - Slides 5 - 38 | Recording Part 1",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  }
]