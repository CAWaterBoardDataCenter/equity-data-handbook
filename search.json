[
  {
    "objectID": "plan-prep/plan.html",
    "href": "plan-prep/plan.html",
    "title": "Planning",
    "section": "",
    "text": "When planning for your data project with an equity lens, it’s best to conduct a thorough equity assessment, or scoping process. Equity assessments1 are systematic examinations of available data and expert input on how various groups - especially those facing inequity or disparities - are or likely will be affected by a policy, program, or process. They aim to minimize unintended adverse outcomes and maximize opportunities and positive outcomes.\nThe steps to conducting an equity assessment are described below.\n\nStep 0: Project Scoping\nStep 1: Describe the selected program, policy, or process, and populations affected by it\nStep 2. Consider historical, societal, and policy context and drivers of disparities\nStep 3. Collect expert input, including from affected community members\nStep 4. Identify information sources and gaps\nStep 5. Analyze program/policy effects - potential or current - on people and communities\nStep 6. Plan for action and accountability\n\nAlthough these steps are numbered for clarity, teams should synthesize information from all steps rather than completing them in isolation or one at a time.\n\n\nTime frame and level of effort:\n\nPlan a detailed schedule that accounts for staff availability, budget, technical assistance needs, data availability, and the need to make decisions in a particular time frame.\nIdentify and document risks to the timeline, such as staff availability, or threats to the comprehensiveness of the assessment, such as lack of access to experts. Consider potential ways to mitigate risks.\n\nProject team:\n\nDefine roles for team members and assign responsibilities. Plan to share information at key milestones to synthesize information from different assessment steps.\nConsider how to meaningfully involve experts, including people with lived experience with relevant programs and topics; people in communities affected by the program, policy, or process; staff who work with program participants/beneficiaries; or representatives of other offices. Experts can contribute to the assessment process in several ways, such as suggesting data sources, providing multiple perspectives to inform and enhance the analysis, and developing recommendations for action.\n\n\n\n\nDescribe the focus of the assessment to provide a foundation for all members of the assessment team and external partners.\n\nWhat is the purpose of the selected program, policy, or process, and what are its goals?\nWhat are the known successes or challenges in meeting those goals?\nWhat types of actions or policy levers are involved in the selected program, policy, or process (such as grants, contracts, waivers, guidance to partners, technical assistance, or other actions)?\nWhich of these actions will be included in the assessment?\nWhat general descriptive or performance data can the organization use to describe the program, policy, or process (such as number served, total funds distributed, uptake estimates, or other key outcomes)?\nAre there existing quantifiable performance targets relevant to the focus of the assessment? Provide a brief summary.\n\nIdentify and describe populations of interest\n\nWhat populations are participating in the program, policy, or process, including program participants/beneficiaries? Consider which characteristics are relevant and of interest, such as race, ethnicity, gender identity, sexual orientation, disability status, income, religion, and rural geography.\nWhat populations are currently left out, or not participating or benefiting at desired rates or at the same rates as others? What are other disparities related to the selected program, policy, or process that are known at the outset of the assessment?\nWhat are the information sources for those inequities or disparities?  What is the comparison population or reference point for observed disparities? Reference point options include the total population in an area, the national population, the largest group, or a benchmark chosen through a planning process. Whenever possible, try to think critically about this population rather than simply defaulting to comparison populations used in the past.\nHow might population groups’ identifying characteristics overlap in ways that expose them to relatively greater inequities (known as intersectionality)? What implications does this overlap have for the impacts of the program, policy or process? For example, immigrants who are also LGBTQIA+ might face multiple barriers in accessing a particular program.\n\n\n\n\n\n\n\nTip\n\n\n\nMany local city and county governments have compiled their own resources for working with equity data, like Oakland’s Geographic Equity Toolbox, Portland’s Racial Equity Toolkit, and New York City’s interactive equity data tool EquityNYC, including a Mapping Equity tool. Before starting your project, it is a good idea to do some research online to identify pre-existing resources available in your region.\n\n\n\n\n\nDescribe the context for observed disparities and the program or policy itself.\n\nWhat is the social and cultural history of the populations listed in Step 1 and how does this history shape their current conditions? How does this context play a role in how these populations might perceive, access, or otherwise interact with the program or policy?\nWhat structural or social drivers of disparities might explain observed disparities? Structural drivers of disparities are governing processes and economic and social policies that distribute power and resources in unfair ways, such as an inequitable distribution of emergency funds to certain communities. Social drivers of disparities are differences in the conditions in which people are born, grow, live, work, and age, such as poverty, employment, housing, environment quality, transportation, food security, and community safety. Differences in these social conditions drive disparities. Although these conditions are also known as social determinants of health, this tool uses a broader term to encompass multiple outcomes, including both health outcomes and other outcomes (e.g., economic outcomes). Thinking through these drivers of disparities is important for placing focus on systems and institutions that need to be changed, and it helps to avoid blaming groups of people for poor outcomes.\nWhat is known about whether structural, systemic, or institutional racism or structural barriers affect the implementation and outcomes of previous programs or policies? Systemic or institutional racism refers to policies and practices that create or sustain disparate outcomes for persons of different races. An example is redlining, where financial services and other housing-related opportunities were restricted for individuals largely based on their race/ethnicity and originating neighborhoods (see this 2021 Memorandum for the Secretary of Housing and Urban Development regarding Redressing Our Nation’s and the Federal Government’s History of Discriminatory Housing Practices and Policies)\n\n\n\n\nExperts can include former or current program participants/beneficiaries, members of communities affected by the program, policy, or process, staff who work with program participants/beneficiaries or affected communities, subject matter experts such as researchers, or staff in other organizations, among others. Sources of expert input on programs, policies, and processes include listening sessions, surveys, interviews, focus groups, and position papers by experts in the field or advocacy groups.\n\nHow will the assessment team engage experts with lived experience with relevant programs, policies, processes, and/or issues? To what extent can these experts be part of the assessment team?\nHow will the assessment team engage other experts in the equity assessment (in addition to potentially involving them in the assessment team)? Which experts will be engaged?\nWhat individuals or communities have historically been excluded or disempowered in decision making? How can they be included and meaningfully engaged?\nHow can the assessment team ensure inclusivity when engaging experts, such as translation services or accommodations for people with disabilities? Will there be different options for sharing input for people with different communication preferences or time or transportation constraints?\nHow will the assessment team work to decrease power dynamics and ensure that experts are comfortable providing candid input? How can the team be transparent about how input will be shared and used?\nWhat methods can the assessment team use to collect input, such as focus groups on participants/beneficiaries’ experiences with programs? What are experts’ experiences with current programs and policies, and what are their views on the benefits and burdens involved in participating?\nWhat are experts’ perceptions about barriers to participation? Can experts help the assessment team understand whether there are current or potential burdens or barriers that are more severe for certain population groups?\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed guidance and resources regarding outreach and engagement, see the Practical Guidance Document developed by the Office of Public Participation.\n\n\n\n\n\nConsider a variety of qualitative and quantitative information sources to support the assessment, including gray and peer-reviewed literature, organization documents and administrative records, surveys, customer inquiry or complaint information, administrative data, program performance data, key informant interviews, and listening sessions or focus groups. Ideally, equity assessments often include both qualitative and quantitative data. Data sources can include, but should not be limited to, expert views.\n\nWhat are the quantitative data sources for the assessment process? Quantitative data such as program, administrative, or survey data shed light on the magnitude and prevalence of an inequity or an opportunity for improvement.\nAre available quantitative data disaggregated by relevant variables, such as race, ethnicity, income, and relevant geographic areas? If not, how can the assessment incorporate data that can help organizations understand or estimate the equity impacts of the program, policy, or process?\nWhat are the qualitative data sources for the assessment process? Qualitative data such as interview or focus group data increase understanding of context, as well as helping to interpret and understand quantitative data.\nAre there gaps or limitations in the information needed for the assessment? If either qualitative or quantitative data are not available, explain why. If there are gaps, how might the assessment team obtain new or better information, or highlight the need for investments in better data? It is important to describe gaps that might reflect historically overlooked inequities or point to the need for information sources that could be developed in future years.\n\n\n\n\nDrawing on all previous steps in the assessment process, analyze the available data and describe equity-related outcomes of the program, policy, or process. Describe findings with as much specificity as possible.\n\nWhat quantitative and qualitative analysis methods did the team use to analyze the available data? Did the team synthesize quantitative and qualitative data to develop a complete picture of current inequities or disparities related to the program, policy, or process?\nWhat are the assessment team’s findings on positive and negative equity-related outcomes of the program, policy, or process? What quantitative and qualitative evidence of inequities exists?\nWhat evidence is there of inequities in areas such as awareness of programs and benefits, processes and rules, administrative burden, access to services, participation, outcomes, quality, and engagement?\nHow do findings change the team’s understanding of disparities related to the selected program, policy, or process known at the outset of the assessment?\nWhat factors might be driving observed inequities or disparities? Are any of those factors potentially caused by the program or policy that is the focus of the assessment?\nHave experts helped the assessment team interpret the available data or validate or refine the initial findings?\nIn what ways might the findings be limited due to data gaps or analysis constraints? What findings point to the need for further research?\n\n\n\n\nDevelop a detailed plan to address inequities identified in Step 5 within the scope of your program.\n\nWhat solutions are needed to resolve observed inequities or disparities, or to address identified drivers of those inequities or disparities? Which solutions are in the program’s sphere of authority?\nWhat are the program’s short-term and long-term goals for improvement? Quantify those goals if possible.\nWhat steps will the program take to accomplish each goal? What coordination, training, information systems changes, business process changes, or other implementation actions are needed?\nHave subject matter experts—including those with lived experience—weighed in on needed solutions, proposed goals, or planned action steps? Are all components of the improvement plan responsive to the needs and cultures of different populations or communities?\nWhat resources will the program need to carry out the improvement plan?\nHas the program consulted or collaborated with key partners on potential improvement options and actions?\nIn what ways could the program coordinate with other partners to achieve equity improvements that are not solely within the control or influence of the program conducting the assessment?\n\nAdditional follow-up actions help programs learn about equity impacts and whether implementation should be adjusted to realize positive outcomes. In addition, equity assessments have the potential to generate many new lessons about equity that could be helpful for other partners. Articulating plans for these actions is part of the equity assessment even though these actions occur after the formal assessment is over.\n\nWould sharing the equity assessment with other partners support collaboration on other policies and programs intended to benefit priority populations?\nWould sharing the equity assessment or a summary of findings with experts who were not directly involved in the assessment further promote equity through transparency and accountability?\nWhat measures or indicators will the program use to track progress over time? Are these disaggregated individual-level or community-level measures? Monitoring can help the program assess whether patterns or trends are in the expected direction or require course corrections.\nHow and when will the organization evaluate the results of potential program changes? Evaluations focus on whether programs or policies reach their goals within a defined period. How can the organization design an equitable and inclusive evaluation?\nWho will be responsible for developing and executing monitoring and evaluation plans?  \nWill the program share monitoring and evaluation results with the experts involved in the assessment or other partners? If so, how?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-0-project-scoping",
    "href": "plan-prep/plan.html#step-0-project-scoping",
    "title": "Planning",
    "section": "",
    "text": "Time frame and level of effort:\n\nPlan a detailed schedule that accounts for staff availability, budget, technical assistance needs, data availability, and the need to make decisions in a particular time frame.\nIdentify and document risks to the timeline, such as staff availability, or threats to the comprehensiveness of the assessment, such as lack of access to experts. Consider potential ways to mitigate risks.\n\nProject team:\n\nDefine roles for team members and assign responsibilities. Plan to share information at key milestones to synthesize information from different assessment steps.\nConsider how to meaningfully involve experts, including people with lived experience with relevant programs and topics; people in communities affected by the program, policy, or process; staff who work with program participants/beneficiaries; or representatives of other offices. Experts can contribute to the assessment process in several ways, such as suggesting data sources, providing multiple perspectives to inform and enhance the analysis, and developing recommendations for action.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-1-describe-the-selected-program-policy-or-process-and-populations-affected-by-it",
    "href": "plan-prep/plan.html#step-1-describe-the-selected-program-policy-or-process-and-populations-affected-by-it",
    "title": "Planning",
    "section": "",
    "text": "Describe the focus of the assessment to provide a foundation for all members of the assessment team and external partners.\n\nWhat is the purpose of the selected program, policy, or process, and what are its goals?\nWhat are the known successes or challenges in meeting those goals?\nWhat types of actions or policy levers are involved in the selected program, policy, or process (such as grants, contracts, waivers, guidance to partners, technical assistance, or other actions)?\nWhich of these actions will be included in the assessment?\nWhat general descriptive or performance data can the organization use to describe the program, policy, or process (such as number served, total funds distributed, uptake estimates, or other key outcomes)?\nAre there existing quantifiable performance targets relevant to the focus of the assessment? Provide a brief summary.\n\nIdentify and describe populations of interest\n\nWhat populations are participating in the program, policy, or process, including program participants/beneficiaries? Consider which characteristics are relevant and of interest, such as race, ethnicity, gender identity, sexual orientation, disability status, income, religion, and rural geography.\nWhat populations are currently left out, or not participating or benefiting at desired rates or at the same rates as others? What are other disparities related to the selected program, policy, or process that are known at the outset of the assessment?\nWhat are the information sources for those inequities or disparities?  What is the comparison population or reference point for observed disparities? Reference point options include the total population in an area, the national population, the largest group, or a benchmark chosen through a planning process. Whenever possible, try to think critically about this population rather than simply defaulting to comparison populations used in the past.\nHow might population groups’ identifying characteristics overlap in ways that expose them to relatively greater inequities (known as intersectionality)? What implications does this overlap have for the impacts of the program, policy or process? For example, immigrants who are also LGBTQIA+ might face multiple barriers in accessing a particular program.\n\n\n\n\n\n\n\nTip\n\n\n\nMany local city and county governments have compiled their own resources for working with equity data, like Oakland’s Geographic Equity Toolbox, Portland’s Racial Equity Toolkit, and New York City’s interactive equity data tool EquityNYC, including a Mapping Equity tool. Before starting your project, it is a good idea to do some research online to identify pre-existing resources available in your region.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-2.-consider-historical-societal-and-policy-context-and-drivers-of-disparities",
    "href": "plan-prep/plan.html#step-2.-consider-historical-societal-and-policy-context-and-drivers-of-disparities",
    "title": "Planning",
    "section": "",
    "text": "Describe the context for observed disparities and the program or policy itself.\n\nWhat is the social and cultural history of the populations listed in Step 1 and how does this history shape their current conditions? How does this context play a role in how these populations might perceive, access, or otherwise interact with the program or policy?\nWhat structural or social drivers of disparities might explain observed disparities? Structural drivers of disparities are governing processes and economic and social policies that distribute power and resources in unfair ways, such as an inequitable distribution of emergency funds to certain communities. Social drivers of disparities are differences in the conditions in which people are born, grow, live, work, and age, such as poverty, employment, housing, environment quality, transportation, food security, and community safety. Differences in these social conditions drive disparities. Although these conditions are also known as social determinants of health, this tool uses a broader term to encompass multiple outcomes, including both health outcomes and other outcomes (e.g., economic outcomes). Thinking through these drivers of disparities is important for placing focus on systems and institutions that need to be changed, and it helps to avoid blaming groups of people for poor outcomes.\nWhat is known about whether structural, systemic, or institutional racism or structural barriers affect the implementation and outcomes of previous programs or policies? Systemic or institutional racism refers to policies and practices that create or sustain disparate outcomes for persons of different races. An example is redlining, where financial services and other housing-related opportunities were restricted for individuals largely based on their race/ethnicity and originating neighborhoods (see this 2021 Memorandum for the Secretary of Housing and Urban Development regarding Redressing Our Nation’s and the Federal Government’s History of Discriminatory Housing Practices and Policies)",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-3.-collect-expert-input-including-from-affected-community-members",
    "href": "plan-prep/plan.html#step-3.-collect-expert-input-including-from-affected-community-members",
    "title": "Planning",
    "section": "",
    "text": "Experts can include former or current program participants/beneficiaries, members of communities affected by the program, policy, or process, staff who work with program participants/beneficiaries or affected communities, subject matter experts such as researchers, or staff in other organizations, among others. Sources of expert input on programs, policies, and processes include listening sessions, surveys, interviews, focus groups, and position papers by experts in the field or advocacy groups.\n\nHow will the assessment team engage experts with lived experience with relevant programs, policies, processes, and/or issues? To what extent can these experts be part of the assessment team?\nHow will the assessment team engage other experts in the equity assessment (in addition to potentially involving them in the assessment team)? Which experts will be engaged?\nWhat individuals or communities have historically been excluded or disempowered in decision making? How can they be included and meaningfully engaged?\nHow can the assessment team ensure inclusivity when engaging experts, such as translation services or accommodations for people with disabilities? Will there be different options for sharing input for people with different communication preferences or time or transportation constraints?\nHow will the assessment team work to decrease power dynamics and ensure that experts are comfortable providing candid input? How can the team be transparent about how input will be shared and used?\nWhat methods can the assessment team use to collect input, such as focus groups on participants/beneficiaries’ experiences with programs? What are experts’ experiences with current programs and policies, and what are their views on the benefits and burdens involved in participating?\nWhat are experts’ perceptions about barriers to participation? Can experts help the assessment team understand whether there are current or potential burdens or barriers that are more severe for certain population groups?\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed guidance and resources regarding outreach and engagement, see the Practical Guidance Document developed by the Office of Public Participation.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-4.-identify-information-sources-and-gaps",
    "href": "plan-prep/plan.html#step-4.-identify-information-sources-and-gaps",
    "title": "Planning",
    "section": "",
    "text": "Consider a variety of qualitative and quantitative information sources to support the assessment, including gray and peer-reviewed literature, organization documents and administrative records, surveys, customer inquiry or complaint information, administrative data, program performance data, key informant interviews, and listening sessions or focus groups. Ideally, equity assessments often include both qualitative and quantitative data. Data sources can include, but should not be limited to, expert views.\n\nWhat are the quantitative data sources for the assessment process? Quantitative data such as program, administrative, or survey data shed light on the magnitude and prevalence of an inequity or an opportunity for improvement.\nAre available quantitative data disaggregated by relevant variables, such as race, ethnicity, income, and relevant geographic areas? If not, how can the assessment incorporate data that can help organizations understand or estimate the equity impacts of the program, policy, or process?\nWhat are the qualitative data sources for the assessment process? Qualitative data such as interview or focus group data increase understanding of context, as well as helping to interpret and understand quantitative data.\nAre there gaps or limitations in the information needed for the assessment? If either qualitative or quantitative data are not available, explain why. If there are gaps, how might the assessment team obtain new or better information, or highlight the need for investments in better data? It is important to describe gaps that might reflect historically overlooked inequities or point to the need for information sources that could be developed in future years.",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-5.-analyze-programpolicy-effects---potential-or-current---on-people-and-communities",
    "href": "plan-prep/plan.html#step-5.-analyze-programpolicy-effects---potential-or-current---on-people-and-communities",
    "title": "Planning",
    "section": "",
    "text": "Drawing on all previous steps in the assessment process, analyze the available data and describe equity-related outcomes of the program, policy, or process. Describe findings with as much specificity as possible.\n\nWhat quantitative and qualitative analysis methods did the team use to analyze the available data? Did the team synthesize quantitative and qualitative data to develop a complete picture of current inequities or disparities related to the program, policy, or process?\nWhat are the assessment team’s findings on positive and negative equity-related outcomes of the program, policy, or process? What quantitative and qualitative evidence of inequities exists?\nWhat evidence is there of inequities in areas such as awareness of programs and benefits, processes and rules, administrative burden, access to services, participation, outcomes, quality, and engagement?\nHow do findings change the team’s understanding of disparities related to the selected program, policy, or process known at the outset of the assessment?\nWhat factors might be driving observed inequities or disparities? Are any of those factors potentially caused by the program or policy that is the focus of the assessment?\nHave experts helped the assessment team interpret the available data or validate or refine the initial findings?\nIn what ways might the findings be limited due to data gaps or analysis constraints? What findings point to the need for further research?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#step-6.-plan-for-action-and-accountability",
    "href": "plan-prep/plan.html#step-6.-plan-for-action-and-accountability",
    "title": "Planning",
    "section": "",
    "text": "Develop a detailed plan to address inequities identified in Step 5 within the scope of your program.\n\nWhat solutions are needed to resolve observed inequities or disparities, or to address identified drivers of those inequities or disparities? Which solutions are in the program’s sphere of authority?\nWhat are the program’s short-term and long-term goals for improvement? Quantify those goals if possible.\nWhat steps will the program take to accomplish each goal? What coordination, training, information systems changes, business process changes, or other implementation actions are needed?\nHave subject matter experts—including those with lived experience—weighed in on needed solutions, proposed goals, or planned action steps? Are all components of the improvement plan responsive to the needs and cultures of different populations or communities?\nWhat resources will the program need to carry out the improvement plan?\nHas the program consulted or collaborated with key partners on potential improvement options and actions?\nIn what ways could the program coordinate with other partners to achieve equity improvements that are not solely within the control or influence of the program conducting the assessment?\n\nAdditional follow-up actions help programs learn about equity impacts and whether implementation should be adjusted to realize positive outcomes. In addition, equity assessments have the potential to generate many new lessons about equity that could be helpful for other partners. Articulating plans for these actions is part of the equity assessment even though these actions occur after the formal assessment is over.\n\nWould sharing the equity assessment with other partners support collaboration on other policies and programs intended to benefit priority populations?\nWould sharing the equity assessment or a summary of findings with experts who were not directly involved in the assessment further promote equity through transparency and accountability?\nWhat measures or indicators will the program use to track progress over time? Are these disaggregated individual-level or community-level measures? Monitoring can help the program assess whether patterns or trends are in the expected direction or require course corrections.\nHow and when will the organization evaluate the results of potential program changes? Evaluations focus on whether programs or policies reach their goals within a defined period. How can the organization design an equitable and inclusive evaluation?\nWho will be responsible for developing and executing monitoring and evaluation plans?  \nWill the program share monitoring and evaluation results with the experts involved in the assessment or other partners? If so, how?",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/plan.html#footnotes",
    "href": "plan-prep/plan.html#footnotes",
    "title": "Planning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content on this page has been informed by: Conducting Intensive Equity Assessments of Existing Programs, Policies, and Processes (hhs.gov)↩︎",
    "crumbs": [
      "Plan & Prepare",
      "Planning"
    ]
  },
  {
    "objectID": "plan-prep/prep.html",
    "href": "plan-prep/prep.html",
    "title": "Data Preparation",
    "section": "",
    "text": "As you’re in the Planning phase of conducting your Equity Assessment, we recommend documenting what you are doing for the remaining phases of the data life cycle and making that documentation as open and transparent as possible. This is best accomplished through the use of a Data Management Plan.\nA Data Management Plan describes what data will be used, how the data will be collected, processed, and managed to conduct your analysis, visualization or other products, and how those data products will be stored, shared, and maintained over the long-term. In other words, the Data Management Plan describes how the project team intends to address and manage each phase of the data life cycle.\nDepending on the complexity of the project, your data management plan can be relatively short, and it can be used as a way to begin to engage with experts and partners that are interested in your project!\n\n\n\n\n\n\nTip\n\n\n\nTry using the development of your Data Management Plan as a way to build relationships and trust with Tribal and community experts!\nSome ideas for how to do this include:\n\nYou can create a Technical Advisory group composed of Tribal and community experts that helps co-create the Data Management Plan with the Project Team.\nThe Project Team develops the Data Management Plan, but solicits feedback on early versions from Tribal and community experts and makes revisions according to their feedback.\n\n\n\nA Data Management Plan should include the following sections:\n\nProject Introduction & Context\nData Collection & Processing\nData Analysis & Product Development\nData & Product Preservation & Storage\nData & Product Publication & Sharing\nData & Product Documentation\nData & Product Evaluation\nOther Potential Sections or Potential Appendices, including: Acknowledgements, Timeline, Project Roles and Responsibilities, Dataset Details, Survey Details, Future work\n\nSince the development the Data Management Plan takes place before data are actually collected, some details, like specific analytical methods, may not be completely worked out. However, the Data Management Plan should include a clear vision and general plan for each section and include as much detail as possible.\n\n\nHere you want to briefly describe the project and the mechanism(s) driving the data collection and product development. Much of this will likely be worked out during the Planning phase, including:\n\nWhat is the purpose of the selected program, policy, or process, related to this project?\nWhat are the objectives of the project?\nWho is the intended audience of the project?\nHow do you envision the project’s resulting data and products contribute to the advancement and operationalization of equity for your the program, policy, or process, related to this project?\n\n\n\n\nIn this section, you will identify the data you plan on collecting, how you will collect it, and how you organize, manage, and process said data once it is collected. More detailed guidance on collection and processing of data and resultant products is outlined on the Data Collection (including information on Surveys), and Data Processing pages.\n\n\n\n\n\n\nBe sure to pause and think about what you actually need to answer the questions/objectives you have using an equity lens.\n\n\n\nAs a reminder - achieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you create the list of data you want to collect for your project, it should contain:\n\nData that can represent your management question(s) or project objectives. See the Planning page for more guidance.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start! See the Data Collection page for more guidance.\n\n\n\n\n\nA good plan will include information that is sufficient to understand the nature of the data that is collected, including:\n\nTypes. A good first step is to list the various types of data that you expect to collect or create. This may include text, spreadsheets, software and algorithms, models, images and movies, audio files, and patient records. \nSources. Data may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and compilations of data from other studies.\nVolume. Both the total volume of data and the total number of files that are expected to be collected can affect all other data management activities.\nData and file formats. Technology changes and formats that are acceptable today may soon be obsolete. Good choices include those formats that are nonproprietary, based upon open standards, and widely adopted and preferred by the larger data consuming community (e.g., Comma Separated Values [CSV] over Excel [.xls, xlsx]). Data are more accessible for the long term if they are uncompressed, unencrypted, and stored using standard character encodings.\n\nSome questions to help guide the development of this section include:\n\nWhat data will we be collecting and/or generating?\nHow and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?\nWhat file formats will be used? Do these formats conform to an open standard and/or are they proprietary?\nHow much data will be generated for this project?\nAre you using data that someone else produced? If so, where is it from?\nHow long will the data be collected/generated and how often will it change?\nTo what extent do the data and methods of collection and use for this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes data collection and use to better meet them?\n\n\n\n\nFAIR Principles (Findable, Accessible, Interoperable, Reusable) within the open data movement primarily focus on characteristics of data that will facilitate increased data sharing among entities while ignoring power differentials and historical contexts. CARE Principles (Collective Benefit, Authority to Control, Responsibility, Ethics) for Indigenous Data Governance are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. Image credit: Global Indigenous Data Allianc\n\n\n\n\n\nDefine how and where the data will be organized and managed.\nFor example, your effort may require a small number of data tables and these can be effectively managed with spreadsheet programs like Excel. Larger data volumes and usage constraints may require the use of relational database management systems for linked data tables like ORACLE or mySQL, or a Geographic Information System (GIS) for geospatial data layers like ArcGIS, or computer programming languages like R or Python for large datasets that cannot be contained within standard database or GIS systems.\nThis section should contain just enough detail to identify basic data organization needs and plan, not the level of detail needed to build a comprehensive system or information technology project plan.\nSome questions to help guide the development of this section include:\n\nHow and where will your data be organized?\nWhat tools or software are required to read or view the data?\nWhat directory and file naming convention will be used?\nWhat are your local storage and backup procedures?\nWill this data require secure storage?\nWho is responsible for managing the data? Who will ensure that the data management plan is carried out?\nWhat steps will be taken to protect privacy, security, confidentiality, intellectual property or other rights?\n\n\n\n\nHere you will define the processes you will use to clean and prepare your data once it is collected - also known as tidying data. Tidy data are structured such that the data are easy to manipulate, model, and visualize - and getting data to the point of being tidy is often the most time consuming step of any data-intensive work. More detailed data processing/tidying steps are outlined on the Data Processing page.\nSimilar to the analysis step - you might not know all of the details of how the data will need to be tidied - what’s important for this section is that you think through the potential methods you will need to use to make disparate datasets interoperable and tidy so that they’re of acceptable quality and easy to use for your analysis and product development steps.\nSome questions to help guide the development of this section include:\n\nWhich datasets (if any) will need to be merged/combined to be made useful for your project? If this needs to be done, how will you plan on doing it?\nWhat are your data quality objectives/standards?\nHow will you assess and establish the quality of the data you use?\nWhat rubric will you use to decide which data are kept and which are excluded from future steps?\n\n\n\n\n\n\n\nConsider how your project aligns with the Water Boards Quality Management System!\n\n\n\nThe Water Boards have a quality management system and overarching Quality Assurance Management Plan. Be sure to review this material to consider how your project falls in that framework, and if more is needed to ensure your all aspects of your project are of sufficient quality. For example, many programs do NOT have Quality Assurance Program Plans, so this may be a step needed to occur to establish data quality objectives, etc.\nIn some cases the Quality Assurance and Data Management plans can be integrated (see this USGS Quality-Assurance and Data-Management Plan as an example)\n\n\n\n\n\n\nIn this section, you will describe how you intend on using the data and the general plan or intended workflow you envision for your data analysis and/or product development phase. If you know you will use certain formulas, methods, or software for this step, you will identify them here. More detailed guidance on data analysis and product development steps are outlined on the Data Analysis and Data Visualization pages.\nSome questions to help guide the development of this section include:\n\nWhat management questions are you planning on answering or informing with this data and project?\n\nIf operational decision making is the use please identify which performance measures or existing resource allocation planning processes that will be using the data (e.g., assigning inspections to staff, determining priority for compliance assurance work, etc.).\nPlease also identify any business interests that will need to be alerted to the data / products and may have concerns over its quality, etc. For example, invoicing for fees will need to have updated information, etc.\n\nWhat workflow will you use to analyse the data and/or develop the resultant data product?\nWhat data analysis or visualization methods or software will you use?\nWhat product(s) will be developed (e.g. analyses, visualizations, applications, reports, etc.)\nWhat opportunities will Tribal and community partners have to review and provide feedback on the data analysis or product development before it is finalized?\n\n\n\n\nIn this section, you will describe how and where you plan on preserving and storing data and products once they are developed. More detailed guidance on preservation and storage of data and resultant products is outlined on the Preservation & Storage page.\nSome questions to help guide the development of this section include:\n\nHow and where will you store and secure your data and resultant products (code, results, products, visualizations, applications, etc.)?\nWhat privacy and confidentiality issues must you address?\nWhat are your plans for preserving the data/products after the project is completed?\nWhat procedures will you use to ensure long-term archiving and preservation of your data?\nAt what point will data, code/scripts, and resultant products/applications be archived or deleted?\n\n\n\n\nIn this section, you will describe how and where you plan on publishing, sharing and otherwise making accessible the project’s data and products once they are developed. More detailed guidance on publishing and sharing data and resultant products is outlined on the Data Sharing page.\nThe Water Boards typically make virtually all of the data we collect available to the public. The exceptions are confidential information (e.g., part of ongoing enforcement actions and/or formal Tribal consultations) and some personally identifiable information (PII). This section should describe any policies that will filter out data from the step of making the data publicly available and, more importantly, how the project plans to provide access to the data.\n\n\n\n\n\n\nMake your project as open and accessible as possible and appropriate!\n\n\n\nPublishing and sharing Water Boards data and resultant products is critical for collaboration and transparency of our data, products, and workflows. Your project should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.”\nThis means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software (e.g. Python, R) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\n\n\nSome questions to help guide the development of this section include:\n\nWhat data and products will be shared, and when?\nWhere and how will data and products be made open and/or accessible?\n\nDatasets that are of high value should, at minimum, be published to the California Open Data Portal in the form of machine readable, well documented, maintained data.\nGeospatial products of high value should, at minimum, be published to the California State Geoportal.\nCode and similar products (scripts, analysis packages) should, at minimum, be published on the Water Boards GitHub in its own, well documented, project repository.\nFor all other data or products, please indicate how the data/product will be made accessible (e.g., via search forms at SMARTS public reports page, etc.).\n\nDoes sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?\nIf you collected data directly from Tribes or communities -\n\nHow will permission be obtained to use and disseminate the data?\nHow is informed consent being handled and how is privacy being protected?\nHow and when will you communicate what will or will not be shared?\n\nTo what extent do the methods of publication and sharing of data, products developed through this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes of publication and sharing to better meet them?\n\n\n\n\nIn this section, you will describe how and where every aspect of the project will be well documented. More detailed guidance on describing the project’s data and products is outlined on the Documentation page.\nMetadata - the details about what, where, when, why, and how the data were collected, processed, and interpreted - provide the information that enables data and files to be discovered, used, and properly cited. Metadata and other project documentation include descriptions of how data and files are named, physically structured, and stored as well as details about the experiments, analytical/visualization methods, project context, and names long-term data/product/project managers/stewards.\n\n\n\n\n\n\nThe utility and longevity of data and products relate directly to how complete and comprehensive the metadata and documentation are.\n\n\n\nThe amount of effort devoted to creating comprehensive metadata and documentation may vary substantially based on the complexity, types, and volume of data/products developed throughout the life of a project - but it’s safe to assume (and plan for) a substantial amount of time and energy will be required to develop adequate metadata and documentation.\n\n\nSome questions to help guide the development of this section include:\n\nWhat types of metadata will be produced alongside the data?\nWhat metadata standards will be used? Are you using metadata that is standard to your field?\nHow will the metadata be managed and stored?\nWhat other documentation will be developed for the project and associated products (e.g. workflows, standard operating procedures, data or product use or interpretation guidance, etc.)? Where will that be stored? How will it be made accessible and shared?\nIf you collected data or partnered directly from Tribes or communities -\n\nDoes it make sense to have these same partners review and provide feedback on your metadata and documentation materials? Doing so would help ensure that documentation is clear, simple, and accessible to a wide array of audiences.\nWhen and how will you share the aforementioned documentation with your partners?\n\n\n\n\n\nIn this section, you will describe how you will evaluate the data, products, and outcomes of the project after it is complete, to assess the extent to which the project has achieved the goals you set for it and advanced and improved equity outcomes. More detailed guidance on describing the project’s data and products is outlined on the Evaluation page.\nSome questions to help guide the development of this section include:\n\nAt what point(s) during the project’s life cycle will you conduct your evaluation? (You don’t need to wait until the project is complete to benefit from this phase!)\nWhat evaluation method(s) will you use?\nHow can the project design an equitable and inclusive evaluation?\nWill the project team share evaluation findings with the experts or other partners involved in the project? If so, with whom will you share it, and how?\nWould sharing the project’s evaluation findings with the experts or other partners who were NOT directly involved in the project further promote equity through transparency and accountability? If so, with whom will you share it, and how?\n\n\n\n\n\n\nIf the Data Management Plan was developed by a group that included external partners, we recommend including an acknowledgements section to acknowledge, express appreciation, and give credit to those efforts.\n\n\n\nIncluding a timeline for project implementation is always recommended (even though it is more of a project management tool than a data management tool) and even if specific dates are not yet known. Including a timeline helps keep ourselves accountable and makes it easier for potential partners see when their contributions, feedback, and partnership might be needed so they can plan ahead and be ready for when its their time to engage.\n\n\n\nIf there are multiple people on the team that will be involved with project implementation, it might be a good idea to define who will be responsible for which parts of the project/data life cycle so that everyone is clear on their roles and responsibilities to this project ( even though it is more of a project management tool than a data management tool).\n\n\n\n\n\n\nTip\n\n\n\nSpelling out project roles and responsibilities during this phase can help identify gaps and resource needs early!\nThis will enable the project team, management, and/or project partners to understand the limitations and dedicate time and resources to find more team members that can help fill those gaps before the project is underway. Doing this will ultimately prevent the project from being delayed, stalled, or put on hold after time, energy, and resources have already been expended (or even wasted).\n\n\nYou might include a Project Roles and Responsibilities table that includes:\n\nData Life Cycle Phase\nRole Title (e.g. project manager, data collection coordinator, data manager, data analysis lead, data product developer, project engagement lead, etc.)\nName (Affiliation)\nResponsibilities (with a short list of responsibilities associated with that role)\n\n\n\n\n\n\n\nThe goal of this section is making it easy for readers to see and understand the content of your data sources without having to view data directly. You might include a data schema for the datasets of interest. A data schema shows what the “guts” of your data will look like, including the identification of tables, columns/fields, data types, constraints, and relationships. This could be provided as a single table that includes your column/field names, and data types or something much more complex that better suits the needs of your project. For a simple example, see Appendix 1 of the SWAMP Bioassessment Reporting Module Data Management Plan.\n\n\n\nIf your project involves collecting data through a survey, you might use this section to document your intended survey questions and possible responses or response types.\n\n\n\nHere you might describe next steps or project ideas that are outside the scope and timelines of the current project, but that you see as being directly related to or building upon the current project.\n\n\n\n\n\nDMP Tool (a web-based tool that helps you construct data management plans using templates that address specific funder requirements) \nCA Healthy Watersheds Partnership Assessment Guidance: Data Acquisition, Monitoring and Management\nUSGS - Data Management Plans\nMIT Libraries - Write a Data Management Plan\nHarvard Medical School - Data Management Plans\nUniversity of Arizona - Data Management Plans Overview\nNational Institutes of Health Data Management Resources\nFAIR and CARE Principles Resources\n\nOperationalizing the CARE and FAIR Principles for Indigenous data futures\nEnabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats\nEarth Science Data Repositories: Implementing the CARE Principles (O’Brien et al., 2024)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#project-introduction-context",
    "href": "plan-prep/prep.html#project-introduction-context",
    "title": "Data Preparation",
    "section": "",
    "text": "Here you want to briefly describe the project and the mechanism(s) driving the data collection and product development. Much of this will likely be worked out during the Planning phase, including:\n\nWhat is the purpose of the selected program, policy, or process, related to this project?\nWhat are the objectives of the project?\nWho is the intended audience of the project?\nHow do you envision the project’s resulting data and products contribute to the advancement and operationalization of equity for your the program, policy, or process, related to this project?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-collection-processing",
    "href": "plan-prep/prep.html#data-collection-processing",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will identify the data you plan on collecting, how you will collect it, and how you organize, manage, and process said data once it is collected. More detailed guidance on collection and processing of data and resultant products is outlined on the Data Collection (including information on Surveys), and Data Processing pages.\n\n\n\n\n\n\nBe sure to pause and think about what you actually need to answer the questions/objectives you have using an equity lens.\n\n\n\nAs a reminder - achieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you create the list of data you want to collect for your project, it should contain:\n\nData that can represent your management question(s) or project objectives. See the Planning page for more guidance.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start! See the Data Collection page for more guidance.\n\n\n\n\n\nA good plan will include information that is sufficient to understand the nature of the data that is collected, including:\n\nTypes. A good first step is to list the various types of data that you expect to collect or create. This may include text, spreadsheets, software and algorithms, models, images and movies, audio files, and patient records. \nSources. Data may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and compilations of data from other studies.\nVolume. Both the total volume of data and the total number of files that are expected to be collected can affect all other data management activities.\nData and file formats. Technology changes and formats that are acceptable today may soon be obsolete. Good choices include those formats that are nonproprietary, based upon open standards, and widely adopted and preferred by the larger data consuming community (e.g., Comma Separated Values [CSV] over Excel [.xls, xlsx]). Data are more accessible for the long term if they are uncompressed, unencrypted, and stored using standard character encodings.\n\nSome questions to help guide the development of this section include:\n\nWhat data will we be collecting and/or generating?\nHow and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?\nWhat file formats will be used? Do these formats conform to an open standard and/or are they proprietary?\nHow much data will be generated for this project?\nAre you using data that someone else produced? If so, where is it from?\nHow long will the data be collected/generated and how often will it change?\nTo what extent do the data and methods of collection and use for this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes data collection and use to better meet them?\n\n\n\n\nFAIR Principles (Findable, Accessible, Interoperable, Reusable) within the open data movement primarily focus on characteristics of data that will facilitate increased data sharing among entities while ignoring power differentials and historical contexts. CARE Principles (Collective Benefit, Authority to Control, Responsibility, Ethics) for Indigenous Data Governance are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. Image credit: Global Indigenous Data Allianc\n\n\n\n\n\nDefine how and where the data will be organized and managed.\nFor example, your effort may require a small number of data tables and these can be effectively managed with spreadsheet programs like Excel. Larger data volumes and usage constraints may require the use of relational database management systems for linked data tables like ORACLE or mySQL, or a Geographic Information System (GIS) for geospatial data layers like ArcGIS, or computer programming languages like R or Python for large datasets that cannot be contained within standard database or GIS systems.\nThis section should contain just enough detail to identify basic data organization needs and plan, not the level of detail needed to build a comprehensive system or information technology project plan.\nSome questions to help guide the development of this section include:\n\nHow and where will your data be organized?\nWhat tools or software are required to read or view the data?\nWhat directory and file naming convention will be used?\nWhat are your local storage and backup procedures?\nWill this data require secure storage?\nWho is responsible for managing the data? Who will ensure that the data management plan is carried out?\nWhat steps will be taken to protect privacy, security, confidentiality, intellectual property or other rights?\n\n\n\n\nHere you will define the processes you will use to clean and prepare your data once it is collected - also known as tidying data. Tidy data are structured such that the data are easy to manipulate, model, and visualize - and getting data to the point of being tidy is often the most time consuming step of any data-intensive work. More detailed data processing/tidying steps are outlined on the Data Processing page.\nSimilar to the analysis step - you might not know all of the details of how the data will need to be tidied - what’s important for this section is that you think through the potential methods you will need to use to make disparate datasets interoperable and tidy so that they’re of acceptable quality and easy to use for your analysis and product development steps.\nSome questions to help guide the development of this section include:\n\nWhich datasets (if any) will need to be merged/combined to be made useful for your project? If this needs to be done, how will you plan on doing it?\nWhat are your data quality objectives/standards?\nHow will you assess and establish the quality of the data you use?\nWhat rubric will you use to decide which data are kept and which are excluded from future steps?\n\n\n\n\n\n\n\nConsider how your project aligns with the Water Boards Quality Management System!\n\n\n\nThe Water Boards have a quality management system and overarching Quality Assurance Management Plan. Be sure to review this material to consider how your project falls in that framework, and if more is needed to ensure your all aspects of your project are of sufficient quality. For example, many programs do NOT have Quality Assurance Program Plans, so this may be a step needed to occur to establish data quality objectives, etc.\nIn some cases the Quality Assurance and Data Management plans can be integrated (see this USGS Quality-Assurance and Data-Management Plan as an example)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-analysis-product-development",
    "href": "plan-prep/prep.html#data-analysis-product-development",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how you intend on using the data and the general plan or intended workflow you envision for your data analysis and/or product development phase. If you know you will use certain formulas, methods, or software for this step, you will identify them here. More detailed guidance on data analysis and product development steps are outlined on the Data Analysis and Data Visualization pages.\nSome questions to help guide the development of this section include:\n\nWhat management questions are you planning on answering or informing with this data and project?\n\nIf operational decision making is the use please identify which performance measures or existing resource allocation planning processes that will be using the data (e.g., assigning inspections to staff, determining priority for compliance assurance work, etc.).\nPlease also identify any business interests that will need to be alerted to the data / products and may have concerns over its quality, etc. For example, invoicing for fees will need to have updated information, etc.\n\nWhat workflow will you use to analyse the data and/or develop the resultant data product?\nWhat data analysis or visualization methods or software will you use?\nWhat product(s) will be developed (e.g. analyses, visualizations, applications, reports, etc.)\nWhat opportunities will Tribal and community partners have to review and provide feedback on the data analysis or product development before it is finalized?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-preservation-storage",
    "href": "plan-prep/prep.html#data-product-preservation-storage",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where you plan on preserving and storing data and products once they are developed. More detailed guidance on preservation and storage of data and resultant products is outlined on the Preservation & Storage page.\nSome questions to help guide the development of this section include:\n\nHow and where will you store and secure your data and resultant products (code, results, products, visualizations, applications, etc.)?\nWhat privacy and confidentiality issues must you address?\nWhat are your plans for preserving the data/products after the project is completed?\nWhat procedures will you use to ensure long-term archiving and preservation of your data?\nAt what point will data, code/scripts, and resultant products/applications be archived or deleted?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-publication-sharing",
    "href": "plan-prep/prep.html#data-product-publication-sharing",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where you plan on publishing, sharing and otherwise making accessible the project’s data and products once they are developed. More detailed guidance on publishing and sharing data and resultant products is outlined on the Data Sharing page.\nThe Water Boards typically make virtually all of the data we collect available to the public. The exceptions are confidential information (e.g., part of ongoing enforcement actions and/or formal Tribal consultations) and some personally identifiable information (PII). This section should describe any policies that will filter out data from the step of making the data publicly available and, more importantly, how the project plans to provide access to the data.\n\n\n\n\n\n\nMake your project as open and accessible as possible and appropriate!\n\n\n\nPublishing and sharing Water Boards data and resultant products is critical for collaboration and transparency of our data, products, and workflows. Your project should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.”\nThis means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software (e.g. Python, R) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\n\n\nSome questions to help guide the development of this section include:\n\nWhat data and products will be shared, and when?\nWhere and how will data and products be made open and/or accessible?\n\nDatasets that are of high value should, at minimum, be published to the California Open Data Portal in the form of machine readable, well documented, maintained data.\nGeospatial products of high value should, at minimum, be published to the California State Geoportal.\nCode and similar products (scripts, analysis packages) should, at minimum, be published on the Water Boards GitHub in its own, well documented, project repository.\nFor all other data or products, please indicate how the data/product will be made accessible (e.g., via search forms at SMARTS public reports page, etc.).\n\nDoes sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?\nIf you collected data directly from Tribes or communities -\n\nHow will permission be obtained to use and disseminate the data?\nHow is informed consent being handled and how is privacy being protected?\nHow and when will you communicate what will or will not be shared?\n\nTo what extent do the methods of publication and sharing of data, products developed through this project abide by FAIR Principles of scientific data management and stewardship and CARE Principles for Indigenous Data Governance? If FAIR and CARE Principles are not being met - how can we modify our methods and processes of publication and sharing to better meet them?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-documentation",
    "href": "plan-prep/prep.html#data-product-documentation",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how and where every aspect of the project will be well documented. More detailed guidance on describing the project’s data and products is outlined on the Documentation page.\nMetadata - the details about what, where, when, why, and how the data were collected, processed, and interpreted - provide the information that enables data and files to be discovered, used, and properly cited. Metadata and other project documentation include descriptions of how data and files are named, physically structured, and stored as well as details about the experiments, analytical/visualization methods, project context, and names long-term data/product/project managers/stewards.\n\n\n\n\n\n\nThe utility and longevity of data and products relate directly to how complete and comprehensive the metadata and documentation are.\n\n\n\nThe amount of effort devoted to creating comprehensive metadata and documentation may vary substantially based on the complexity, types, and volume of data/products developed throughout the life of a project - but it’s safe to assume (and plan for) a substantial amount of time and energy will be required to develop adequate metadata and documentation.\n\n\nSome questions to help guide the development of this section include:\n\nWhat types of metadata will be produced alongside the data?\nWhat metadata standards will be used? Are you using metadata that is standard to your field?\nHow will the metadata be managed and stored?\nWhat other documentation will be developed for the project and associated products (e.g. workflows, standard operating procedures, data or product use or interpretation guidance, etc.)? Where will that be stored? How will it be made accessible and shared?\nIf you collected data or partnered directly from Tribes or communities -\n\nDoes it make sense to have these same partners review and provide feedback on your metadata and documentation materials? Doing so would help ensure that documentation is clear, simple, and accessible to a wide array of audiences.\nWhen and how will you share the aforementioned documentation with your partners?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#data-product-evaluation",
    "href": "plan-prep/prep.html#data-product-evaluation",
    "title": "Data Preparation",
    "section": "",
    "text": "In this section, you will describe how you will evaluate the data, products, and outcomes of the project after it is complete, to assess the extent to which the project has achieved the goals you set for it and advanced and improved equity outcomes. More detailed guidance on describing the project’s data and products is outlined on the Evaluation page.\nSome questions to help guide the development of this section include:\n\nAt what point(s) during the project’s life cycle will you conduct your evaluation? (You don’t need to wait until the project is complete to benefit from this phase!)\nWhat evaluation method(s) will you use?\nHow can the project design an equitable and inclusive evaluation?\nWill the project team share evaluation findings with the experts or other partners involved in the project? If so, with whom will you share it, and how?\nWould sharing the project’s evaluation findings with the experts or other partners who were NOT directly involved in the project further promote equity through transparency and accountability? If so, with whom will you share it, and how?",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#other-potential-sections",
    "href": "plan-prep/prep.html#other-potential-sections",
    "title": "Data Preparation",
    "section": "",
    "text": "If the Data Management Plan was developed by a group that included external partners, we recommend including an acknowledgements section to acknowledge, express appreciation, and give credit to those efforts.\n\n\n\nIncluding a timeline for project implementation is always recommended (even though it is more of a project management tool than a data management tool) and even if specific dates are not yet known. Including a timeline helps keep ourselves accountable and makes it easier for potential partners see when their contributions, feedback, and partnership might be needed so they can plan ahead and be ready for when its their time to engage.\n\n\n\nIf there are multiple people on the team that will be involved with project implementation, it might be a good idea to define who will be responsible for which parts of the project/data life cycle so that everyone is clear on their roles and responsibilities to this project ( even though it is more of a project management tool than a data management tool).\n\n\n\n\n\n\nTip\n\n\n\nSpelling out project roles and responsibilities during this phase can help identify gaps and resource needs early!\nThis will enable the project team, management, and/or project partners to understand the limitations and dedicate time and resources to find more team members that can help fill those gaps before the project is underway. Doing this will ultimately prevent the project from being delayed, stalled, or put on hold after time, energy, and resources have already been expended (or even wasted).\n\n\nYou might include a Project Roles and Responsibilities table that includes:\n\nData Life Cycle Phase\nRole Title (e.g. project manager, data collection coordinator, data manager, data analysis lead, data product developer, project engagement lead, etc.)\nName (Affiliation)\nResponsibilities (with a short list of responsibilities associated with that role)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#potential-appendices",
    "href": "plan-prep/prep.html#potential-appendices",
    "title": "Data Preparation",
    "section": "",
    "text": "The goal of this section is making it easy for readers to see and understand the content of your data sources without having to view data directly. You might include a data schema for the datasets of interest. A data schema shows what the “guts” of your data will look like, including the identification of tables, columns/fields, data types, constraints, and relationships. This could be provided as a single table that includes your column/field names, and data types or something much more complex that better suits the needs of your project. For a simple example, see Appendix 1 of the SWAMP Bioassessment Reporting Module Data Management Plan.\n\n\n\nIf your project involves collecting data through a survey, you might use this section to document your intended survey questions and possible responses or response types.\n\n\n\nHere you might describe next steps or project ideas that are outside the scope and timelines of the current project, but that you see as being directly related to or building upon the current project.",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "plan-prep/prep.html#additional-resources",
    "href": "plan-prep/prep.html#additional-resources",
    "title": "Data Preparation",
    "section": "",
    "text": "DMP Tool (a web-based tool that helps you construct data management plans using templates that address specific funder requirements) \nCA Healthy Watersheds Partnership Assessment Guidance: Data Acquisition, Monitoring and Management\nUSGS - Data Management Plans\nMIT Libraries - Write a Data Management Plan\nHarvard Medical School - Data Management Plans\nUniversity of Arizona - Data Management Plans Overview\nNational Institutes of Health Data Management Resources\nFAIR and CARE Principles Resources\n\nOperationalizing the CARE and FAIR Principles for Indigenous data futures\nEnabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats\nEarth Science Data Repositories: Implementing the CARE Principles (O’Brien et al., 2024)",
    "crumbs": [
      "Plan & Prepare",
      "Data Preparation"
    ]
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Tip\n\n\n\nBefore embarking on your data project using a racial equity lens you should:\n\nMake your Water Boards data open and accessible to the public! See the Open Data Handbook for guidance.\nConsult the Racial Equity Data Subcommittee by emailing equitydatahelp@waterboards.ca.gov to help identify audience and potential questions or information your teams’ analysis may answer and provide guidance.\nReview the guidance and best practices described in this Equity Data Handbook and complete the Racial Equity Data Project Form\nReview the GARE Framework: Normalize, Organize, and Operationalize  \n\nOther Optional Recommendations\n\nTake the Advancing Racial Equity training series offered by the Water Boards Training Academy\nPut a team together and join an Openscapes Champions Cohort at the Water Boards\n\n\n\n\n\nIn accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nGuidance on open data publishing at the Water Boards is available in the Open Data Handbook.\n\n\n\nProgram staff that are beginning data project using a racial equity lens should meet with the Racial Equity Data Team to discuss key aspects of the project including who the audience is and what questions or information you are trying to convey using the available racial equity data. This engagement should occur prior to beginning any data collection or analysis.\nTo initiate a consultation with the Racial Equity Data Team, please send an email to equitydatahelp@waterboards.ca.gov. \n\n\n\nThis Equity Data Handbook is a curated compilation of emerging and comprehensive (but not exhaustive) guidance on:  \n\nHow to break down the management questions where racial equity information is being posed against administrative data; and \nHow to apply equity best practices during each phase of the data life cycle to begin an iterative process into advancing racial equity. \n\nThis Handbook is specifically structured to support Water Boards staff on incorporating racial equity concepts into all phases of their data projects. The guidance and best practices provided serves as a strategic guide emphasizing the importance of collection, analysis and utilization of racial equity data.\nThese intended users of this Handbook include, but are not limited to:  \n\nProgram Staff\nProgram managers\nExecutives\nAgency Partners\nTribal Governments\nThe Public \n\nAfter reviewing this Handbook, please complete the Racial Equity Data Project Form so that the Racial Equity Data Subcommittee is better equipped for your project consultation.\n\n\n\nIt’s strongly suggested staff review the framework outlined by the Government Alliance for Racial Equity (GARE) to normalize, organize, and operationalize racial equity throughout data integration (see image below).\n\n\n\nGARE model of change. Source: GARE Communications Guide, May 2018\n\n\nIn addition staff should also review why it is important to lead with race:\n\n“with the recognition that the creation and perpetuation of racial inequities has been baked into government, and that racial inequities across all indicators for success are deep and pervasive. We also know that other groups of people are still marginalized, including based on gender, sexual orientation, ability and age, to name but a few. Focusing on racial equity provides the opportunity to introduce a framework, tools and resources that can also be applied to other areas of marginalization. It is critical to address all areas of marginalization, and an institutional approach is necessary across the board. As the local and regional government deepens its ability to eliminate racial inequity, it will be better equipped to transform systems and institutions impacting other marginalized groups.”\n\n\n\n\n\n\nWhen possible, staff should take the Advancing Racial Equity training series offered by the Water Boards Training Academy to foster a consistent baseline knowledge of racial equity work and the importance of applying a racial equity lens to our work.\n\n\n\nOpenscapes through their Champions Program, provides a framework for education, integration, and operationalization of open science, equity, communication, and kindness into individual and team collaborations and workflows. Teams that participate in the Champions Program are empowered to evolve and invest in their culture, processes, and workflows so that they can embody the better science for future us mindset.\nWater Boards teams are able to join Openscapes Champions Cohorts that are created for and led by Water Boards staff.\nFor more information and to learn how to join an upcoming Openscapes Champions Cohort at the Water Boards, visit the Openscapes at the Water Boards webpage.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#make-your-water-boards-data-open-and-accessible",
    "href": "get-started/index.html#make-your-water-boards-data-open-and-accessible",
    "title": "Getting Started",
    "section": "",
    "text": "In accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nGuidance on open data publishing at the Water Boards is available in the Open Data Handbook.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#consult-the-racial-equity-data-subcommittee",
    "href": "get-started/index.html#consult-the-racial-equity-data-subcommittee",
    "title": "Getting Started",
    "section": "",
    "text": "Program staff that are beginning data project using a racial equity lens should meet with the Racial Equity Data Team to discuss key aspects of the project including who the audience is and what questions or information you are trying to convey using the available racial equity data. This engagement should occur prior to beginning any data collection or analysis.\nTo initiate a consultation with the Racial Equity Data Team, please send an email to equitydatahelp@waterboards.ca.gov.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#review-the-equity-data-handbook-and-complete-the-racial-equity-data-project-form",
    "href": "get-started/index.html#review-the-equity-data-handbook-and-complete-the-racial-equity-data-project-form",
    "title": "Getting Started",
    "section": "",
    "text": "This Equity Data Handbook is a curated compilation of emerging and comprehensive (but not exhaustive) guidance on:  \n\nHow to break down the management questions where racial equity information is being posed against administrative data; and \nHow to apply equity best practices during each phase of the data life cycle to begin an iterative process into advancing racial equity. \n\nThis Handbook is specifically structured to support Water Boards staff on incorporating racial equity concepts into all phases of their data projects. The guidance and best practices provided serves as a strategic guide emphasizing the importance of collection, analysis and utilization of racial equity data.\nThese intended users of this Handbook include, but are not limited to:  \n\nProgram Staff\nProgram managers\nExecutives\nAgency Partners\nTribal Governments\nThe Public \n\nAfter reviewing this Handbook, please complete the Racial Equity Data Project Form so that the Racial Equity Data Subcommittee is better equipped for your project consultation.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#review-the-gare-framework",
    "href": "get-started/index.html#review-the-gare-framework",
    "title": "Getting Started",
    "section": "",
    "text": "It’s strongly suggested staff review the framework outlined by the Government Alliance for Racial Equity (GARE) to normalize, organize, and operationalize racial equity throughout data integration (see image below).\n\n\n\nGARE model of change. Source: GARE Communications Guide, May 2018\n\n\nIn addition staff should also review why it is important to lead with race:\n\n“with the recognition that the creation and perpetuation of racial inequities has been baked into government, and that racial inequities across all indicators for success are deep and pervasive. We also know that other groups of people are still marginalized, including based on gender, sexual orientation, ability and age, to name but a few. Focusing on racial equity provides the opportunity to introduce a framework, tools and resources that can also be applied to other areas of marginalization. It is critical to address all areas of marginalization, and an institutional approach is necessary across the board. As the local and regional government deepens its ability to eliminate racial inequity, it will be better equipped to transform systems and institutions impacting other marginalized groups.”",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#other-optional-recommendations",
    "href": "get-started/index.html#other-optional-recommendations",
    "title": "Getting Started",
    "section": "",
    "text": "When possible, staff should take the Advancing Racial Equity training series offered by the Water Boards Training Academy to foster a consistent baseline knowledge of racial equity work and the importance of applying a racial equity lens to our work.\n\n\n\nOpenscapes through their Champions Program, provides a framework for education, integration, and operationalization of open science, equity, communication, and kindness into individual and team collaborations and workflows. Teams that participate in the Champions Program are empowered to evolve and invest in their culture, processes, and workflows so that they can embody the better science for future us mindset.\nWater Boards teams are able to join Openscapes Champions Cohorts that are created for and led by Water Boards staff.\nFor more information and to learn how to join an upcoming Openscapes Champions Cohort at the Water Boards, visit the Openscapes at the Water Boards webpage.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "assure-analyze/index.html",
    "href": "assure-analyze/index.html",
    "title": "Assure & Analyze",
    "section": "",
    "text": "Assure & Analyze\nBy now, you’ve established your project equity and data management plans (Plan & Prepare) and you’ve finished assembling and tidying the data you plan on using for your project (Collect & Process).\nThis phase of the data life cycle involves conducing your final data quality checks (Quality Assurance) and beginning your analyses or product development steps (Data Analysis, Data Visualization).",
    "crumbs": [
      "Assure & Analyze"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html",
    "href": "assure-analyze/analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "By now, you’ve completed all of your planning and prep and are ready to get to the analysis part of the project. Congratulations!\n\n\n\n\n\n\nThe whole point of data analysis is to turn data into information, knowledge, and wisdom that we can use to make decisions and take action.\n\n\n\nIdeally, the data-informed decisions we make and the corresponding actions we take will help us advance and operationalize equity. Using data to inform decisions can be challenging, and is even more so when applying an equity lens to our data work.\n\n\n\n\n\nIllustrations of two perspectives on Data-Information-Knowledge-Wisdom (DIKW) In practice. Graphic adapted from Flood et al. (2016) and DataCamp (2023).\n\n\nAs the graphic above illustrates, the transition and transformation from data to wisdom requires adding context, meaning, and insight to the original data, while gaining experience and understanding as we progress. In the above graphic we define data, information, knowledge, and wisdom as:\n\nData: individual measurements, facts, figures, and signals, without context.\nInformation: organized, structured, and contextualized data that can be used to identify concepts of “who”, “what”, “where”, and “when”.\nKnowledge: the result of analyzing, synthesizing, and interpreting information to identify patterns, trends, and relationships, which can be used to understand the “how” and “why” of what we’re observing.\nWisdom: the ability to integrate what we understand from the data with other context, reflection, and underlying knowledge of the broader question or topic at hand with the express purpose of making data and equity-informed decisions and to take effective and impactful action.\n\n\n\nIncorporating a racial equity lens during data analysis includes incorporating individual, community, political, and historical contexts of race to inform analysis, conclusions, and recommendations. Solely relying on statistical outputs will not necessarily lead to meaningful insights without careful consideration during the analytical process, such as ensuring data quality is sufficient and determining appropriate statistical power.\nTurning data into information in the context of racial equity involves navigating complex ethical considerations. The process requires an understanding of the potential impact on Black, Indigenous, and other People of Color (BIPOC) communities and the responsibility to mitigate perpetuating or reinforcing biases. Upholding ethical standards requires a commitment to maintaining privacy, accessibility, and fostering transparency throughout the data transformation process. Additionally, acknowledging the limitations of the data and being transparent about potential biases is essential for maintaining the integrity of the data and information generated and shared. The transformation of racial equity data into meaningful information requires a thoughtful and intentional approach which we will highlight in the next sections.\nFor example, many programs will rely on demographic and socioeconomic data, like those collected from the U.S. Census and the American Community Survey (ACS). Because the ACS is based on a sample, rather than all housing units and people, ACS estimates have a degree of uncertainty associated with them, called sampling error. In general, the larger the sample, the smaller the level of sampling error. To help users understand the impact of sampling error on data reliability, the Census Bureau provides a “margin of error” for each published ACS estimate. The margin of error, combined with the ACS estimate, gives users a range of values within which the actual “real-world” value is likely to fall. Also see: Using American Community Survey (ACS) Data: What All Data Users Need to Know Handbook.\nIt is important to acknowledge this uncertainty up front to be transparent with your audience about the data and conclusions you are drawing.\nExamples of how others have done this work include:\n\nCalif. Dept. of Finance Demographic Research Unit’s California Hard-to-Count Index Interactive Map, which measures the potential inaccuracies associated with relying on census data to enumerate demographic and socioeconomic characteristics in California\nOffice of Environmental Health and Hazard Assessment CalEnviroScreen 4.0 Race and Equity Analysis\n\n\n\nThere are five main data analysis method types, and each has a different process, purpose, and interpretation (see table below). As you embark on your analysis, it’s important to:\n\nunderstand which method(s) you can use given your data\nunderstand the limitations of your data and those method(s)\nselect the method(s) that are most appropriate for your project questions/objectives\n\n\n\n\n\n\n\n\n\nAnalysis Method\nPurpose\nLimitation\n\n\n\n\nDescriptive Analytics\nSummarize and describe data in clear and simple way\nDescriptive analyses cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified.\n\n\nInferential Analytics\nMake (infer) conclusions or generalizations about populations based on sample data\nConclusions can only be made on samples / data that are analysed, and the appropriate use and interpretation of the statistical method used depends on whether the data meets the assumptions of the method. If method assumptions are not met - results and conclusions are meaningless.\n\n\nDiagnostic Analytics\nTell us something about the “why” behind the results, patterns, or trends that are identified\nDepending on the data you have and the analysis you are able to use, your results may not be able to identify the root cause in the context of the lived experiences of the communities the data you’re using are meant to represent.\n\n\nPredictive Analytics\nHelp identify what might be likely to happen in the future given the previous trends or patterns found in the data\nYour ability to predict what may happen is highly dependent on (and limited by) the quantity, quality, and representativeness of the data that you use. If there are gaps in that data you feed your analytical methods, there will be similar gaps in your results.\n\n\nPrescriptive Analytics\nRecommend actions or decisions.\nThe potential analytical methods are complex and require a large amount of quality and relevant data and computing power to be implemented appropriately. One should also consider the implications of the potential actions and decisions being considered using an equity lens. Striking the balance between data-driven insights and equity considerations is essential for advancing equity outcomes.\n\n\nCausal Analytics\nUnderstand the cause and effect relationships between variables of interest.\nAll causal analytical tools require strong assumptions and can never fully capture all of the context of the relationships in questions (i.e. extraneous variables that cannot be measured or assessed). If these methods are used, be sure to ground-truth the results with the communities the data you’re using are meant to represent.\n\n\n\n\n\n\nAs with any analysis, we need to understand the limits of our data and the methods we use so we interpret the results we find appropriately. Below is an overview of common data fallacies to be aware of and avoid as you’re interpreting your results and making conclusions1:\n\n\n\nData Fallacy\nDescription\nAdditional Equity Considerations\n\n\n\n\nCherry Picking\n\nSelecting results that fit your hypothesis or claim, and excluding those that don’t.\nThis doesn’t mean you should always keep all data you have access to (see Data Processing).\nIt’s important to be aware of data or results you are excluding and to be honest and transparent about why you’re excluding it.\n\n\nCobra Effect\n(aka Perverse Incentive)\n\nSetting an incentive that accidentally produces the opposite result to the one intended.\nThis is all about unintended consequences of our decisions and actions.\nWhen you’re developing your project objectives, be sure to take the Planning step seriously and take the time needed to answer as many of the questions provided as possible.\n\n\nDanger of Summary Metrics\n\nOnly looking at summary metrics and missing big differences in the raw data.\nSummary metrics or descriptive analytics can (and should!) be used to help us understand out data. However it’s important to remember that they cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified. Be sure to use summary metrics in concert with context and other analyses so you can see your data holistically.\n\n\nData Dredging\n\nRepeatedly testing new hypotheses against the same set of data, failing to acknowledge that most correlations will be the result of chance.\nIt can be exciting to uncover meaningful and logical correlations when conducting your analyses.\nSimilar to cherry picking, it’s important to be aware of your analytical methods (and their limitations), and to be honest and transparent about your project objectives before you begin your analysis (see Plan and Prepare) as well as why you’re accepting or rejecting the results you get after analyses are conducted appropriately.\n\n\nFalse Causality\n\nFalsely assuming when two events appear related that one must have caused the other.\nThe classic: correlation does not imply causation. Adding context for which you don’t have data can help here. Discussing preliminary trends/results with partners and communities can help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\n\n\nGambler’s Fallacy\n(aka Monte Carlo Fallacy)\n\nMistakenly believing that because something has happened more frequently than usual, it’s now less likely to happen in future (and vice versa).\nTo combat this fallacy it will be important to understand the context and “why” behind the results you are seeing. There are a number methods designed to help identify the the “why” (and there may be more than one reason!).\nSee the Evaluation section for guidance and remember that you don’t need to wait until the project is complete to benefit from using these evaluation tools! In fact, it may be helpful to use these tools at multiple points during your project or process.\n\n\nGerrymandering\n\nManipulating the geographical boundaries used to group data in order to change the result.\nThis is closely related to how you use geographical boundaries to aggregate or group your data. During your Plan and Prepare phase, think through what geographic units are most appropriate for your project needs. Know that the data you have access to might be in one geographic unit (e.g. census tract, hydrologic unit code (HUC)) but your questions are related to a different unit (e.g. county, region, statewide) - so you may need to do some analyses to standardize units. Doing so has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nHawthorne Effect\n(aka Observer Effect)\n\nThe act of monitoring someone can affect their behavior, leading to spurious findings.\nThis may not be directly relevant in the usual data work you do. However, it is particularly important when delivering surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nMcNamara Fallacy\n\nRelying solely on metrics in complex situations and losing sight of the bigger picture.\nIt can be easy for some of us to get lost in the weeds of data analysis. What’s key here is to take a step back from time to time and continue to connect our analyses or product development back to the broader and contextualized project and equity goals you set for the project during your Plan and Prepare phase.\nWorking with and/or discussing your analytical process and/or preliminary trends/results with partners and communities can also help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project, and pull you out of the data weeds and back into context-informed data work.\n\n\nOverfitting\n\nCreating a model that’s overly tailored to the data you have and not representative of the general trend.\nUsing data to describe the environment and inform our decisions is complex, and is even more so when applying an equity lens to our data work. What’s important here is to keep this in mind as you develop, test, and interpret models for your analyses.\n\n\nPublication Bias\n\nInteresting research findings are more likely to be published, distorting our impression of reality.\nIt’s important to remember that academic systems - like our government systems and including publishing institutions - have racist origins and biases that have excluded the science, research, and ways of knowing that have come from of Black, Indigenous, and other People of Color (BIPOC) communities. This is why taking the time to develop relationships and partnerships with the communities our decisions and actions may impact is crucial.\nIncluding partners with the expertise that can only come from lived experiences in our projects from the beginnig, and giving that expertise the same weight as academic or research expertise can broaden our perspectives and help prevent publication (and other biases) from negatively impacting our project, process and advancement of equity.\nSee the Planning section on collecting expert input for more guidance.\n\n\nRegression Towards the Mean\n\nWhen something happens that’s unusually good or bad, it will revert back towards the average over time.\nA key component of this fallacy is that random chance influences the outcome. When looking at our data through an equity lens, we need to remember that racism and injustice are central to our collective history and can be traced back to before the founding of our country. We live and work in institutions and systems that have inherited those unjust decisions and processes. We know that, as government representatives, if we’re not clear and intentional about advancing racial equity in the work we do, it won’t happen and we will continue to perpetuate racial inequity. In other words, because of the unjust systems in which we work, we cannot depend on this fallacy and wait for the unusually “bad” or unjust results/trends to correct themselves or revert back to more equitable trends. Because government agencies created and perpetuated environmental racism, it is our responsibility to proactively advance racial equity and justice in all the work we do.\nIf our collective history and its deep connection to racism and injustice are not familiar to you - we recommend taking the Advancing Racial Equity training series offered by the Water Boards Training Academy and reviewing the GARE Framework: Normalize, Organize, and Operationalize. More details on both of these actions and others can be found on the Getting Started page. |\n\n\nSampling Bias\n\nDrawing conclusions from a set of data that isn’t representative of the population you’re trying to understand.\nDuring your Plan and Prepare you will think through the type of data that’s needed to adequitely represent the populations related to your project objectives. If the data available is not adequately representative, then you may need to revise the types of questions you have of the data (and analytical methods). Or, it might be worth considering collecting the data yourself or with partners.\nNote that this principle and others can also be applied to surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nSimpson’s Paradox\n\nWhen a trend appears in different subsets of data but disappears or reverses when the groups are combined.\nThis is critical when trying to understand and quantify how environmental outcomes might impact different groups. During your Plan and Prepare phase, think through the different ways you might aggregate or disaggregate your data and consider what is appropriate for the questions you have.\nWithout disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nEach decision has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nSurvivorship Bias\n\nDrawing conclusions from an incomplete set of data, because that data has ‘survived’ some selection criteria.\nAs soon as we get our hands on a dataset it can be tempting to dive into analysis without taking the time to consider that data in the context of our project goals and objectives. Sometimes the data we need to understand every aspect of ideal our project objectives simply does not exist or is not accessible.\nIt’s important to take the time during the Plan and Prepare phase to understand what data are needed to achieve the objectives of your project, what data are actually available, and to understand what all of that means for your Collect and Process phase, and how that might impact your interpretation.\n\n\n\n\n\n\n\n\nCollege of Water Informatics Machine Learning Handbook\nFlood M. D., Lemieux V. L., Varga M., and Wong B. L. W. (2016) ‘The application of visual analytics to financial stability monitoring’, Journal of Financial Stability\nDataCamp (2023) The Data-Information-Knowledge-Wisdom Pyramid",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html#data-analysis-with-an-equity-lens",
    "href": "assure-analyze/analysis.html#data-analysis-with-an-equity-lens",
    "title": "Data Analysis",
    "section": "",
    "text": "Incorporating a racial equity lens during data analysis includes incorporating individual, community, political, and historical contexts of race to inform analysis, conclusions, and recommendations. Solely relying on statistical outputs will not necessarily lead to meaningful insights without careful consideration during the analytical process, such as ensuring data quality is sufficient and determining appropriate statistical power.\nTurning data into information in the context of racial equity involves navigating complex ethical considerations. The process requires an understanding of the potential impact on Black, Indigenous, and other People of Color (BIPOC) communities and the responsibility to mitigate perpetuating or reinforcing biases. Upholding ethical standards requires a commitment to maintaining privacy, accessibility, and fostering transparency throughout the data transformation process. Additionally, acknowledging the limitations of the data and being transparent about potential biases is essential for maintaining the integrity of the data and information generated and shared. The transformation of racial equity data into meaningful information requires a thoughtful and intentional approach which we will highlight in the next sections.\nFor example, many programs will rely on demographic and socioeconomic data, like those collected from the U.S. Census and the American Community Survey (ACS). Because the ACS is based on a sample, rather than all housing units and people, ACS estimates have a degree of uncertainty associated with them, called sampling error. In general, the larger the sample, the smaller the level of sampling error. To help users understand the impact of sampling error on data reliability, the Census Bureau provides a “margin of error” for each published ACS estimate. The margin of error, combined with the ACS estimate, gives users a range of values within which the actual “real-world” value is likely to fall. Also see: Using American Community Survey (ACS) Data: What All Data Users Need to Know Handbook.\nIt is important to acknowledge this uncertainty up front to be transparent with your audience about the data and conclusions you are drawing.\nExamples of how others have done this work include:\n\nCalif. Dept. of Finance Demographic Research Unit’s California Hard-to-Count Index Interactive Map, which measures the potential inaccuracies associated with relying on census data to enumerate demographic and socioeconomic characteristics in California\nOffice of Environmental Health and Hazard Assessment CalEnviroScreen 4.0 Race and Equity Analysis\n\n\n\nThere are five main data analysis method types, and each has a different process, purpose, and interpretation (see table below). As you embark on your analysis, it’s important to:\n\nunderstand which method(s) you can use given your data\nunderstand the limitations of your data and those method(s)\nselect the method(s) that are most appropriate for your project questions/objectives\n\n\n\n\n\n\n\n\n\nAnalysis Method\nPurpose\nLimitation\n\n\n\n\nDescriptive Analytics\nSummarize and describe data in clear and simple way\nDescriptive analyses cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified.\n\n\nInferential Analytics\nMake (infer) conclusions or generalizations about populations based on sample data\nConclusions can only be made on samples / data that are analysed, and the appropriate use and interpretation of the statistical method used depends on whether the data meets the assumptions of the method. If method assumptions are not met - results and conclusions are meaningless.\n\n\nDiagnostic Analytics\nTell us something about the “why” behind the results, patterns, or trends that are identified\nDepending on the data you have and the analysis you are able to use, your results may not be able to identify the root cause in the context of the lived experiences of the communities the data you’re using are meant to represent.\n\n\nPredictive Analytics\nHelp identify what might be likely to happen in the future given the previous trends or patterns found in the data\nYour ability to predict what may happen is highly dependent on (and limited by) the quantity, quality, and representativeness of the data that you use. If there are gaps in that data you feed your analytical methods, there will be similar gaps in your results.\n\n\nPrescriptive Analytics\nRecommend actions or decisions.\nThe potential analytical methods are complex and require a large amount of quality and relevant data and computing power to be implemented appropriately. One should also consider the implications of the potential actions and decisions being considered using an equity lens. Striking the balance between data-driven insights and equity considerations is essential for advancing equity outcomes.\n\n\nCausal Analytics\nUnderstand the cause and effect relationships between variables of interest.\nAll causal analytical tools require strong assumptions and can never fully capture all of the context of the relationships in questions (i.e. extraneous variables that cannot be measured or assessed). If these methods are used, be sure to ground-truth the results with the communities the data you’re using are meant to represent.\n\n\n\n\n\n\nAs with any analysis, we need to understand the limits of our data and the methods we use so we interpret the results we find appropriately. Below is an overview of common data fallacies to be aware of and avoid as you’re interpreting your results and making conclusions1:\n\n\n\nData Fallacy\nDescription\nAdditional Equity Considerations\n\n\n\n\nCherry Picking\n\nSelecting results that fit your hypothesis or claim, and excluding those that don’t.\nThis doesn’t mean you should always keep all data you have access to (see Data Processing).\nIt’s important to be aware of data or results you are excluding and to be honest and transparent about why you’re excluding it.\n\n\nCobra Effect\n(aka Perverse Incentive)\n\nSetting an incentive that accidentally produces the opposite result to the one intended.\nThis is all about unintended consequences of our decisions and actions.\nWhen you’re developing your project objectives, be sure to take the Planning step seriously and take the time needed to answer as many of the questions provided as possible.\n\n\nDanger of Summary Metrics\n\nOnly looking at summary metrics and missing big differences in the raw data.\nSummary metrics or descriptive analytics can (and should!) be used to help us understand out data. However it’s important to remember that they cannot tell us anything about why we’re seeing the results, patterns, or trends that are identified. Be sure to use summary metrics in concert with context and other analyses so you can see your data holistically.\n\n\nData Dredging\n\nRepeatedly testing new hypotheses against the same set of data, failing to acknowledge that most correlations will be the result of chance.\nIt can be exciting to uncover meaningful and logical correlations when conducting your analyses.\nSimilar to cherry picking, it’s important to be aware of your analytical methods (and their limitations), and to be honest and transparent about your project objectives before you begin your analysis (see Plan and Prepare) as well as why you’re accepting or rejecting the results you get after analyses are conducted appropriately.\n\n\nFalse Causality\n\nFalsely assuming when two events appear related that one must have caused the other.\nThe classic: correlation does not imply causation. Adding context for which you don’t have data can help here. Discussing preliminary trends/results with partners and communities can help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\n\n\nGambler’s Fallacy\n(aka Monte Carlo Fallacy)\n\nMistakenly believing that because something has happened more frequently than usual, it’s now less likely to happen in future (and vice versa).\nTo combat this fallacy it will be important to understand the context and “why” behind the results you are seeing. There are a number methods designed to help identify the the “why” (and there may be more than one reason!).\nSee the Evaluation section for guidance and remember that you don’t need to wait until the project is complete to benefit from using these evaluation tools! In fact, it may be helpful to use these tools at multiple points during your project or process.\n\n\nGerrymandering\n\nManipulating the geographical boundaries used to group data in order to change the result.\nThis is closely related to how you use geographical boundaries to aggregate or group your data. During your Plan and Prepare phase, think through what geographic units are most appropriate for your project needs. Know that the data you have access to might be in one geographic unit (e.g. census tract, hydrologic unit code (HUC)) but your questions are related to a different unit (e.g. county, region, statewide) - so you may need to do some analyses to standardize units. Doing so has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nHawthorne Effect\n(aka Observer Effect)\n\nThe act of monitoring someone can affect their behavior, leading to spurious findings.\nThis may not be directly relevant in the usual data work you do. However, it is particularly important when delivering surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nMcNamara Fallacy\n\nRelying solely on metrics in complex situations and losing sight of the bigger picture.\nIt can be easy for some of us to get lost in the weeds of data analysis. What’s key here is to take a step back from time to time and continue to connect our analyses or product development back to the broader and contextualized project and equity goals you set for the project during your Plan and Prepare phase.\nWorking with and/or discussing your analytical process and/or preliminary trends/results with partners and communities can also help you ground truth the numerical results you’re seeing with the lived experiences of those that may be impacted by your project, and pull you out of the data weeds and back into context-informed data work.\n\n\nOverfitting\n\nCreating a model that’s overly tailored to the data you have and not representative of the general trend.\nUsing data to describe the environment and inform our decisions is complex, and is even more so when applying an equity lens to our data work. What’s important here is to keep this in mind as you develop, test, and interpret models for your analyses.\n\n\nPublication Bias\n\nInteresting research findings are more likely to be published, distorting our impression of reality.\nIt’s important to remember that academic systems - like our government systems and including publishing institutions - have racist origins and biases that have excluded the science, research, and ways of knowing that have come from of Black, Indigenous, and other People of Color (BIPOC) communities. This is why taking the time to develop relationships and partnerships with the communities our decisions and actions may impact is crucial.\nIncluding partners with the expertise that can only come from lived experiences in our projects from the beginnig, and giving that expertise the same weight as academic or research expertise can broaden our perspectives and help prevent publication (and other biases) from negatively impacting our project, process and advancement of equity.\nSee the Planning section on collecting expert input for more guidance.\n\n\nRegression Towards the Mean\n\nWhen something happens that’s unusually good or bad, it will revert back towards the average over time.\nA key component of this fallacy is that random chance influences the outcome. When looking at our data through an equity lens, we need to remember that racism and injustice are central to our collective history and can be traced back to before the founding of our country. We live and work in institutions and systems that have inherited those unjust decisions and processes. We know that, as government representatives, if we’re not clear and intentional about advancing racial equity in the work we do, it won’t happen and we will continue to perpetuate racial inequity. In other words, because of the unjust systems in which we work, we cannot depend on this fallacy and wait for the unusually “bad” or unjust results/trends to correct themselves or revert back to more equitable trends. Because government agencies created and perpetuated environmental racism, it is our responsibility to proactively advance racial equity and justice in all the work we do.\nIf our collective history and its deep connection to racism and injustice are not familiar to you - we recommend taking the Advancing Racial Equity training series offered by the Water Boards Training Academy and reviewing the GARE Framework: Normalize, Organize, and Operationalize. More details on both of these actions and others can be found on the Getting Started page. |\n\n\nSampling Bias\n\nDrawing conclusions from a set of data that isn’t representative of the population you’re trying to understand.\nDuring your Plan and Prepare you will think through the type of data that’s needed to adequitely represent the populations related to your project objectives. If the data available is not adequately representative, then you may need to revise the types of questions you have of the data (and analytical methods). Or, it might be worth considering collecting the data yourself or with partners.\nNote that this principle and others can also be applied to surveys. Keep that in mind as you design, deliver, and analyze survey results.\n\n\nSimpson’s Paradox\n\nWhen a trend appears in different subsets of data but disappears or reverses when the groups are combined.\nThis is critical when trying to understand and quantify how environmental outcomes might impact different groups. During your Plan and Prepare phase, think through the different ways you might aggregate or disaggregate your data and consider what is appropriate for the questions you have.\nWithout disaggregating data by subgroup, analysis can unintentionally gloss over inequity and lead to invisible experiences. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled. (Centering Racial Equity Throughout Data Integration)\nEach decision has its benefits and limitations. No matter what analytical decision you make, it’s important to clearly document the reasoning behind your decision.\n\n\nSurvivorship Bias\n\nDrawing conclusions from an incomplete set of data, because that data has ‘survived’ some selection criteria.\nAs soon as we get our hands on a dataset it can be tempting to dive into analysis without taking the time to consider that data in the context of our project goals and objectives. Sometimes the data we need to understand every aspect of ideal our project objectives simply does not exist or is not accessible.\nIt’s important to take the time during the Plan and Prepare phase to understand what data are needed to achieve the objectives of your project, what data are actually available, and to understand what all of that means for your Collect and Process phase, and how that might impact your interpretation.",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html#additional-resources",
    "href": "assure-analyze/analysis.html#additional-resources",
    "title": "Data Analysis",
    "section": "",
    "text": "College of Water Informatics Machine Learning Handbook\nFlood M. D., Lemieux V. L., Varga M., and Wong B. L. W. (2016) ‘The application of visual analytics to financial stability monitoring’, Journal of Financial Stability\nDataCamp (2023) The Data-Information-Knowledge-Wisdom Pyramid",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "assure-analyze/analysis.html#footnotes",
    "href": "assure-analyze/analysis.html#footnotes",
    "title": "Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content in this section has been informed by or pulled directly from: Geckoboard’s Data Fallacies↩︎",
    "crumbs": [
      "Assure & Analyze",
      "Data Analysis"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here you will find a curated list of resources related to the development, implementation and scaling of the principles and practices outlined in this Equity Data Handbook.\n\n\n\nGovOps medium article on Data Ethics\nFederal data ethics framework\nData Ethics Canvas\nEthics Framework (CalData); Framework at a Glance\nLooking at Data Through an Equity Lens\nAcademic Data Science Alliance Data Science Ethos\nFive Ethical Risks to Consider before Filling Missing Race and Ethnicity Data\n\n\n\n\n\nData Equity Framework\nPrinciples for Advancing Equitable Data Practice\nCalEPA Practices to Advance Racial Equity in Workforce Planning\nPrinciples for Using Public Health Data to Drive Equity: A guide to embedding equitable practices throughout the data life cycle\nCalifornia Data Strategy (2020) and ensuing Report (2024)\nCalifornia Indian Water Rights Study\nAntiracist Data Project Survey and Focus Group Findings\nThe National Equity Atlas\nMN DOT Governance and Best Practices\nConsidering Equity in Community Impact Analysis for Projects\n\n\n\n\n\nA Toolkit for Centering Racial Equity Throughout Data Integration\nBeyond Compliance Network Advocacy Toolkit\nThe Water Boards Office of Research Planning and Performance Water Conservation and Urban Water folks tool\nWater Boards Division of Drinking Water SAFER Outreach Tool\n\n\n\n\n\nWater Boards Racial Equity Data Resource Hub\nWater Boards Environmental Justice Roundtable Resource Catalog\nCalifornia Office of Data and Innovation: Innovation Hub",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#data-ethics",
    "href": "resources.html#data-ethics",
    "title": "Resources",
    "section": "",
    "text": "GovOps medium article on Data Ethics\nFederal data ethics framework\nData Ethics Canvas\nEthics Framework (CalData); Framework at a Glance\nLooking at Data Through an Equity Lens\nAcademic Data Science Alliance Data Science Ethos\nFive Ethical Risks to Consider before Filling Missing Race and Ethnicity Data",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#best-practices-and-frameworks",
    "href": "resources.html#best-practices-and-frameworks",
    "title": "Resources",
    "section": "",
    "text": "Data Equity Framework\nPrinciples for Advancing Equitable Data Practice\nCalEPA Practices to Advance Racial Equity in Workforce Planning\nPrinciples for Using Public Health Data to Drive Equity: A guide to embedding equitable practices throughout the data life cycle\nCalifornia Data Strategy (2020) and ensuing Report (2024)\nCalifornia Indian Water Rights Study\nAntiracist Data Project Survey and Focus Group Findings\nThe National Equity Atlas\nMN DOT Governance and Best Practices\nConsidering Equity in Community Impact Analysis for Projects",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#toolkits",
    "href": "resources.html#toolkits",
    "title": "Resources",
    "section": "",
    "text": "A Toolkit for Centering Racial Equity Throughout Data Integration\nBeyond Compliance Network Advocacy Toolkit\nThe Water Boards Office of Research Planning and Performance Water Conservation and Urban Water folks tool\nWater Boards Division of Drinking Water SAFER Outreach Tool",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#resource-hubs",
    "href": "resources.html#resource-hubs",
    "title": "Resources",
    "section": "",
    "text": "Water Boards Racial Equity Data Resource Hub\nWater Boards Environmental Justice Roundtable Resource Catalog\nCalifornia Office of Data and Innovation: Innovation Hub",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "use-cases/demographics.html",
    "href": "use-cases/demographics.html",
    "title": "Demographics",
    "section": "",
    "text": "This page is still under development - new content may be added and existing content may change at any time without notice\n\n\n\n\n\n\nDepending on what demographics data sources and software you decide to use, the methods needed to combine, overlay, or compare with the data you are interested in may vary. See the Demographics Data section of the Data Collection page for general guidance and things to consider when using demographics data. See below for step by step guidance on how to download and compare demographics data to point, line, and polygon data types programmatically using R or Python, or manually using ESRI tools.\n\n\n\n\n\n\nImportant Reminders Before You Dive In\n\n\n\nData are NOT people - We need to use these data to get a better understanding of what’s going on in our communities, but the data (at best) only represent a sample of the communitie’s population and in no way reflect everyone or their lived experiences.\nThere’s no such thing as “equity data” - how we use data, interpret it, and act on what we learn makes our use equitable (or not). Simply including demographics data in your project’s analysis or data products does not make those resources equitable - to operationalize equity we need to take actions and make decisions in ways to advance equitable outcomes.\nThe data you’re using has limitations, be sure you know what they are before moving forward - as discussed on the Data Collection page, all data have limitations, and that is particularly true for demographics data. Be sure you have a clear and comprehensive understanding of the limitations that apply to the specific datasets you’re using so you can collect and eventually process and analyze those data in ways that are appropriate.\n\n\n\n\nR1 is a free software environment for statistical computing and graphics (Training Resources). RStudio is an integrated development environment (IDE) that includes is a set of tools and user interfaces built to help you be more productive with R and Python.\n\n\nIf you haven’t already, you will need to install R and RStudio. Water Boards staff should be able to do so through the Software Center. Also see step by step installiation instructions for outside of the Software Center environment.\nIf you will be using U.S. Census data regularly and will be accessing and analyzing it programmatically (e.g. using R or Python), you will also need to Request a U.S. Census Data API Key.\n\n\n\nR Packages / Libraries are extensions to the R statistical programming language that contain code, data, and documentation in a standardized collection format that can be installed by users of R.\n\n\nYou must install any packages you will use on your computer before you can load them. You only need to install a package once; if you have already installed the below packages you can skip this step and proceed to the Load Packages step.\n1install.packages(\"here\")\n2install.packages(\"tidyverse\")\n3install.packages(\"ggplot2\")\n4install.packages(\"tidycensus\")\n5install.packages(\"sf\")\n6install.packages(\"patchwork\")\n\n1\n\nThe here package enables easy, shareable and reproducible file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files. Package Documentation\n\n2\n\nThe tidyverse package installs all packages in the tidyverse at once, including: ggplot2, dplyr, tidyr, among others. Package Documentation\n\n3\n\nThe ggplot2 package is used to create graphics and data visualizations. Package Documentation\n\n4\n\nThe tidycensus package allows users to interface with the US Census Bureau’s decennial Census and five-year American Community APIs and return tidyverse-ready data frames. Package Documentation\n\n5\n\nThe sf package provides access to simple features in R so users can work with geographic vectors. Package Documentation\n\n6\n\nThe patchwork package is an extension of the ggplot2 package, designed to simplify the process of combining multiple plots into a single layout. Package Documentation\n\n\n\n\n\nYou must load all packages you will use for an analysis before each use.\nlibrary(here) \nlibrary(ggplot2) \nlibrary(tidyverse) \nlibrary(tidycensus) \nlibrary(sf) \nlibrary(patchwork)\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLEFT OFF HERE\npull over content + code from:\nhttps://datamade.github.io/waterboard-coaching/\nhttps://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html\nhttps://daltare.github.io/example-census-race-ethnicity-calculation/example_census_race_ethnicity_calculation.html\n\n\n7census_api_key(\" \")\n\n7\n\nmodify code to pull from api key located in file to increase security\n\n\n\ngeographies\nlast data update\nage categories\n\nTake some time to review the Concept and Labels in the table -\nsummary_var = \"P2_001N\" # Total population\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Water Boards and Demographic Data for Equity. Jun 2024. Hannah Cushman Garland. State Water Board Racial Equity Data Subcommittee Webinar. Recording | Download and Use the Code | View Code\nEstimating Demographics of Custom Spatial Features is another detailed example of how to use the R programming language to estimate demographics and other characteristics with U.S. census data to be used for custom spatial features, and can be tailored to programs with the help of a data scientist proficient in R and staff familiar with the program.\n\n\n\n\n\nPython is a….\n\n\n\nESRI is a….",
    "crumbs": [
      "Use Cases",
      "Demographics"
    ]
  },
  {
    "objectID": "use-cases/demographics.html#r-data-integration-example",
    "href": "use-cases/demographics.html#r-data-integration-example",
    "title": "Demographics",
    "section": "",
    "text": "R1 is a free software environment for statistical computing and graphics (Training Resources). RStudio is an integrated development environment (IDE) that includes is a set of tools and user interfaces built to help you be more productive with R and Python.\n\n\nIf you haven’t already, you will need to install R and RStudio. Water Boards staff should be able to do so through the Software Center. Also see step by step installiation instructions for outside of the Software Center environment.\nIf you will be using U.S. Census data regularly and will be accessing and analyzing it programmatically (e.g. using R or Python), you will also need to Request a U.S. Census Data API Key.\n\n\n\nR Packages / Libraries are extensions to the R statistical programming language that contain code, data, and documentation in a standardized collection format that can be installed by users of R.\n\n\nYou must install any packages you will use on your computer before you can load them. You only need to install a package once; if you have already installed the below packages you can skip this step and proceed to the Load Packages step.\n1install.packages(\"here\")\n2install.packages(\"tidyverse\")\n3install.packages(\"ggplot2\")\n4install.packages(\"tidycensus\")\n5install.packages(\"sf\")\n6install.packages(\"patchwork\")\n\n1\n\nThe here package enables easy, shareable and reproducible file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files. Package Documentation\n\n2\n\nThe tidyverse package installs all packages in the tidyverse at once, including: ggplot2, dplyr, tidyr, among others. Package Documentation\n\n3\n\nThe ggplot2 package is used to create graphics and data visualizations. Package Documentation\n\n4\n\nThe tidycensus package allows users to interface with the US Census Bureau’s decennial Census and five-year American Community APIs and return tidyverse-ready data frames. Package Documentation\n\n5\n\nThe sf package provides access to simple features in R so users can work with geographic vectors. Package Documentation\n\n6\n\nThe patchwork package is an extension of the ggplot2 package, designed to simplify the process of combining multiple plots into a single layout. Package Documentation\n\n\n\n\n\nYou must load all packages you will use for an analysis before each use.\nlibrary(here) \nlibrary(ggplot2) \nlibrary(tidyverse) \nlibrary(tidycensus) \nlibrary(sf) \nlibrary(patchwork)\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLEFT OFF HERE\npull over content + code from:\nhttps://datamade.github.io/waterboard-coaching/\nhttps://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html\nhttps://daltare.github.io/example-census-race-ethnicity-calculation/example_census_race_ethnicity_calculation.html\n\n\n7census_api_key(\" \")\n\n7\n\nmodify code to pull from api key located in file to increase security\n\n\n\ngeographies\nlast data update\nage categories\n\nTake some time to review the Concept and Labels in the table -\nsummary_var = \"P2_001N\" # Total population\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Water Boards and Demographic Data for Equity. Jun 2024. Hannah Cushman Garland. State Water Board Racial Equity Data Subcommittee Webinar. Recording | Download and Use the Code | View Code\nEstimating Demographics of Custom Spatial Features is another detailed example of how to use the R programming language to estimate demographics and other characteristics with U.S. census data to be used for custom spatial features, and can be tailored to programs with the help of a data scientist proficient in R and staff familiar with the program.",
    "crumbs": [
      "Use Cases",
      "Demographics"
    ]
  },
  {
    "objectID": "use-cases/demographics.html#python-data-integration-example",
    "href": "use-cases/demographics.html#python-data-integration-example",
    "title": "Demographics",
    "section": "",
    "text": "Python is a….",
    "crumbs": [
      "Use Cases",
      "Demographics"
    ]
  },
  {
    "objectID": "use-cases/demographics.html#esri-data-integration-example",
    "href": "use-cases/demographics.html#esri-data-integration-example",
    "title": "Demographics",
    "section": "",
    "text": "ESRI is a….",
    "crumbs": [
      "Use Cases",
      "Demographics"
    ]
  },
  {
    "objectID": "use-cases/demographics.html#footnotes",
    "href": "use-cases/demographics.html#footnotes",
    "title": "Demographics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content in this section has been informed by, adapted or directly pulled from DataMade guidance, resources (e.g., Analysis Steps).↩︎",
    "crumbs": [
      "Use Cases",
      "Demographics"
    ]
  },
  {
    "objectID": "use-cases/swamp.html",
    "href": "use-cases/swamp.html",
    "title": "SWAMP",
    "section": "",
    "text": "California’s Surface Water Ambient Monitoring Program (SWAMP) was established by the California Legislature in 2000 (Assembly Bill 982; Ducheny 1999) to develop a comprehensive statewide surface water quality monitoring program to assess the condition of California’s surface waters and watersheds.\nThere are four statewide monitoring programs, including:\n\nThe Bioaccumulation Monitoring Program\nThe Bioassessment Program\nThe Freshwater and Estuarine Harmful Algal Bloom (FHAB) Program\nThe Toxicology and Contaminants Program\n\nSWAMP is also leading the SWAMP environmental DNA Metabarcoding Monitoring and Analysis Project (SeMMAP).\nThe California State Water Resources Control Board adopted principles of open data (Assembly Bill 1755 (Bloom, 2016); State Board Resolution 2018-0032) and racial equity (State Water Board Resolution 2021-0050) and directed programs to strategically implement those principles into their activities. In preparation for and response to these Resolutions, SWAMP has dedicated resources to make its monitoring data open, accessible, and used. Moving beyond basic open data requirements, SWAMP has embraced open science and racial equity as core, interconnected values and has begun to operationalize and implement them into their work.\nBelow are use cases of how SWAMP statewide programs and projects are working to operationalize open science and equity principles and practices into all phases of the data life cycle. Each program and project have differing levels of resources, capacity, and partnerships which have resulted in differing approaches. SWAMP does not claim to have finished operationalizing open science and equity principles and practices into all phases of the data life cycle. Rather, SWAMP sees each of the use cases below as iterative, living, and continuously improving works in progress.\n\n\n\nThe Bioaccumulation Monitoring Program funds and coordinates the collection and analysis of tissue from fish (and sometimes shellfish) to understand which fish are safer to eat in water bodies throughout the state.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Program Realignment process focuses on improving the Plan and Prepare, Collect and Process, and Publish and Share phases of the data life cycle so that the data and projects the program generates can support the advancement of equity outcomes throughout the state.\n\n\nThe Bioaccumulation Monitoring Program has realized several successes since its inception in 2006, including forming and leading the Safe to Eat Workgroup, establishing comprehensive and robust bioaccumulation monitoring methodologies, and generating data that is regularly used in the California Integrated Report process and to create Fish Consumption Advisories statewide.\nIn 2020, SWAMP evaluated the Program and determined that, although it has had several successes, to fully align with and achieve its original mission it must address three key issues:\n\nThe Program needs to develop and build stronger relationships with California Native American Tribes (tribes) and communities.\nSignificant data and information gaps remain regarding the question: “Is it safe to eat fish and shellfish from our waters?”, especially for waterbodies or species that are important for subsistence by traditionally underrepresented communities, as well as tribal traditions, culture, and subsistence. \nThe Program needs to better collaborate and connect with other Water Boards Divisions, Regions, and programs.\n\nThe Program Realignment process was created to begin to address these issues while balancing and maintaining core aspects of the statewide and regional program efforts, implementing Realignment efforts, and the limited availability of Program and implementation resources. To maintain this balance, the Realignment process was designed to be implemented through a cyclical approach, where a single Realignment cycle consists of a three-year process focused in a single Water Board Region.\nDuring the three year process, Program, Tribal, community and other government and monitoring partners invest time to meet (via advosory meetings or workshops) to:\n\nbuild relationships and trust among Program, Tribal government and community based organization representatives, as well as with state and federal agency and other monitoring partners\nlearn more about and document Tribal and community monitoring, data, and information needs\nwork with Tribal and community partners to determine how a percentage of the Program’s budget will be used to fill priority monitoring, data, and information gaps\n\nFor more information see the Bioaccumulation Monitoring Program Realignment Webpage.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Tribally-centered Training Series focuses on improving the Publish and Share phases of the data life cycle so that we can continue to build relationships and trust with our Tribal Government partners, and grow the bioaccumulation monitoring community. All of which will ultimately result in a more equitable Program and the advancement of equity outcomes throughout the state.\n\n\nThe purpose of this training series is to support California Native American Tribes in developing programs in the areas of bioaccumulation monitoring, data analysis, and data use processes, and enable Tribes and other bioaccumulation monitoring groups to adopt those processes into their workflows and more easily partner with the Program and the Safe to Eat Workgroup.\nThe training series idea was brought to the program by Tribal partners during early Realignment discussions and the Progam decided to invest resources into developing the training series with our Tribal partners as another way the program can operationalize open science and equity principles in the Program.\nKey decisions were made with equity in mind:\n\nThe training series would be co-created with Tribal partners. Course outlines were to be approved by Tribal partners, and slides were to be shared with Tribal partners ahead of each course with enough time for them to review and provide feedback on content.\nCourses would make time and space for Tribal monitoring experts to share their perspectives and expertise in a way that is comfortable for Tribal contributors. In some cases this meant Tribal experts would lead instruction, in others it meant reserving time for open and tribally-led discussion about topics.\nAll courses would be virtual, less than three hours in length, and free to attend, to increase access to live attendance.\nCourses would be recorded and posted online after the training, along with training slides, to increase access to content for those that are not able to attend live.\nCourse faciliators would offer to pause the recording during discussion, upon request of an attendee, or to delete discussions sections from recordings before posting. This can help make attendees feel more open to contributing to discussions.\n\nFor more information see the Bioaccumulation Monitoring Program Training Series Webpage\n\n\n\n\nThe mission of the FHAB Program is to support the protection, prevention, and reduction of health risks of algal blooms impacting humans, animals, and the environment. The program coordinates and provides resources for monitoring, response, and research to collect quality data to inform mitigation and management decisions to address drivers of FHABs statewide.  \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Partner Monitoring Program focuses on improving the Collect and Process phases of the data life cycle so that how we collect data is more equitable.\n\n\nThe FHAB Partner Monitoring Program involves our SWAMP FHAB Program coordinating monitoring with our partners and funding lab analysis for monitoring conducted by partners during the HAB season. Partners can include non-profit organizations, California Tribes, counties, cities, bi-state collaborations, and others.\nBy covering the costs of our partner’s laboratory analyses for regular monitoring during the HAB season, we hope to:\n\nbuild relationships and trust with partners\nincrease coordination among partners so our collective monitoring resources are used more effectively\nlower financial and logistical barriers for partners to collect data of importance to them\nincrease the amount of quality data that comes in throughout the FHAB season statewide\n\nFor more information see the FHAB Partner Monitoring Program Webpage\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Pre-Holiday Assessments focuses on improving the Collect and Process phases of the data life cycle so that how we collect data is more equitable and protective of public health during high risk times.\n\n\nSimilar to the FHAB Partner Monitoring Program, the FHAB Pre-holiday Assessments involve working with partners to conduct sampling at popular water bodies prior to major holiday weekends, and working with waterbody managers and the county to determine most appropriate signage to inform visitors of potential HABs.\nBy leading coordination efforts with our partners, and covering the costs of our partner’s laboratory analyses for targeted monitoring before major holiday weekends, we hope to:\n\nbuild relationships and trust with partners and the public\nincrease coordination among partners so our collective monitoring resources are used more effectively\nlower financial and logistical barriers for partners to collect data of importance to them\nincrease the amount of quality data that comes in before major holiday weekends (i.e., Memorial Day, Independence Day, Labor Day)\nincrease the consistency of communication critical to protect public health\n\nFor more information see the FHAB Pre-Holiday Assessment Webpage\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Training Resources focus on improving the Publish and Share phases of the data life cycle so that we can continue to build relationships and trust with our partners, and grow the FHAB monitoring community. All of which will ultimately result in a more equitable Program, the advancement of equity outcomes, and protection of public health throughout the state.\n\n\nWith greater than 3,000 lakes, 190,000 river miles, diverse ecosystems ranging from deserts to temperate rainforests, and over 40 million inhabitants, California faces complex monitoring and management issues for FHABs.\nThe purpose of providing free detailed guidance and training resources online and via live in-person or remote training by SWAMP FHAB Staff, is to make it easier for partners to:\n\nbuild capacity within their own organizations and governments\nconduct their own high-quality monitoring in areas of interest to them\nlower barriers to partnership with the FHAB Program\n\nFor more information see the FHAB Program Wiki.\n\n\n\n\nSeMMAP is exploring how we might use eDNA to augment, complement, and add to other statewide and regional monitoring programs at the Water Boards, with specific goals including:\n\nExplore the use of eDNA in SWAMP monitoring at the Water Boards\nBuild a diverse community of practice in CA\nDevelop Guidance for eDNA monitoring projects\nUnderstand how biodiversity data can be used to assess overall ecosystem health and how the biological, physical and anthropogenic environments interact\n\n\n\n\n\n\n\nNote\n\n\n\nThe SWAMP eDNA Metabarcoding Monitoring and Analysis Project (SeMMAP) has worked on integrating equity into all phases of the data life cycle, with particular focus on the Plan and Prepare, Collect and Process, Assure & Analyze, Preserve & Store, Publish and Share phases so that the data and data products the project generates can support the advancement of equity outcomes throughout the state.\n\n\nAs SWAMP was developing SeMMAP, key decisions were made with equity in mind, including:\n\nPrioritizing engagement and building relationships with our Tribal and community partners\nMaking time to discuss the data sharing and use process, and to address partner concerns around data management, control, and sovereignty\nLowering financial and logistical barriers for partners to collect data of importance to them by: paying for kits and analysis, handling kit distribution, leading data management and quality assurance process\nSharing data as soon as it’s available via an open, accessible, and interactive dashboard that includes a partner project gallery that enables partners to view the data that they collected and share how they are using their eDNA data to advance their water quality monitoring goals\nProviding free detailed guidance and training resources online to make it easier for partners to:\n\nbuild capacity within their own organizations and governments\nconduct their own high-quality monitoring in areas of interest to them\nlower barriers to partnership with SWAMP\n\n\n\n\n\n\nCA Surface Water Ambient Monitoring Programs: Partnering with Tribes for a more open, inclusive, and equitable future. Oct 2024. Anna Holder. Tribal EPA & U.S. EPA Region 9 Annual Conference. Recording.\nOperationalizing Open Science and Equity in California’s Surface Water Ambient Monitoring Programs. May 2022. Anna Holder. Joint Aquatic Sciences Meeting. Recording.",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "use-cases/swamp.html#background",
    "href": "use-cases/swamp.html#background",
    "title": "SWAMP",
    "section": "",
    "text": "California’s Surface Water Ambient Monitoring Program (SWAMP) was established by the California Legislature in 2000 (Assembly Bill 982; Ducheny 1999) to develop a comprehensive statewide surface water quality monitoring program to assess the condition of California’s surface waters and watersheds.\nThere are four statewide monitoring programs, including:\n\nThe Bioaccumulation Monitoring Program\nThe Bioassessment Program\nThe Freshwater and Estuarine Harmful Algal Bloom (FHAB) Program\nThe Toxicology and Contaminants Program\n\nSWAMP is also leading the SWAMP environmental DNA Metabarcoding Monitoring and Analysis Project (SeMMAP).\nThe California State Water Resources Control Board adopted principles of open data (Assembly Bill 1755 (Bloom, 2016); State Board Resolution 2018-0032) and racial equity (State Water Board Resolution 2021-0050) and directed programs to strategically implement those principles into their activities. In preparation for and response to these Resolutions, SWAMP has dedicated resources to make its monitoring data open, accessible, and used. Moving beyond basic open data requirements, SWAMP has embraced open science and racial equity as core, interconnected values and has begun to operationalize and implement them into their work.\nBelow are use cases of how SWAMP statewide programs and projects are working to operationalize open science and equity principles and practices into all phases of the data life cycle. Each program and project have differing levels of resources, capacity, and partnerships which have resulted in differing approaches. SWAMP does not claim to have finished operationalizing open science and equity principles and practices into all phases of the data life cycle. Rather, SWAMP sees each of the use cases below as iterative, living, and continuously improving works in progress.",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "use-cases/swamp.html#bioaccumulation-monitoring-program",
    "href": "use-cases/swamp.html#bioaccumulation-monitoring-program",
    "title": "SWAMP",
    "section": "",
    "text": "The Bioaccumulation Monitoring Program funds and coordinates the collection and analysis of tissue from fish (and sometimes shellfish) to understand which fish are safer to eat in water bodies throughout the state.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Program Realignment process focuses on improving the Plan and Prepare, Collect and Process, and Publish and Share phases of the data life cycle so that the data and projects the program generates can support the advancement of equity outcomes throughout the state.\n\n\nThe Bioaccumulation Monitoring Program has realized several successes since its inception in 2006, including forming and leading the Safe to Eat Workgroup, establishing comprehensive and robust bioaccumulation monitoring methodologies, and generating data that is regularly used in the California Integrated Report process and to create Fish Consumption Advisories statewide.\nIn 2020, SWAMP evaluated the Program and determined that, although it has had several successes, to fully align with and achieve its original mission it must address three key issues:\n\nThe Program needs to develop and build stronger relationships with California Native American Tribes (tribes) and communities.\nSignificant data and information gaps remain regarding the question: “Is it safe to eat fish and shellfish from our waters?”, especially for waterbodies or species that are important for subsistence by traditionally underrepresented communities, as well as tribal traditions, culture, and subsistence. \nThe Program needs to better collaborate and connect with other Water Boards Divisions, Regions, and programs.\n\nThe Program Realignment process was created to begin to address these issues while balancing and maintaining core aspects of the statewide and regional program efforts, implementing Realignment efforts, and the limited availability of Program and implementation resources. To maintain this balance, the Realignment process was designed to be implemented through a cyclical approach, where a single Realignment cycle consists of a three-year process focused in a single Water Board Region.\nDuring the three year process, Program, Tribal, community and other government and monitoring partners invest time to meet (via advosory meetings or workshops) to:\n\nbuild relationships and trust among Program, Tribal government and community based organization representatives, as well as with state and federal agency and other monitoring partners\nlearn more about and document Tribal and community monitoring, data, and information needs\nwork with Tribal and community partners to determine how a percentage of the Program’s budget will be used to fill priority monitoring, data, and information gaps\n\nFor more information see the Bioaccumulation Monitoring Program Realignment Webpage.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Tribally-centered Training Series focuses on improving the Publish and Share phases of the data life cycle so that we can continue to build relationships and trust with our Tribal Government partners, and grow the bioaccumulation monitoring community. All of which will ultimately result in a more equitable Program and the advancement of equity outcomes throughout the state.\n\n\nThe purpose of this training series is to support California Native American Tribes in developing programs in the areas of bioaccumulation monitoring, data analysis, and data use processes, and enable Tribes and other bioaccumulation monitoring groups to adopt those processes into their workflows and more easily partner with the Program and the Safe to Eat Workgroup.\nThe training series idea was brought to the program by Tribal partners during early Realignment discussions and the Progam decided to invest resources into developing the training series with our Tribal partners as another way the program can operationalize open science and equity principles in the Program.\nKey decisions were made with equity in mind:\n\nThe training series would be co-created with Tribal partners. Course outlines were to be approved by Tribal partners, and slides were to be shared with Tribal partners ahead of each course with enough time for them to review and provide feedback on content.\nCourses would make time and space for Tribal monitoring experts to share their perspectives and expertise in a way that is comfortable for Tribal contributors. In some cases this meant Tribal experts would lead instruction, in others it meant reserving time for open and tribally-led discussion about topics.\nAll courses would be virtual, less than three hours in length, and free to attend, to increase access to live attendance.\nCourses would be recorded and posted online after the training, along with training slides, to increase access to content for those that are not able to attend live.\nCourse faciliators would offer to pause the recording during discussion, upon request of an attendee, or to delete discussions sections from recordings before posting. This can help make attendees feel more open to contributing to discussions.\n\nFor more information see the Bioaccumulation Monitoring Program Training Series Webpage",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "use-cases/swamp.html#fhab-program",
    "href": "use-cases/swamp.html#fhab-program",
    "title": "SWAMP",
    "section": "",
    "text": "The mission of the FHAB Program is to support the protection, prevention, and reduction of health risks of algal blooms impacting humans, animals, and the environment. The program coordinates and provides resources for monitoring, response, and research to collect quality data to inform mitigation and management decisions to address drivers of FHABs statewide.  \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Partner Monitoring Program focuses on improving the Collect and Process phases of the data life cycle so that how we collect data is more equitable.\n\n\nThe FHAB Partner Monitoring Program involves our SWAMP FHAB Program coordinating monitoring with our partners and funding lab analysis for monitoring conducted by partners during the HAB season. Partners can include non-profit organizations, California Tribes, counties, cities, bi-state collaborations, and others.\nBy covering the costs of our partner’s laboratory analyses for regular monitoring during the HAB season, we hope to:\n\nbuild relationships and trust with partners\nincrease coordination among partners so our collective monitoring resources are used more effectively\nlower financial and logistical barriers for partners to collect data of importance to them\nincrease the amount of quality data that comes in throughout the FHAB season statewide\n\nFor more information see the FHAB Partner Monitoring Program Webpage\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Pre-Holiday Assessments focuses on improving the Collect and Process phases of the data life cycle so that how we collect data is more equitable and protective of public health during high risk times.\n\n\nSimilar to the FHAB Partner Monitoring Program, the FHAB Pre-holiday Assessments involve working with partners to conduct sampling at popular water bodies prior to major holiday weekends, and working with waterbody managers and the county to determine most appropriate signage to inform visitors of potential HABs.\nBy leading coordination efforts with our partners, and covering the costs of our partner’s laboratory analyses for targeted monitoring before major holiday weekends, we hope to:\n\nbuild relationships and trust with partners and the public\nincrease coordination among partners so our collective monitoring resources are used more effectively\nlower financial and logistical barriers for partners to collect data of importance to them\nincrease the amount of quality data that comes in before major holiday weekends (i.e., Memorial Day, Independence Day, Labor Day)\nincrease the consistency of communication critical to protect public health\n\nFor more information see the FHAB Pre-Holiday Assessment Webpage\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe FHAB Training Resources focus on improving the Publish and Share phases of the data life cycle so that we can continue to build relationships and trust with our partners, and grow the FHAB monitoring community. All of which will ultimately result in a more equitable Program, the advancement of equity outcomes, and protection of public health throughout the state.\n\n\nWith greater than 3,000 lakes, 190,000 river miles, diverse ecosystems ranging from deserts to temperate rainforests, and over 40 million inhabitants, California faces complex monitoring and management issues for FHABs.\nThe purpose of providing free detailed guidance and training resources online and via live in-person or remote training by SWAMP FHAB Staff, is to make it easier for partners to:\n\nbuild capacity within their own organizations and governments\nconduct their own high-quality monitoring in areas of interest to them\nlower barriers to partnership with the FHAB Program\n\nFor more information see the FHAB Program Wiki.",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "use-cases/swamp.html#semmap",
    "href": "use-cases/swamp.html#semmap",
    "title": "SWAMP",
    "section": "",
    "text": "SeMMAP is exploring how we might use eDNA to augment, complement, and add to other statewide and regional monitoring programs at the Water Boards, with specific goals including:\n\nExplore the use of eDNA in SWAMP monitoring at the Water Boards\nBuild a diverse community of practice in CA\nDevelop Guidance for eDNA monitoring projects\nUnderstand how biodiversity data can be used to assess overall ecosystem health and how the biological, physical and anthropogenic environments interact\n\n\n\n\n\n\n\nNote\n\n\n\nThe SWAMP eDNA Metabarcoding Monitoring and Analysis Project (SeMMAP) has worked on integrating equity into all phases of the data life cycle, with particular focus on the Plan and Prepare, Collect and Process, Assure & Analyze, Preserve & Store, Publish and Share phases so that the data and data products the project generates can support the advancement of equity outcomes throughout the state.\n\n\nAs SWAMP was developing SeMMAP, key decisions were made with equity in mind, including:\n\nPrioritizing engagement and building relationships with our Tribal and community partners\nMaking time to discuss the data sharing and use process, and to address partner concerns around data management, control, and sovereignty\nLowering financial and logistical barriers for partners to collect data of importance to them by: paying for kits and analysis, handling kit distribution, leading data management and quality assurance process\nSharing data as soon as it’s available via an open, accessible, and interactive dashboard that includes a partner project gallery that enables partners to view the data that they collected and share how they are using their eDNA data to advance their water quality monitoring goals\nProviding free detailed guidance and training resources online to make it easier for partners to:\n\nbuild capacity within their own organizations and governments\nconduct their own high-quality monitoring in areas of interest to them\nlower barriers to partnership with SWAMP",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "use-cases/swamp.html#additional-resources",
    "href": "use-cases/swamp.html#additional-resources",
    "title": "SWAMP",
    "section": "",
    "text": "CA Surface Water Ambient Monitoring Programs: Partnering with Tribes for a more open, inclusive, and equitable future. Oct 2024. Anna Holder. Tribal EPA & U.S. EPA Region 9 Annual Conference. Recording.\nOperationalizing Open Science and Equity in California’s Surface Water Ambient Monitoring Programs. May 2022. Anna Holder. Joint Aquatic Sciences Meeting. Recording.",
    "crumbs": [
      "Use Cases",
      "SWAMP"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "Publication & Sharing",
    "section": "",
    "text": "By now, you’ve completed some or all of your product development steps (Data Analysis, Data Visualization) and are ready to publish and share product components. Congratulations!\nThis phase of the data life cycle involves making the project’s data and products open and accessible as appropriate.\nAlso see a short list of best practices for this phase.\n\n\n\n\n\n\nTip\n\n\n\nJust like the Preservation & Storage phase, this content should be reviewed during the Plan & Prepare Phase of your project! Depending on your project needs, you may even integrate your responses to questions below into your project’s data management plan. Doing so will make it easier for all team members to prepare their project components accordingly and will save you time when implementing this phase of the project.\n\n\n\n\nThe Water Boards typically make virtually all of the data we collect available to the public, with the exception of confidential information that should remain private and secure. As such, the openness and accessibility of your project’s data and resultant products should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.” This means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software and tools (e.g., Python, R, GitHub) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\nFrom a data science lens - Making data and products open and accessible (as appropriate) increases transparency of the work and makes it easier for future project team members (including yourself!) to reference, reuse, and build upon components in the future. This will reduce the amount of time future teams need to spend looking for and reinventing proverbial wheels you already invested time in developing, thus increasing future efficiency and reproducibility and decreasing frustration and workload for all.\nFrom an equity lens - In addition to the benefits listed above, making data and products open and accessible (as appropriate) can contribute to trust building and future opportunities for collaboration between the project team, partners, and communities, enable communities and partners to use the data for their interests and needs, and empower all to take data-informed actions that ultimately advance and operationalize equity in the institutions, communities, and systems in which we all live, work, and play.\n\n\n\nWhen getting ready to publish and share components or products of your data project, it’s important to be thoughtful of how you do so. Similar to the creation of data visualizations - plopping content or products on a webpage or data portal without context or care can (at best) be unhelpful and (at worst) actually perpetuate or reinforce inequity and injustice.\nAs you embark upon this exciting phase of your project’s data life cycle, be sure to keep the below considerations in mind and make publication and sharing choices that support the advancement of equity, inclusion, and justice.\n\n\nWe’ve said it before and we’ll repeat it again here: project teams should begin communication with communities and partners at the earliest stages of project development, and continue to communicate and check-in during each phase of implementation, including during this phase. What’s critical here is to communicate with communities and partners BEFORE any content is finalized or published to get a (hopefully another) round of critical feedback on the interpretation and framing of the project and its content.\nUnfortunately, it’s not uncommon for communities - particularly those that have been excluded, marginalized, neglected and face the impacts and burden of systemic racism and injustice - to get requests for data, information, or feedback during one phase of a government project, never hear back from the project team, and then learn after something is published that what they shared has been misrepresented, miscommunicated, or even shared when it should not have been. As one might expect, this is a very easy and fast way to erode trust, burn bridges, and ruin relationships that you have invested precious time, energy, and resources to build.\nUse this time to:\n\nConnect with those you have reached out to in previous phases of the project and share your publication, sharing, and communication plans for the project. Also see the notes on iteration in the Take a user-centered design approach section for more guidance.\nIntegrate the feedback you receive into the product(s) before publishing and sharing broadly. Remember, this can take some time to implement well - plan ahead so you aren’t rushed. Also see the notes on iteration in the Take a user-centered design approach section for more guidance.\nDiscuss whether a coordinated communication strategy across community and partner institutions can/should be developed and implemented. If appropriate and communities and partners on board, this could result in all parties involved (and their media / communication teams) distributing findings and products across their respective networks in a coordinated and consistent way, thus furthering the reach and potential impact of the project.\n\n\n\n\nIn addition to all of the reasons outlined in the above Why Publish & Share Data Products?section, the Urban Institute’s Principles for Advancing Equitable Data Practice notes that sharing data can reduce the burden of duplicate data collection on communities. Specifically:\n\nSome people and communities are consistently the targets of data collection and study, sometimes from organizations seeking the same information for similar purposes. Siloed data place an additional—and potentially unnecessary—burden on community members. Sharing nonconfidential data, when it is unlikely that it could lead to harm or add risks, may reduce the burden that individuals and communities experience from data collection\n\nAs you publish your project’s data and products, be sure to:\n\nPublish data and complete metadata in publicly accessible locations, such as Water Boards databases and/or the California Open Data Portal (or California State Geoportal), as appropriate. Also see the Publish Your Water Boards Data page for more guidance\nPublish or embed accessible products onto Water Boards webpages, as appropriate, including: planning or summary documents/reports, fact sheets, applications, presentation slides or recordings, etc.\nPublish any code and related software packages or products in the Water Boards GitHub, as appropriate, including: data and analysis workflow diagrams or related documentation, all application and source code files, and all data, analysis, interpretation, and visualization processing scripts. Also see the Open Source Code Handbook for more guidance.\nPublish any peer-reviewed literature in open access journals, as appropriate. Public data and records are public property, as appropriate. Therefore, any products developed by public agencies using those public resources should also be public property, as appropriate. The best way to ensure peer-reviewed literature remains accessible to the public is by publishing in open access journals or equivalent open and free to access publication avenues.\nEnsure all documentation associated with the project is written in plain, accessible, and inclusive language, as appropriate, including: metadata, ReadMe files, code comments, and anything else pertaining to the project components listed.\n\n\n\n\n\n\n\nRemember - Open does not always mean public\n\n\n\nThe Water Board’s Open Data Resolution urges us to make our data and products open and accessible to the public, as appropriate. Equally important is ensuring that the confidential components of our work are secured and protected appropriately. See the Data Preparation and Preservation & Storage pages for more detailed guidance.\n\n\n\n\n\nIn addition to all of the considerations outlined in the above Make data and products open and accessible to the public section, it’s also important to take time to make project, data, products, information and knowledge accessible to and useful for the communities and partners that contributed to or could be impacted by the project. The Urban Institute’s Principles for Advancing Equitable Data Practice notes that:\n\nAnalysts have the power to disrupt the dynamic of people’s having no ownership of and deriving no utility from what their data have produced. Ensuring that the results are communicated in a way that community members can use and understand is an important step toward equity.\n\nAs you implement other components of this phase, be sure to:\n\nConnect with those you have reached out to in previous phases of the project and walk through the data and products you plan to publish and share. Ideally, this will be wrapped into the conversations you have with these folks BEFORE publication to streamline conversations and reduce the burden of providing feedback.\nDiscuss whether the existing results and products are communicated in ways that resonate with and are useful to the community. If they are generous and able to tell you how to improve your communication, be sure to integrate that into existing products! See the Data Visualization for guidance on how to effectively communicate with data.\nDiscuss whether additional products or resources should be developed to improve communication and usefulness of the data and products for the community. It will be important to be clear about your capacity and limitations going into these conversations so you can manage expectations and only commit to actions and timelines you can realistically accomplish. Depending on what products you decide to develop, it might be helpful to co-create them with your community partners so that messaging and functionality is as useful and impactful as possible. Some products that might be considered for development include:\n\nA project fact sheet or brochure that describes the project, available resources, and what it means for the community\nA webpage update (or development of a new page) that makes it easy for communities to find the information and resources they need about the project and products\nA step by step guide, workshop, training, or video that walks people through how to use what has been developed and interpret the results or findings appropriately\nCustomized code or program that communities can use to pull data from databases and process it for their own data analysis and visualization needs\nA media and/or press kit, that would include resources for communities and partners to use to share information about the project and products, including:\n\nlanguage for press releases or social media posts, including:\n\nproject background, goals\nkey results, facts, or statistics\ncase studies\navailable resources, products, or services\ncontact details (name/email addresses, social media handles)\nrelevant hashtags\n\nfigures or images that are ready to be shared broadly, including the image titles, captions, and alt text that should accompany the use of each image\ninstructions on how to embed products into community-managed web-pages\nbranding guidelines\n\n\n\n\n\n\nTip\n\n\n\nAs you develop your media kit, be sure to work with your Water Boards Communications team to ensure all content is consistent with other Water Boards communications.\n\n\n\nAnything else communities and partners say they need to effectively use the data and products, you’ve invested time, energy, and resources to develop!",
    "crumbs": [
      "Publish & Share"
    ]
  },
  {
    "objectID": "share.html#why-publish-share-data-products",
    "href": "share.html#why-publish-share-data-products",
    "title": "Publication & Sharing",
    "section": "",
    "text": "The Water Boards typically make virtually all of the data we collect available to the public, with the exception of confidential information that should remain private and secure. As such, the openness and accessibility of your project’s data and resultant products should be in alignment with the Water Board’s Open Data Resolution: “Adopting Principles of Open Data as a Core Value and Directing Programs and Activities to Implement Strategic Actions to Improve Data Accessibility and Associated Innovation.” This means:\n\nDocumenting your process throughout the project so as to make it open, transparent, and reproducible\nUtilizing open data and open source software and tools (e.g., Python, R, GitHub) as much as possible\nMaking the data you use and code you develop transparent and accessible to the public after the project is complete, as appropriate\n\nFrom a data science lens - Making data and products open and accessible (as appropriate) increases transparency of the work and makes it easier for future project team members (including yourself!) to reference, reuse, and build upon components in the future. This will reduce the amount of time future teams need to spend looking for and reinventing proverbial wheels you already invested time in developing, thus increasing future efficiency and reproducibility and decreasing frustration and workload for all.\nFrom an equity lens - In addition to the benefits listed above, making data and products open and accessible (as appropriate) can contribute to trust building and future opportunities for collaboration between the project team, partners, and communities, enable communities and partners to use the data for their interests and needs, and empower all to take data-informed actions that ultimately advance and operationalize equity in the institutions, communities, and systems in which we all live, work, and play.",
    "crumbs": [
      "Publish & Share"
    ]
  },
  {
    "objectID": "share.html#project-publication-sharing-with-an-equity-lens",
    "href": "share.html#project-publication-sharing-with-an-equity-lens",
    "title": "Publication & Sharing",
    "section": "",
    "text": "When getting ready to publish and share components or products of your data project, it’s important to be thoughtful of how you do so. Similar to the creation of data visualizations - plopping content or products on a webpage or data portal without context or care can (at best) be unhelpful and (at worst) actually perpetuate or reinforce inequity and injustice.\nAs you embark upon this exciting phase of your project’s data life cycle, be sure to keep the below considerations in mind and make publication and sharing choices that support the advancement of equity, inclusion, and justice.\n\n\nWe’ve said it before and we’ll repeat it again here: project teams should begin communication with communities and partners at the earliest stages of project development, and continue to communicate and check-in during each phase of implementation, including during this phase. What’s critical here is to communicate with communities and partners BEFORE any content is finalized or published to get a (hopefully another) round of critical feedback on the interpretation and framing of the project and its content.\nUnfortunately, it’s not uncommon for communities - particularly those that have been excluded, marginalized, neglected and face the impacts and burden of systemic racism and injustice - to get requests for data, information, or feedback during one phase of a government project, never hear back from the project team, and then learn after something is published that what they shared has been misrepresented, miscommunicated, or even shared when it should not have been. As one might expect, this is a very easy and fast way to erode trust, burn bridges, and ruin relationships that you have invested precious time, energy, and resources to build.\nUse this time to:\n\nConnect with those you have reached out to in previous phases of the project and share your publication, sharing, and communication plans for the project. Also see the notes on iteration in the Take a user-centered design approach section for more guidance.\nIntegrate the feedback you receive into the product(s) before publishing and sharing broadly. Remember, this can take some time to implement well - plan ahead so you aren’t rushed. Also see the notes on iteration in the Take a user-centered design approach section for more guidance.\nDiscuss whether a coordinated communication strategy across community and partner institutions can/should be developed and implemented. If appropriate and communities and partners on board, this could result in all parties involved (and their media / communication teams) distributing findings and products across their respective networks in a coordinated and consistent way, thus furthering the reach and potential impact of the project.\n\n\n\n\nIn addition to all of the reasons outlined in the above Why Publish & Share Data Products?section, the Urban Institute’s Principles for Advancing Equitable Data Practice notes that sharing data can reduce the burden of duplicate data collection on communities. Specifically:\n\nSome people and communities are consistently the targets of data collection and study, sometimes from organizations seeking the same information for similar purposes. Siloed data place an additional—and potentially unnecessary—burden on community members. Sharing nonconfidential data, when it is unlikely that it could lead to harm or add risks, may reduce the burden that individuals and communities experience from data collection\n\nAs you publish your project’s data and products, be sure to:\n\nPublish data and complete metadata in publicly accessible locations, such as Water Boards databases and/or the California Open Data Portal (or California State Geoportal), as appropriate. Also see the Publish Your Water Boards Data page for more guidance\nPublish or embed accessible products onto Water Boards webpages, as appropriate, including: planning or summary documents/reports, fact sheets, applications, presentation slides or recordings, etc.\nPublish any code and related software packages or products in the Water Boards GitHub, as appropriate, including: data and analysis workflow diagrams or related documentation, all application and source code files, and all data, analysis, interpretation, and visualization processing scripts. Also see the Open Source Code Handbook for more guidance.\nPublish any peer-reviewed literature in open access journals, as appropriate. Public data and records are public property, as appropriate. Therefore, any products developed by public agencies using those public resources should also be public property, as appropriate. The best way to ensure peer-reviewed literature remains accessible to the public is by publishing in open access journals or equivalent open and free to access publication avenues.\nEnsure all documentation associated with the project is written in plain, accessible, and inclusive language, as appropriate, including: metadata, ReadMe files, code comments, and anything else pertaining to the project components listed.\n\n\n\n\n\n\n\nRemember - Open does not always mean public\n\n\n\nThe Water Board’s Open Data Resolution urges us to make our data and products open and accessible to the public, as appropriate. Equally important is ensuring that the confidential components of our work are secured and protected appropriately. See the Data Preparation and Preservation & Storage pages for more detailed guidance.\n\n\n\n\n\nIn addition to all of the considerations outlined in the above Make data and products open and accessible to the public section, it’s also important to take time to make project, data, products, information and knowledge accessible to and useful for the communities and partners that contributed to or could be impacted by the project. The Urban Institute’s Principles for Advancing Equitable Data Practice notes that:\n\nAnalysts have the power to disrupt the dynamic of people’s having no ownership of and deriving no utility from what their data have produced. Ensuring that the results are communicated in a way that community members can use and understand is an important step toward equity.\n\nAs you implement other components of this phase, be sure to:\n\nConnect with those you have reached out to in previous phases of the project and walk through the data and products you plan to publish and share. Ideally, this will be wrapped into the conversations you have with these folks BEFORE publication to streamline conversations and reduce the burden of providing feedback.\nDiscuss whether the existing results and products are communicated in ways that resonate with and are useful to the community. If they are generous and able to tell you how to improve your communication, be sure to integrate that into existing products! See the Data Visualization for guidance on how to effectively communicate with data.\nDiscuss whether additional products or resources should be developed to improve communication and usefulness of the data and products for the community. It will be important to be clear about your capacity and limitations going into these conversations so you can manage expectations and only commit to actions and timelines you can realistically accomplish. Depending on what products you decide to develop, it might be helpful to co-create them with your community partners so that messaging and functionality is as useful and impactful as possible. Some products that might be considered for development include:\n\nA project fact sheet or brochure that describes the project, available resources, and what it means for the community\nA webpage update (or development of a new page) that makes it easy for communities to find the information and resources they need about the project and products\nA step by step guide, workshop, training, or video that walks people through how to use what has been developed and interpret the results or findings appropriately\nCustomized code or program that communities can use to pull data from databases and process it for their own data analysis and visualization needs\nA media and/or press kit, that would include resources for communities and partners to use to share information about the project and products, including:\n\nlanguage for press releases or social media posts, including:\n\nproject background, goals\nkey results, facts, or statistics\ncase studies\navailable resources, products, or services\ncontact details (name/email addresses, social media handles)\nrelevant hashtags\n\nfigures or images that are ready to be shared broadly, including the image titles, captions, and alt text that should accompany the use of each image\ninstructions on how to embed products into community-managed web-pages\nbranding guidelines\n\n\n\n\n\n\nTip\n\n\n\nAs you develop your media kit, be sure to work with your Water Boards Communications team to ensure all content is consistent with other Water Boards communications.\n\n\n\nAnything else communities and partners say they need to effectively use the data and products, you’ve invested time, energy, and resources to develop!",
    "crumbs": [
      "Publish & Share"
    ]
  },
  {
    "objectID": "collect-process/process.html",
    "href": "collect-process/process.html",
    "title": "Data Processing",
    "section": "",
    "text": "“Raw data, like raw potatoes, usually require cleaning before use.”\n— Ronald A. Thisted, Professor Emeritus - Departments of Statistics and Public Health Sciences, The University of Chicago\n\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy, Russian author known as one of the world’s greatest novelists.\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham, Chief Scientist at Posit (formerly RStudio)\n\nGood data organization and formatting is the foundation of any data project. Once the data are collected, you will need to process them so they can be used in your analyses or product development steps.\n\n\n\n\n\n\nData processing takes time - plan accordingly!\n\n\n\nIt’s commonly understood in the data science field that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003) - expect the same will be true for your project and prepare to invest time and resources accordingly.\n\n\nThe Resources section below links to resources that provide detailed technical guidance on how to process data using data science methods. Here, we discuss the why and how we go about this step with an equity lens.\nWhen we talk about data processing here, we’re referring to the steps required to organize, format, and clean the data so it’s more efficient to use in future steps, and so others can consistently reproduce your steps - - this is commonly referred to as tidying data.\n\n\nFrom a data science lens - Investing the time to properly tidy your data upfront makes the analyses or product development steps much more efficient and reproducible. This will cut down the time it takes in the long term to complete your analysis and product development steps (even if they need to evolve/change over time!), enabling you to spend more time on the actual data, science, management, and equity questions you have for your project (aka the good stuff!).\nFrom an equity lens - having consistent and reproducible data makes it easy for others to replicate your work, which can increase transparency of your process and ultimately help build trust with your partners, the communities impacted by the management decisions associated with your project, and the public at large.\nAdditionally, working from a foundation of tidy data (and using consistent tools) can make it easier for others to collaborate with and contribute to your data project, which generally results in a better product in the end.\n\n\n\nKeep your “raw data raw”: It’s important to keep (and back up) a version of your data that is untouched by your process (i.e. raw). We also recommend backing up a version of your data that is tidied and cleaned (i.e. what you will use for analysis). Not only does doing so protect you if your working version of the data are lost for some reason (e.g., computer crash, freak system failure) - but it also helps with reproducibility and transparency of your process.\nRemove duplicate or irrelevant data: When you’re processing your data and making it tidy, think through which data from your sources are applicable to your question(s). Sometimes we don’t have the technical or logistical capacity to keep ALL of the data we are able to collect - so if you are going to remove data at this step, be sure to document WHY you’re removing it in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making. Some reasons might include:\n\nData are outside of the geographic scope of the analysis (e.g. we’re focused on a particular region for the project, and we are removing data outside of that region so we can increase loading/analysis speeds)\nFields/columns from source data are not relevant to our analysis for XYZ reasons.\n\nTransform your data into a tidy structure: Here you will reformat or restructure your dataset so it can be used in your future analyses or product development steps in a way that is efficient, effective, and meaningful for your project’s objectives. A common data transformation at this phase is converting a dataset from wide to long formats.\n\n\n\nGraphic illustrating tidy data as a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, each observation has its own row, and each cell is a single measurement. Artwork by Allison Horst from the Openscapes Blog: Tidy data for efficiency, reproducibility, and collaboration by Julie Lowndes and Allison Horst.\n\n\nExplore your data - but don’t analyze it: Once duplicate and irrelevant data are removed from the dataset, you may want to explore your data using simple visualizations or statistics (see the Data Exploration Checklist for ideas). This will help you find extreme outliers in your data, or otherwise incorrect or missing data that should be removed, separated, corrected, substituted or imputed before you begin your data analysis phase. Similar to when you removed duplicate or irrelevant data, you will want to document WHY you’re removing/correcting/imputing these data in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making.\nAny aggregation or preliminary analysis of your data should be completed during the analysis phase of the project. Keeping the values and aggregation of the data as they are at this phase increases reproducibility and transparency now and during future steps. Note that this is different from transforming data as described above (e.g. restructuring from wide to long format), which is a common and very useful part of the data processing phase.\nYou might be tempted to begin your data quality assessments as you’re looking at the data during this phase, but you’ll actually want to hold off on that for now. You’ll complete your data quality and assurance steps after all of your data have been tidied and you can look at all of your data holistically.\n\n\n\n\nJulia Lowndes and Allison Horst (2020) Tidy Data for reproducibility, efficiency, and collaboration. Openscapes blog.\nCollege of Water Informatics Data Management Handbook - Collect and Process Section\nHadley Wickham. Tidy Data. Journal of Statistical Software\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science (2e) - Data Tidying Chapter\nThe Py4DS Community. Python for Data Science - Tidy Data Chapter\nKarl W. Broman, and Kara H Woo (2018) Data Organization in Spreadsheets. The American Statistician 72 (1). Available open access as a PeerJ preprint.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#why-tidy-data",
    "href": "collect-process/process.html#why-tidy-data",
    "title": "Data Processing",
    "section": "",
    "text": "From a data science lens - Investing the time to properly tidy your data upfront makes the analyses or product development steps much more efficient and reproducible. This will cut down the time it takes in the long term to complete your analysis and product development steps (even if they need to evolve/change over time!), enabling you to spend more time on the actual data, science, management, and equity questions you have for your project (aka the good stuff!).\nFrom an equity lens - having consistent and reproducible data makes it easy for others to replicate your work, which can increase transparency of your process and ultimately help build trust with your partners, the communities impacted by the management decisions associated with your project, and the public at large.\nAdditionally, working from a foundation of tidy data (and using consistent tools) can make it easier for others to collaborate with and contribute to your data project, which generally results in a better product in the end.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#processing-data-with-an-equity-lens",
    "href": "collect-process/process.html#processing-data-with-an-equity-lens",
    "title": "Data Processing",
    "section": "",
    "text": "Keep your “raw data raw”: It’s important to keep (and back up) a version of your data that is untouched by your process (i.e. raw). We also recommend backing up a version of your data that is tidied and cleaned (i.e. what you will use for analysis). Not only does doing so protect you if your working version of the data are lost for some reason (e.g., computer crash, freak system failure) - but it also helps with reproducibility and transparency of your process.\nRemove duplicate or irrelevant data: When you’re processing your data and making it tidy, think through which data from your sources are applicable to your question(s). Sometimes we don’t have the technical or logistical capacity to keep ALL of the data we are able to collect - so if you are going to remove data at this step, be sure to document WHY you’re removing it in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making. Some reasons might include:\n\nData are outside of the geographic scope of the analysis (e.g. we’re focused on a particular region for the project, and we are removing data outside of that region so we can increase loading/analysis speeds)\nFields/columns from source data are not relevant to our analysis for XYZ reasons.\n\nTransform your data into a tidy structure: Here you will reformat or restructure your dataset so it can be used in your future analyses or product development steps in a way that is efficient, effective, and meaningful for your project’s objectives. A common data transformation at this phase is converting a dataset from wide to long formats.\n\n\n\nGraphic illustrating tidy data as a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, each observation has its own row, and each cell is a single measurement. Artwork by Allison Horst from the Openscapes Blog: Tidy data for efficiency, reproducibility, and collaboration by Julie Lowndes and Allison Horst.\n\n\nExplore your data - but don’t analyze it: Once duplicate and irrelevant data are removed from the dataset, you may want to explore your data using simple visualizations or statistics (see the Data Exploration Checklist for ideas). This will help you find extreme outliers in your data, or otherwise incorrect or missing data that should be removed, separated, corrected, substituted or imputed before you begin your data analysis phase. Similar to when you removed duplicate or irrelevant data, you will want to document WHY you’re removing/correcting/imputing these data in a way that makes it easy for others to easily understand the reasoning behind the data decisions you’re making.\nAny aggregation or preliminary analysis of your data should be completed during the analysis phase of the project. Keeping the values and aggregation of the data as they are at this phase increases reproducibility and transparency now and during future steps. Note that this is different from transforming data as described above (e.g. restructuring from wide to long format), which is a common and very useful part of the data processing phase.\nYou might be tempted to begin your data quality assessments as you’re looking at the data during this phase, but you’ll actually want to hold off on that for now. You’ll complete your data quality and assurance steps after all of your data have been tidied and you can look at all of your data holistically.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/process.html#additional-resources",
    "href": "collect-process/process.html#additional-resources",
    "title": "Data Processing",
    "section": "",
    "text": "Julia Lowndes and Allison Horst (2020) Tidy Data for reproducibility, efficiency, and collaboration. Openscapes blog.\nCollege of Water Informatics Data Management Handbook - Collect and Process Section\nHadley Wickham. Tidy Data. Journal of Statistical Software\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science (2e) - Data Tidying Chapter\nThe Py4DS Community. Python for Data Science - Tidy Data Chapter\nKarl W. Broman, and Kara H Woo (2018) Data Organization in Spreadsheets. The American Statistician 72 (1). Available open access as a PeerJ preprint.",
    "crumbs": [
      "Collect & Process",
      "Data Processing"
    ]
  },
  {
    "objectID": "collect-process/collection.html",
    "href": "collect-process/collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "Important\n\n\n\nYou should have addressed this during the Plan & Prepare phase of your process…but just in case you haven’t (yet) or you need a refresher - we’re restating here:\nAchieving racial equity outcomes means that race can no longer be used to predict life outcomes and outcomes for all groups are improved (Glossary)\nSo, as you begin to collect the data for your project, be sure it includes:\n\nData that can represent your management question(s) or project objectives.\nData that can tell us something about the extent to which we are achieving equity outcomes. This may be limited to simple demographics data - but it could also be something more! Working with Tribal and community experts to decide what type(s) of data are most applicable to and reflective of their lived experiences as they relate to your management questions and project objectives is a great place to start!\n\n\n\nThe data related to your project may come from direct human observation, laboratory and field instruments, experiments, simulations, surveys, and/or compilations of data from other sources. Below we focus on data that can be downloaded from open data sources, and survey guidance.\n\n\nAs you begin to collect the data required for your project, it’s important to remember that ALL data have limits in what they can actually tell us, constraints on how they should be used appropriately, and biases related to initial data collection or generation - and it’s crucial to be aware of and account for them during your project.\nIn addition to data limitations related to data quality or analysis methods (e.g., insufficient quality, low sample size, data gaps; see the the Quality Assurance and Data Analysis pages for more details), be sure to also keep the below considerations in mind so that you can prepare and analyze your data in a way that supports the advancement of equity, inclusion, and justice.\n\n\n\n\n\n\nRemember - it’s OK to have data gaps!\n\n\n\nIt’s OK to have some data gaps, and in the case of conducting data analyses with a racial equity lens, gaps will be the norm. What’s important is to acknowledge what gaps exist, document how you will account for them, and (ideally) set a course for filling those gaps, as appropriate.\n\n\n\n\nWe all have unconscious biases and operate in inequitable and unjust systems. That can unconsciously and unintentionally impact how data are collected and result in datasets that reflect those biases. Take time to ask the following questions about the data you’re interested in using for your project so you can have a better understanding of the data’s context and be better able to detect and account for potential biases of said data:\n\nWho collected the data?\nWhy were these specific data collected? What were the data collection goals?\nHow was data collected?\nWhat was prioritized during data collection?\nWhat assumptions were made during data collection?\n\n\n\n\nIt’s not uncommon for project teams to have gaps in data, even after all of your diligent work and investment into the planning and data preparation steps of the project. This limitation of the data may be out of your control - especially when you are using data from external sources. Quality data regarding marginalized communities is often lacking. For example, recent research has shown that US lakes are monitored disproportionately less in communities of color - similar trends may be likely in other data sources.\nWhen this happens, it’s important to acknowledge, document, and accessibly communicate those gaps and who is not adequately represented in your data product. In some cases, It may be appropriate to still present or analyze these data and also present caveats for the data limitations. In other cases, it may be more appropriate to rely only on qualitative discussion based on information derived from background research and feedback from affected communities.\n\n\n\n\nAs you collect your data, start thinking through how the raw data you are collecting is already grouped and how you intend on grouping the data. You may not be able to decide on how you will ultimately aggregate or disaggregate the data until you are further along in the analysis or visualization stages - but it’s important to begin thinking about this issue during the data collection phase for the reasons outlined below.\nHow we aggregate or disaggregate the data can impact which groups are “seen” and represented (or not) in our data products. This can also influence who is centered, valued, or prioritized in the narrative of the visualization, and who is excluded.\nCarefully consider how groups are lumped or split - by aggregating many groups in the visualization beyond what might be statistically necessary (and not acknowledging who is being grouped together and why), we can unintentionally misrepresent said groups, minimize inequities and perpetuate invisible and erased experiences of those communities. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled (Centering Racial Equity Throughout Data Integration).\nThe UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we’re thinking about how we will aggregate the data (or not):\n\nIs important data/nuance lost by combining categories? Ensure there is not a meaningful difference in our ability to understand equity outcomes between groups that would be lost if combined.\nDoes the inclusion of uncombined data negatively impact the interpretation of the data visualization? Having too many groups can make visualizations cluttered and hard to interpret. Additionally, disaggregation leads to smaller group sizes, which can make comparisons to larger groups more difficult and quantifying statistical significance more challenging. For those reasons, it can sometimes be best to combine groups.\nDoes sharing uncombined data compromise confidential information (e.g., Personal Identifiable Information) or information considered private by the community from which it comes (e.g., locations of sacred practices)? This will depend on the audience you are sharing the visualization with (e.g. internal vs public) and what information it contains.\n\nIf you ultimately decide to aggregate / combine groups, be sure to:\n\nAvoid creating a dichotomy of races. Don’t use “White” vs. “non-White” or “people of color.” Rather, disaggregate the “non-White” group to show the diversity among communities.\nBe transparent about why you’re making those decisions (including the trade offs you considered) and documenting those decisions accordingly.\nAcknowledge who is now not included in the data or visualization and explain what groups have been combined and why. Use comments, tooltips, or footnotes that can be easily accessed within the visualization to make it easier for users to find this information.\nThink carefully about how groups are lumped in the “other” category of our analysis or visualization. Sometimes it’s necessary to combine groups into a single “other” category (e.g. to generalize small groups to protect confidentiality or to achieve adequate sample size for your analysis). The Urban Institute’s Do No Harm Data Visualization Recommendations include considering alternatives to using the term “other” as a catch-all category, including:\n\nAnother ______ (e.g. Another race or Another group)\nAdditional ______ (e.g. Additional races or Additional languages)\nAll other self-descriptions\nPeople identifying as other or multiple races\nIdentity not listed\nIdentity not listed in the survey or dataset\n\n\n\n\n\nIn most cases, the Water Boards programs were not developed or designed to collect the types of data needed to conduct analyses with an equity lens as a matter of process which means that most will rely on external data sources. Below we have provided a list of common data sources that can tell us something about the extent to which we are achieving equity outcomes.\n\n\nAdding demographics data to your data project can help increase understanding of potential correlations or relationships between your data and demographic and socioeconomic characteristics of locations of interest. More specifically, the integration of demographics data into your data project can highlight the extent to which race can still be used to predict outcomes associated with your project and therefore underscore where and to what extent racial equity outcomes have not yet been achieved.\n\n\nIt’s important to remember to provide context about your project before trying to communicate demographics related results or answers to specific racial equity questions posed by our Board, the public, or our partners in this work. Contextual topics could include:\n\nWhat is your program/project about? What is the mission?\nWhat is your program/project meant to do? What are your objectives/goals?\nHow well does your program/project do these things now (aka Performance Report)?\nWhat approaches are you taking with this project to advance equity outcomes?\nWhat equity and data related questions do you have for your program/project? Which data types and datasets would be the best to use to answer that questions?\n\nCommunicating this contextual information may require different complementary modes of communication (e.g. presentation, fact sheets, visualizations), but investing the time to provide that grounding and framing will help the audience understand how you are approaching your data work with an equity lens within the scope of your program/project. \n\n\n\n\n\n\nALL of the questions above (and more!) should have already been answered to some degree during the Plan & Prepare Phase of your project.\n\n\n\nBe sure to reference, pull from, and build on information you have already synthesized in your Equity Assessment and Data Management Plan documents!\n\n\n\n\n\nThe Safe and Affordable Funding for Equity and Resilience (SAFER) Program developed the Final FY 2024-25 Fund Expenditure Plan for the Safe and Affordable Drinking Water Fund in response to the Senate Bill (SB) 200 (Ch. 120, Stats. 2019) which requires the annual adoption of a Fund Expenditure Plan for the Safe and Affordable Drinking Water Fund.\nThe SAFER Program’s goal is to provide safe and affordable drinking water in every California community, for every Californian. FY 2024-25 marks the halfway point of the initial ten years of continuously appropriated funding to the Safe and Affordable Drinking Water Fund as originally envisioned in SB 200.\nLooking at the SAFER Program goal with an equity lens, we can add that the goal would be for race to no longer predict a person’s ability to have access to safe and affordable drinking water.\nThe Racial Equity and Environmental Justice Section of the Plan (Section VIII.G.) provides several tables with data that incorporates demographics data. As we see below, presenting demographics data alone does not tell the full story of the issue. The tables and the text lack clarity and in depth analysis on why the data is telling the story it is.\n\n\n\n\n\n\nTo tell a compelling story and truly advance equity - adding demographics data to an analysis is not enough!\n\n\n\nAs you go through the process of collecting demographics data for your project - be sure to understand:\n\nWHY demographics data are needed for your project\nHow you plan on using demographics data, in concert with the other data sources you identified in your Data Management Plan, to tell the (often complex and nuanced) story behind the data.\nHow you plan on taking action to advance equity based on whatever is shown in the analysis. You might not be able to have a full plan in place at the data collection step - but now is the time to begin brainstorming so that you can take action as swiftly as possible once the analysis or data product is complete.\n\n\n\nFor this example, race and ethnicity of the populations served by the water systems likely isn’t the only difference between the systems. What other factors are associated with different populations that could be driving the imbalance in failing systems and funding? Perhaps the majority-Hispanic population systems are much larger, or older, or have more severe problems.\nA more nuanced comparison would look at the major factors that determine cost (or other metrics driving the imbalance in failing systems and funding) and compare the racial difference between those subgroups. For example, comparing the racial and ethnic differences between systems of medium sized cities with a water treatment plants built within the past 30 years.\nSome recommendations to improving the storytelling and meaningful impact of the data being shown in the report to support the advancement of equity could include:\n\nmaking proportions of different populations and their association to failing systems (or other metrics) more explicit\nadding a section articulating how the most impacted and burdend population systems (Hispanics in this case) will get more funding, proportionally\nlist some additional explanatory factors that could be explored in future analyses\n\n\n\n\nThe data sources many use to make inferences related to demographic and socioeconomic characteristics are from the United States Census Bureau and the associated American Community Survey (ACS) Data.\nWhile we are fortunate to have just updated this dataset in 2020 there are limitations and potential inaccuracies associated with relying solely on census data to enumerate demographic characteristics within a given census tract. This tool from the Department of Finance exists to measure this limitation.\nThe ACS also has a Handbooks for Data Users to make it easier for folks to use the data appropriately.\n\n\n\nIn addition to the terms defined in the Handbook Glossary, users should know the following terms when using demographics data:\n\nDemographics: statical data relating to characteristics of human populations, such as: age, race, ethnicity, sex, gender, income, education, etc.\nGeographic (Geospatial) Vectors: how data are stored in geographic information systems:\n\nPoints: zero-dimensional objects that contain only a single coordinate pair (i.e. latitude, longitude) to define their location. Example: Census landmarks\nLines: one-dimensional features composed of multiple, explicitly connected points. Examples: roads, streams\nPolygons: two-dimensional features created by multiple lines that loop back to create a “closed” feature. Examples: region, city, county, census tract, block boundaries, lakes\n\nGeographies: the geographic unit(s) into which demographic data are aggregated.\n\nBlocks (Census Blocks or Tabulation Blocks) are statistical areas bounded by visible features, such as streets, roads, streams, and railroad tracks, and by nonvisible boundaries, such as selected property lines and city, township, school district, and county limits and short line-of-sight extensions of streets and roads. Blocks are numbered uniquely with a four-digit census block number from 0000 to 9999 within census tract, which nest within state and county. \nBlock Groups are statistical divisions of census tracts, are generally defined to contain between 600 and 3,000 people, and are used to present data and control block numbering. A block group consists of clusters of blocks within the same census tract that have the same first digit of their four-digit census block number.\n\nTribal Block Groups are only applicable to legal federally recognized American Indian reservation and off-reservation trust land areas, and are defined independently of the standard county-based block group delineation. Tribal block groups use the letter range A through K (except “I,” which could be confused with a number “1”) to identify and code the tribal block group. Tribal block groups nest within tribal census tracts.\n\nCensus Tracts are small, relatively permanent statistical subdivisions of a county or statistically equivalent entity, that generally have a population size between 1,200 and 8,000 people, with an optimum size of 4,000 people. State and county boundaries always are census tract boundaries in the standard census geographic hierarchy.\n\nTribal Census Tracts are only applicable to legal federally recognized American Indian reservation and off-reservation trust land areas, are defined independently of the standard county-based census tract delineation, and can cross state and county boundaries. Tribal census tract codes are six characters long with a leading “T” alphabetic character followed by five-digit numeric codes having an implied decimal between the fourth and fifth character.\n\n\n\n\nCensus Geography Unit Hierarchies\n\n\n\n\nAlso see this U.S. Census Glossary for additional terms specific to U.S. Census programs, data, and products.\n\n\n\nDepending on what demographics data sources and software you decide to use, the methods needed to combine, overlay, or compare with the data you are interested in may vary. See the Demographcs Use Case page for step by step guidance on how to download and compare demographics data to point, line, and polygon data types.\n\n\n\n\n\n\nImportant Reminders Before You Dive In\n\n\n\nData are NOT people - We need to use these data to get a better understanding of what’s going on in our communities, but the data (at best) only represent a sample of the communitie’s population and in no way reflect everyone or their lived experiences.\nThere’s no such thing as “equity data” - how we use data, interpret it, and act on what we learn makes our use equitable (or not). Simply including demographics data in your project’s analysis or data products does not make those resources equitable - to operationalize equity we need to take actions and make decisions in ways to advance equitable outcomes.\nThe data you’re using has limitations, be sure you know what they are before moving forward - as discussed above, all data have limitations, and that is particularly true for demographics data. Be sure you have a clear and comprehensive understanding of the limitations that apply to the specific datasets you’re using so you can collect and eventually process and analyze those data in ways that are appropriate.\n\n\n\n\n\n\nCalEnviroScreen is a mapping tool that helps identify California communities that are most affected by many sources of pollution, and where people are often especially vulnerable to pollution’s effects. CalEnviroScreen can be a helpful tool in creating visualizations and performing analysis as it provides a number of indices, as well as a “rolled-up” score that combines environmental and demographic data together. As with any dataset or visualization tool there can be things to consider, a couple of which are discussed below.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores. These missing values are distinct from zeros, which are also in the CES dataset.\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores encodes these missing values as negative numbers (-999 for most variables, and -1998 for one variable). The Excel workbook containing CES 4.0 scores encodes these missing values as NA. Also, note that the CalEnviroScreen 3 shapefile (June 2018 update version) encoded missing values as 0, so users should be aware of this change if/when updating an analysis from CES 3 to CES 4.0 data.\nUsers should account for these missing values – and their different encodings – as needed when doing any analysis using CES data.\nFor more information about the missing (and zero) values in the CES 4.0 dataset, see the data dictionary (PDF file) that accompanies the CalEnviroScreen 4.0 results Excel workbook, both of which are available for download as a zip file.\n\n\n\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores uses a simplified version of the polygons that represent 2010 census tracts. The boundaries of the census tracts defined by these simplified polygons do not always align with the boundaries of neighboring census tracts, resulting in slight gaps or overlaps between some neighboring census tracts. These inconsistencies are not likely to have a significant impact on most uses of the CES data, but they could impact some types of analysis based on CES data. For example, when assessing sites or facilities based on the CES score of the census tract they are located in, sites located near a census tract boundary could be associated with more than one census tract (and more than one CES score) in areas where there are overlapping census tract polygons, or not associated with any census tract (and no CES score) in areas where there are gaps between census tract polygons.\nThis issue may be addressed in a future release of the CES dataset; in the meantime, a possible workaround is to use the official 2010 census tract boundaries from the US Census Bureau for any calculations, then use census tract IDs to tie this information to the associated CES score for each tract.\n\n\n\n\nAnalysis of Race/Ethnicity and CalEnviroScreen 4.0 Scores Storymap | Report\nSB 535 Disadvantaged Communities\nCalEnviroScreen page of the California Water Boards Racial Equity Resource Hub\n\n\n\n\n\nEJScreen is EPA’s environmental justice mapping and screening tool that provides EPA with a nationally consistent dataset and approach for combining environmental and socioeconomic indicators.\nFirst-time users may find the 5-minute EJScreen in 5: A Quick Overview of EJScreen video helpful as an introduction to the tool.\n\n\n\nEJScreen User Guide for navigating the various features of the tool,\nEJScreen Glossary for better understanding the map layers and indicators being displayed in the tool, and\nFrequent Questions about EJScreen\n\n\n\n\n\nMost organizations, including the Water Boards, have various types of Administrative Data which includes internal demographics data related to the workforce in the organization. This data is normally confidential but can be very valuable when working on addressing workforce equity. If and when these data are used, it is critical to ensure the protection and security of the data to preserve confidentiality through the development and sharing (or not) of the final data product. See the Data Collection and Processing section of the Planning Phase for more guidance.\n\n\n\n\nThere may be instances where the data you need are not already available and you need to collect it yourself through the use of survey(s).\n\n\nCreating surveys that yield actionable insights is all about the details - and writing effective survey questions is the first step. You do not have to be an expert to build and distribute an effective online survey, but by checking your survey against tried-and-tested benchmarks, you can help ensure you are collecting the best data possible.  \nTips for Building an Effective Survey:\n\nMake Sure That Every Question Is Necessary\nKeep it Short and Simple\nAsk Direct Questions\nAsk One Question at a Time\nAvoid Leading and Biased Questions\nSpeak Your Respondent’s Language\nUse Response Scales Whenever Possible\nAvoid Using Grids or Matrices for Responses\nRephrase Yes/No Questions if Possible\nTake Your Survey for a Test Drive\n\nGuides for good survey design include:\n\nCreating Effective Surveys - Best Practices\nHow to Create an Effective Survey\n\n\n\n\nAs you develop your survey, it’s important to design and implement the survey in a way that minimizes or eliminates the biases as much as possible. Below are three tables that provide an overview of common survey biases to be aware of and avoid1:\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: problematic wording, data for the intended purpose of the question is missing or inadequate, the scale included the question is faulty, questions that are leading, intrusive, or inconsistent.\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nAbrupt question\nThe question is too short and can come off as jarring to the respondent.\nAdd a transition or introduction to the beginning of the question.\n\n\nAmbiguous question\nThe question leads respondents to understand the question differently than was intended and, therefore, to answer a different question than was intended\nHave a trusted partner, but one who had not participated in survey development, review and provide feedback on questions.\n\n\nComplex question\nThe question is usually long, vague, too formal, or overly complex.\nKeep questions short and to the point.\n\n\nData degradation\nThe question is phrased in a way that encourages respondents to provide less accurate information, which, can not be recovered or made more accurate after the survey is complete.\nPhrase questions in a manner that collects the most accurate information (ideally in the form of continuous data)\nExample:\nWhat age category do you belong to? ➔ What is your age? OR What year were you born?\nNote that sometimes it is more beneficial to collect generalized information (e.g. age category) so that you can prevent the unnecessary collection of Personal Identifiable Information (PII).\n\n\nDouble-barrelled question\nThe question is made up of two or more questions, and therefore make it difficult for the respondent to know which part of the question to answer and for the investigator to know which part of the question the respondent actually answered\nSeparate questions so that each question is only asking for one thing.\n\n\nForced choice (aka insufficient category)\nThe question provides too few categories that results in the respondents being forced to choose an imprecise option.\nAdd one or more of the below category options to your question:\n\nUnsure\nDon’t know\nOther\nNot Applicable (N/A)\n\n\n\nHypothetical question\nThe question asks the respondent about their beliefs (hypothetical), which can yield more generalized results than are helpful.\nKeep questions specific to the respondent’s behaviors.\nExample:\nDo you think…? ➔ Have you ever…?\n\n\nIncomplete interval\nThe question does not include all possible intervals so respondents cannot select a category that most accurately reflects their experience.\nAdd more intervals, or broaden interval categories, when appropriate.\nExample - insufficient intervals\n\nOnce per month\nOnce per week\nMore than once per week\n\nExample - sufficient intervals\n\nLess than once per month\nOnce per month to once per week\nMore than once per week\n\n\n\nInsensitive scales\nThe question does not contain a range or scale that would result in sufficient discriminating power to differentiate the respondents because of the limited categories.\nUse a scale has five or more categories.\nExample:\nOn a scale of 1-3…? ➔ On a scale of 1-5… OR On a scale of 1-10…?\n\n\nIntrusive question\nThe question is requesting sensitive information (e.g. PII, income, identity, culture, etc.) too abruptly or directly and can feel intrusive to the respondent.\nThis can result in the respondent electively suppressing information, providing inaccurate information and can influence how the respondent answers subsequent questions.\nConfirm the information you are asking for is truly essential to your project. If it is, frame the question in a way that respects the sensitivity of the information you are requesting.\nTry adding a transition into the question that provides context and explains why the information is needed.\n\n\nLeading question\nThe question is worded in a way that subtly guides respondents toward a certain answer.\nUse wording that is generalized and unbiased towards the respective answer choices.\nExample:\nDon’t you agree that…? ➔ Do you agree or disagree that…? OR What are your thoughts about…?\n\n\nOverlapping interval\nThe question contains intervals that overlap, which can result in respondent confusion related to which interval should be selected\nUse intervals that do not overlap\nExample - Overlapping intervals\n\nNone\n5 or less\n5 - 10\n10 or more\n\nExample - sufficient intervals\n\nNone\n1 - 4\n5 - 9\n10 or more\n\n\n\nScale format\nAn even or an odd number of categories in the scale for the respondents to choose from may produce different results.\nQuestions with an odd number of categories in the scale can result in neutral answers, whereas those with even categories forces respondents to pick a side on the scale.\nThere is no consensus on which approach is better overall (i.e. even vs. odd).\nWhat’s important is that you select the scale that is most appropriate for the question and data you need.\n\n\nTechnical jargon\nThe question includes technical terms that are specific to a profession that may not be understood by those outside of that field.\nReplace jargon with more plain, accessible, and inclusive language.\n\n\nTime period\nThe question does not identify a common time period for the respondents experience\nInclude a specific time period in the question.\nExample:\nIn the last 12 months…? ➔ Between Jan 1 to Dec 31 of last year…?\n\n\nUncommon word\nThe question uses uncommon or difficult words.\nReplace uncommon words with those that are more commonly used by your survey audience.\nExample Uncommon/Common word pairs\n\n\nVague word\nThe question includes words that undefined or may have multiple definitions.\nReplace vague words with those that are more precise.\nExamples:\n\nOccasionally ➔ one per month\nRegularly ➔ once per week\nBi-weekly ➔ twice per week OR every other week\nBi-monthly ➔ twice per month OR every other month\n\n\n\nWord choice\nThe question may use words or phrases that pull focus or frame aspects of the question in a way that increases the likelihood of respondents choosing an answer that does not accurately reflect their intended choice.\nUse consistent question structure and terminology.\nExample - poor word choice:\nWhich operation would you prefer?\n\nAn operation that has a 5% mortality.\nAn operation in which 90% of the patients will survive.\n\nPatients scheduled for surgery may choose the second option when they see or hear the words “90%” and “survive,” but in fact a 90% survival rate (or 10% mortality) is worse than a 5% mortality\nExample - improved word choice:\nWhich operation would you prefer?\n\nAn operation in which 95% of the patients will survive.\nAn operation in which 90% of the patients will survive.\n\n\n\n\n\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: inconsistencies with past surveys which makes it difficult to compare responses over time, problematic formatting or design that can cause confusion or fatigue among respondents and result in inaccurate answers.\n\n\n\n\n\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nHorizontal formatting\nFor questions associated with multiple choice responses, displaying multiple choices horizontally can cause confusion.\nThis is especially important for surveys completed on paper.\nList multiple choices vertically.\nExample - horizontal formatting\nExcellent … [ ] Good … [ ] Fair … [ ] Poor … [ ]\nExample - vertical formatting\nExcellent …….. [ ]\nGood …………. [ ]\nFair …………… [ ]\nPoor ………….. [ ]\n\n\nInconsistency among surveys\nComponents of a survey are changed over time and over the course of multiple offerings of the survey.\nWhen components of a survey change, it can influence how people respond and the results of the different surveys may not be comparable.\nSurvey components include:\n\nFormatting\nWord choice\nScales / multiple choice options\nDefinitions used\n\nIf there is interest in comparing responses to surveys that have been administered at multiple points in time, keep the sections of the survey that you want to compare identical.\nIf there are instances where new questions arise and are essential to include in future surveys, add the new questions to the end of the existing survey so the respondent’s experience for the initial questions are as comparable as possible. If new questions are added - be sure the survey length does not exceed the times recommended in the “response fatigue” row below.\n\n\nJuxtaposed scale (aka Likert scale)\nThis is often referred to as a Likert Scale question, which displays a list of single-answer questions and a rating scale for the answers, so a respondent can select a value from the scale to answer each question.\nLikert Scale questions tend to ask about:\n\nAgreement\nFrequency\nSatisfaction\nImportance\nLikelihood\nQuality\nInterest\nUsefulness\nEase of use\n\nThe advantage of using Juxtaposed (aka Likert) scale formatting is that it can force respondents to think about and compare their responses for each item because they are side by side.\nHowever, this format has been shown to cause confusion among respondents who are less educated. If you suspect that may apply to your intended survey audience, you may prefer to separate the questions so respondents only review and answer one question at a time.\n\n\nNo-saying / yes-saying (aka nay-saying / yea-saying)\nFor groups of questions that only include statements associated with yes/no response options, respondents tend to answer yes to all questions or no to all questions.\nUse both positive and negative statements about the same issue sprinkled through the group of questions to break up the pattern and encourage respondents to consider one question at a time, rather than to group them together.\n\n\nNon-response\nEven with adequate sample representation (see below), individuals are choosing not to respond to your survey.\nConsider when and how you are reaching out to your audience. Maybe changing the timing or method through which you are offering the survey will improve the response rate.\nConsider reducing the length of time it take to complete the survey. Remember that the average attention span for adults in the U.S. is approximately 8 seconds - taking even a five minute survey may feel like too much for your audience.\nConsider offering incentives for completing the survey.\n\n\nOpen-ended questions\nOpen-ended questions allow respondents to provide short or long text responses to a question. There is no way to standardize the quality and vocaublary used in the responses which can make analysis more challenging.\nMoreover, respondents are less likely to take time to answer the questions fully.\nOnly use open-ended questions when necessary.\nOpen-ended questions are more appropriate than close-ended questions, particularly in surveys of knowledge and attitudes, and can yield a wealth of information.\nIf you decide to use an open-ended question, be sure to decide on how you will analyze the responses using appropriate qualitative methods.\n\n\nSample representation (aka sampling bias, selection bias)\nThe sample of individuals selected to complete the survey is not representative of the population, which can lead it inaccurate results and conclusions.\nEnsure you are delivering the survey randomly to the audience(s) that represent the population, and that you have enough responses to be representative of the population.\nConsider the best way(s) to reach your target audience - and use methods that would be the most effective for and accessible to them.\nConsider extending the deadline to complete the survey and sending reminders to those who have not yet responded.\n\n\nSkipping questions\nQuestions that instruct respondents to skip to another question based on their response can lead to the loss of important information.\nBe sure questions that will be skipped cannot be applied to different respondents based on their first response. Work with partners to test the survey before it is administered to work out any such issues.\nExample of a skipped question:\n(1) Are you self-employed?\n\nYes\nNo (Go to question 8)\n\nIn this case, individuals who are not self-employed would not be able to complete questions 2 - 7. If the information requested in one or more of those skipped questions is pertinent to all respondents, try re-ordering or grouping questions so respondents are able to provide all essential information regardless of their choices.\n\n\nResponse choice alignment\nIf interviewers are completing the survey for the respondents (e.g., during in-person or telephone interviews): placing the check-box to the left of (before) the possible options can result in errors.\nIf respondents are completing the survey themselves (e.g., mailed, online surveys): placing the check-box to the right of (after) the possible options can make it more difficult for respondents to complete the survey.\nSelect the response choice alignment according to how you will be delivering the survey to reduce confusion and errors.\nIf interviewers are completing the survey for the respondents, use a right-aligned format:\nExcellent …….. [ ]\nGood …………. [ ]\nFair …………… [ ]\nPoor ………….. [ ]\nIf respondents are completing the survey themselves, use a left-aligned format:\n[ ] excellent\n[ ] good\n[ ] fair\n[ ] poor\n\n\nResponse fatigue\nThe survey is too long, which induces fatigue among respondents and can result in rushed, uniform and/or inaccurate answers.\nFor example, towards the end of a lengthy survey, respondents tend to say all yes or all no or refuse to answer all remaining questions.\nReview the original purpose of your project and survey and only include essential questions.\nThe length of time to complete surveys should not exceed the following times:\n\nSelf-administered survey (e.g. online, by mail): 10 to 20 minutes\nTelephone survey: 30 to 60 minutes\nIn-person survey: 50 to 90 minutes\n\n\n\n\n\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: the interviewer is not objective, respondent’s conscious or subconscious reactions, learning, inaccurate recall, or perception of questions based on their lived experiences and culture.\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nAcquiescence (aka yes bias, friendliness bias, confirmation bias)\nRespondents tend to agree with survey questions, regardless of their actual beliefs, to avoid being disagreeable in the eyes of the interviewer or to expedite completing the survey.\nReframe questions so they use more neutral language and avoid asking for agreement on a topic.\nAvoid close-ended questions that do not leave room for nuance; allow for multiple choice and scale questions and provide space for additional open-ended responses.\nTrain interviewers to deliver surveys consistently and objectively - so respondents don’t feel pressured to agree with the interviewer.\nEnable respondents to complete the survey anonymously and without an interviewer present.\n\n\nCultural differences\nThe culture and lived experiences of the respondents can affect their perception of questions and therefore their answers.\nHave a trusted partner, but one who had not participated in survey development, review and provide feedback on questions.\n\n\nEnd aversion (aka central tendency)\nRespondents usually avoid ends of scales in their answers and tend to provide responses that are somewhere closer to the middle of the response options.\nExample: Respondents are more likely to check “Agree” or “Disagree” than “Strongly agree” or “Strongly disagree”\nNone - but be aware of this potential bias as you analyze your results.\n\n\nExtreme response bias\nRespondents tend to submit responses that are at the ends of scales and provide responses that are at the extremes of the possible response options.\nExample: Respondents are less likely to check “Agree” or “Disagree” than “Strongly agree” or “Strongly disagree”\nNone - but be aware of this potential bias as you analyze your results.\n\n\nFaking bad (aka hello-goodbye effect)\nRespondents try to appear worse off than they actually are to qualify for support or resources that could be granted in according to their responses.\nIf receipt of resources is tied to how people complete the survey, consider when and how to communicate how respondent data will be used to make such decisions.\n\n\nFaking good (aka social desirability, conformity bias, obsequiousness)\nRespondents may alter their responses in the direction they perceive to be desired by the investigator or society at large. Socially undesirable answers tend to be under-reported.\nUse wording that is generalized and neutral.\nAsk questions that might be associated with individual or social desirability toward the end of the questionnaire so that they will not affect other questions.\nLet respondents complete the survey anonymously. Make questions about name or contact information optional. Instead of requiring in-person or telephone interviews, let respondents submit anonymous, mailed in surveys.\nWhen asking about socially undesirable behaviors, it is better to ask whether the person had ever engaged in the behavior in the past before asking about current practices, because past events are less threatening.\n\n\nHypothesis guessing\nRespondents may systematically alter their responses when, during the process of answering the survey, they think they know the study hypothesis.\nNone - but be aware of this potential bias as you analyze your results.\n\n\nInterviewer data gathering methods\nThe interviewer can pose questions, or gather data in a way that is informed and led by their own biases, information they think they know about the respondent, etc. This can result in errors that impact survey results.\nTrain interviewers to deliver surveys consistently and objectively.\n\n\nNonblinding\nWhen an interviewer is not blind to the study hypotheses, they may consciously gather selective data\nTrain interviewers to deliver surveys consistently and objectively and ensure those delivering the survey are blind to the study hypotheses.\n\n\nPositive satisfaction (aka positive skew)\nRespondents tend to give positive answers when answering questions on satisfaction.\nNone - but be aware of this potential bias as you analyze your results.\n\n\nPrimacy and recency\nResearch has indicated that in mailed surveys, respondents may tend to choose the first few response options on the list (primacy bias), though in telephone or personal interview surveys, they are more likely to respond in favor of the later categories (recency bias).\nReduce the number of categories presented to respondents and randomize the order of categories in the survey.\nRandomize the answer option order.\n\n\nProxy respondent (aka surrogate data)\nSoliciting information from proxies (e.g., spouse, family members) may result in inaccurate information.\nKeep questions related to the the respondent’s experience. Do not ask someone to answer attitudinal, knowledge, or behavior questions for others.\n\n\nRecall / Telescope Bias\nThis type of bias is because of differences in accuracy or completeness of respondents recall prior to major events or experiences (general recall), and because respondents may recall an event or experience in the distant past as happening more recently (telescope).\nNone - but be aware of this potential bias as you analyze your results.\n\n\nRespondent’s learning\nHaving thought about prior questions can affect the respondent’s answer to subsequent questions through the learning process as the questionnaire is completed.\nRandomize the order of the questions for different respondents.\n\n\nUnacceptability\nQuestions or measurements that can hurt, embarrass, invade privacy, or require excessive commitment may be systematically refused or evaded.\nWhenever possible, do not ask such questions.\nIf asking such a question is absolutely essential and unavoidable, do so with sensitivity and consider using incentives to increase participation rate.\n\n\n\n\n\n\n\nMost Water Board staff will use Microsoft Forms which is available to all staff through the Microsoft 365 suite of applications. Microsoft Forms has a lot of advantages because of its integration with other Microsoft tools like Excel and PowerBI which allow for the survey results to be analyzed and visualized.  See this 6 min video on Using Microsoft Forms data with Power BI for guidance on how to make the connection between Forms and PowerBi via Sharepoint that allows for consistent updating of results\nNote that those who use Microsoft Forms and other free software like Google Forms will likely need to transform the form output from a wide format to a long format for analysis. See the Processing Data with an Equity Lens section of the Data Processing page for more guidance.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#data-limitations",
    "href": "collect-process/collection.html#data-limitations",
    "title": "Data Collection",
    "section": "",
    "text": "As you begin to collect the data required for your project, it’s important to remember that ALL data have limits in what they can actually tell us, constraints on how they should be used appropriately, and biases related to initial data collection or generation - and it’s crucial to be aware of and account for them during your project.\nIn addition to data limitations related to data quality or analysis methods (e.g., insufficient quality, low sample size, data gaps; see the the Quality Assurance and Data Analysis pages for more details), be sure to also keep the below considerations in mind so that you can prepare and analyze your data in a way that supports the advancement of equity, inclusion, and justice.\n\n\n\n\n\n\nRemember - it’s OK to have data gaps!\n\n\n\nIt’s OK to have some data gaps, and in the case of conducting data analyses with a racial equity lens, gaps will be the norm. What’s important is to acknowledge what gaps exist, document how you will account for them, and (ideally) set a course for filling those gaps, as appropriate.\n\n\n\n\nWe all have unconscious biases and operate in inequitable and unjust systems. That can unconsciously and unintentionally impact how data are collected and result in datasets that reflect those biases. Take time to ask the following questions about the data you’re interested in using for your project so you can have a better understanding of the data’s context and be better able to detect and account for potential biases of said data:\n\nWho collected the data?\nWhy were these specific data collected? What were the data collection goals?\nHow was data collected?\nWhat was prioritized during data collection?\nWhat assumptions were made during data collection?\n\n\n\n\nIt’s not uncommon for project teams to have gaps in data, even after all of your diligent work and investment into the planning and data preparation steps of the project. This limitation of the data may be out of your control - especially when you are using data from external sources. Quality data regarding marginalized communities is often lacking. For example, recent research has shown that US lakes are monitored disproportionately less in communities of color - similar trends may be likely in other data sources.\nWhen this happens, it’s important to acknowledge, document, and accessibly communicate those gaps and who is not adequately represented in your data product. In some cases, It may be appropriate to still present or analyze these data and also present caveats for the data limitations. In other cases, it may be more appropriate to rely only on qualitative discussion based on information derived from background research and feedback from affected communities.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#consider-how-you-will-group-the-data",
    "href": "collect-process/collection.html#consider-how-you-will-group-the-data",
    "title": "Data Collection",
    "section": "",
    "text": "As you collect your data, start thinking through how the raw data you are collecting is already grouped and how you intend on grouping the data. You may not be able to decide on how you will ultimately aggregate or disaggregate the data until you are further along in the analysis or visualization stages - but it’s important to begin thinking about this issue during the data collection phase for the reasons outlined below.\nHow we aggregate or disaggregate the data can impact which groups are “seen” and represented (or not) in our data products. This can also influence who is centered, valued, or prioritized in the narrative of the visualization, and who is excluded.\nCarefully consider how groups are lumped or split - by aggregating many groups in the visualization beyond what might be statistically necessary (and not acknowledging who is being grouped together and why), we can unintentionally misrepresent said groups, minimize inequities and perpetuate invisible and erased experiences of those communities. On the other hand, when analysts create a subgroup, they may be shifting the focus of analysis to a specific population that is likely already over-surveilled (Centering Racial Equity Throughout Data Integration).\nThe UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we’re thinking about how we will aggregate the data (or not):\n\nIs important data/nuance lost by combining categories? Ensure there is not a meaningful difference in our ability to understand equity outcomes between groups that would be lost if combined.\nDoes the inclusion of uncombined data negatively impact the interpretation of the data visualization? Having too many groups can make visualizations cluttered and hard to interpret. Additionally, disaggregation leads to smaller group sizes, which can make comparisons to larger groups more difficult and quantifying statistical significance more challenging. For those reasons, it can sometimes be best to combine groups.\nDoes sharing uncombined data compromise confidential information (e.g., Personal Identifiable Information) or information considered private by the community from which it comes (e.g., locations of sacred practices)? This will depend on the audience you are sharing the visualization with (e.g. internal vs public) and what information it contains.\n\nIf you ultimately decide to aggregate / combine groups, be sure to:\n\nAvoid creating a dichotomy of races. Don’t use “White” vs. “non-White” or “people of color.” Rather, disaggregate the “non-White” group to show the diversity among communities.\nBe transparent about why you’re making those decisions (including the trade offs you considered) and documenting those decisions accordingly.\nAcknowledge who is now not included in the data or visualization and explain what groups have been combined and why. Use comments, tooltips, or footnotes that can be easily accessed within the visualization to make it easier for users to find this information.\nThink carefully about how groups are lumped in the “other” category of our analysis or visualization. Sometimes it’s necessary to combine groups into a single “other” category (e.g. to generalize small groups to protect confidentiality or to achieve adequate sample size for your analysis). The Urban Institute’s Do No Harm Data Visualization Recommendations include considering alternatives to using the term “other” as a catch-all category, including:\n\nAnother ______ (e.g. Another race or Another group)\nAdditional ______ (e.g. Additional races or Additional languages)\nAll other self-descriptions\nPeople identifying as other or multiple races\nIdentity not listed\nIdentity not listed in the survey or dataset",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#common-data-sources",
    "href": "collect-process/collection.html#common-data-sources",
    "title": "Data Collection",
    "section": "",
    "text": "In most cases, the Water Boards programs were not developed or designed to collect the types of data needed to conduct analyses with an equity lens as a matter of process which means that most will rely on external data sources. Below we have provided a list of common data sources that can tell us something about the extent to which we are achieving equity outcomes.\n\n\nAdding demographics data to your data project can help increase understanding of potential correlations or relationships between your data and demographic and socioeconomic characteristics of locations of interest. More specifically, the integration of demographics data into your data project can highlight the extent to which race can still be used to predict outcomes associated with your project and therefore underscore where and to what extent racial equity outcomes have not yet been achieved.\n\n\nIt’s important to remember to provide context about your project before trying to communicate demographics related results or answers to specific racial equity questions posed by our Board, the public, or our partners in this work. Contextual topics could include:\n\nWhat is your program/project about? What is the mission?\nWhat is your program/project meant to do? What are your objectives/goals?\nHow well does your program/project do these things now (aka Performance Report)?\nWhat approaches are you taking with this project to advance equity outcomes?\nWhat equity and data related questions do you have for your program/project? Which data types and datasets would be the best to use to answer that questions?\n\nCommunicating this contextual information may require different complementary modes of communication (e.g. presentation, fact sheets, visualizations), but investing the time to provide that grounding and framing will help the audience understand how you are approaching your data work with an equity lens within the scope of your program/project. \n\n\n\n\n\n\nALL of the questions above (and more!) should have already been answered to some degree during the Plan & Prepare Phase of your project.\n\n\n\nBe sure to reference, pull from, and build on information you have already synthesized in your Equity Assessment and Data Management Plan documents!\n\n\n\n\n\nThe Safe and Affordable Funding for Equity and Resilience (SAFER) Program developed the Final FY 2024-25 Fund Expenditure Plan for the Safe and Affordable Drinking Water Fund in response to the Senate Bill (SB) 200 (Ch. 120, Stats. 2019) which requires the annual adoption of a Fund Expenditure Plan for the Safe and Affordable Drinking Water Fund.\nThe SAFER Program’s goal is to provide safe and affordable drinking water in every California community, for every Californian. FY 2024-25 marks the halfway point of the initial ten years of continuously appropriated funding to the Safe and Affordable Drinking Water Fund as originally envisioned in SB 200.\nLooking at the SAFER Program goal with an equity lens, we can add that the goal would be for race to no longer predict a person’s ability to have access to safe and affordable drinking water.\nThe Racial Equity and Environmental Justice Section of the Plan (Section VIII.G.) provides several tables with data that incorporates demographics data. As we see below, presenting demographics data alone does not tell the full story of the issue. The tables and the text lack clarity and in depth analysis on why the data is telling the story it is.\n\n\n\n\n\n\nTo tell a compelling story and truly advance equity - adding demographics data to an analysis is not enough!\n\n\n\nAs you go through the process of collecting demographics data for your project - be sure to understand:\n\nWHY demographics data are needed for your project\nHow you plan on using demographics data, in concert with the other data sources you identified in your Data Management Plan, to tell the (often complex and nuanced) story behind the data.\nHow you plan on taking action to advance equity based on whatever is shown in the analysis. You might not be able to have a full plan in place at the data collection step - but now is the time to begin brainstorming so that you can take action as swiftly as possible once the analysis or data product is complete.\n\n\n\nFor this example, race and ethnicity of the populations served by the water systems likely isn’t the only difference between the systems. What other factors are associated with different populations that could be driving the imbalance in failing systems and funding? Perhaps the majority-Hispanic population systems are much larger, or older, or have more severe problems.\nA more nuanced comparison would look at the major factors that determine cost (or other metrics driving the imbalance in failing systems and funding) and compare the racial difference between those subgroups. For example, comparing the racial and ethnic differences between systems of medium sized cities with a water treatment plants built within the past 30 years.\nSome recommendations to improving the storytelling and meaningful impact of the data being shown in the report to support the advancement of equity could include:\n\nmaking proportions of different populations and their association to failing systems (or other metrics) more explicit\nadding a section articulating how the most impacted and burdend population systems (Hispanics in this case) will get more funding, proportionally\nlist some additional explanatory factors that could be explored in future analyses\n\n\n\n\nThe data sources many use to make inferences related to demographic and socioeconomic characteristics are from the United States Census Bureau and the associated American Community Survey (ACS) Data.\nWhile we are fortunate to have just updated this dataset in 2020 there are limitations and potential inaccuracies associated with relying solely on census data to enumerate demographic characteristics within a given census tract. This tool from the Department of Finance exists to measure this limitation.\nThe ACS also has a Handbooks for Data Users to make it easier for folks to use the data appropriately.\n\n\n\nIn addition to the terms defined in the Handbook Glossary, users should know the following terms when using demographics data:\n\nDemographics: statical data relating to characteristics of human populations, such as: age, race, ethnicity, sex, gender, income, education, etc.\nGeographic (Geospatial) Vectors: how data are stored in geographic information systems:\n\nPoints: zero-dimensional objects that contain only a single coordinate pair (i.e. latitude, longitude) to define their location. Example: Census landmarks\nLines: one-dimensional features composed of multiple, explicitly connected points. Examples: roads, streams\nPolygons: two-dimensional features created by multiple lines that loop back to create a “closed” feature. Examples: region, city, county, census tract, block boundaries, lakes\n\nGeographies: the geographic unit(s) into which demographic data are aggregated.\n\nBlocks (Census Blocks or Tabulation Blocks) are statistical areas bounded by visible features, such as streets, roads, streams, and railroad tracks, and by nonvisible boundaries, such as selected property lines and city, township, school district, and county limits and short line-of-sight extensions of streets and roads. Blocks are numbered uniquely with a four-digit census block number from 0000 to 9999 within census tract, which nest within state and county. \nBlock Groups are statistical divisions of census tracts, are generally defined to contain between 600 and 3,000 people, and are used to present data and control block numbering. A block group consists of clusters of blocks within the same census tract that have the same first digit of their four-digit census block number.\n\nTribal Block Groups are only applicable to legal federally recognized American Indian reservation and off-reservation trust land areas, and are defined independently of the standard county-based block group delineation. Tribal block groups use the letter range A through K (except “I,” which could be confused with a number “1”) to identify and code the tribal block group. Tribal block groups nest within tribal census tracts.\n\nCensus Tracts are small, relatively permanent statistical subdivisions of a county or statistically equivalent entity, that generally have a population size between 1,200 and 8,000 people, with an optimum size of 4,000 people. State and county boundaries always are census tract boundaries in the standard census geographic hierarchy.\n\nTribal Census Tracts are only applicable to legal federally recognized American Indian reservation and off-reservation trust land areas, are defined independently of the standard county-based census tract delineation, and can cross state and county boundaries. Tribal census tract codes are six characters long with a leading “T” alphabetic character followed by five-digit numeric codes having an implied decimal between the fourth and fifth character.\n\n\n\n\nCensus Geography Unit Hierarchies\n\n\n\n\nAlso see this U.S. Census Glossary for additional terms specific to U.S. Census programs, data, and products.\n\n\n\nDepending on what demographics data sources and software you decide to use, the methods needed to combine, overlay, or compare with the data you are interested in may vary. See the Demographcs Use Case page for step by step guidance on how to download and compare demographics data to point, line, and polygon data types.\n\n\n\n\n\n\nImportant Reminders Before You Dive In\n\n\n\nData are NOT people - We need to use these data to get a better understanding of what’s going on in our communities, but the data (at best) only represent a sample of the communitie’s population and in no way reflect everyone or their lived experiences.\nThere’s no such thing as “equity data” - how we use data, interpret it, and act on what we learn makes our use equitable (or not). Simply including demographics data in your project’s analysis or data products does not make those resources equitable - to operationalize equity we need to take actions and make decisions in ways to advance equitable outcomes.\nThe data you’re using has limitations, be sure you know what they are before moving forward - as discussed above, all data have limitations, and that is particularly true for demographics data. Be sure you have a clear and comprehensive understanding of the limitations that apply to the specific datasets you’re using so you can collect and eventually process and analyze those data in ways that are appropriate.\n\n\n\n\n\n\nCalEnviroScreen is a mapping tool that helps identify California communities that are most affected by many sources of pollution, and where people are often especially vulnerable to pollution’s effects. CalEnviroScreen can be a helpful tool in creating visualizations and performing analysis as it provides a number of indices, as well as a “rolled-up” score that combines environmental and demographic data together. As with any dataset or visualization tool there can be things to consider, a couple of which are discussed below.\n\n\nUsers conducting an analysis with the CalEnviroScreen (CES) 4.0 dataset should be aware that it contains missing values, both for individual indicators and overall CES scores. These missing values are distinct from zeros, which are also in the CES dataset.\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores encodes these missing values as negative numbers (-999 for most variables, and -1998 for one variable). The Excel workbook containing CES 4.0 scores encodes these missing values as NA. Also, note that the CalEnviroScreen 3 shapefile (June 2018 update version) encoded missing values as 0, so users should be aware of this change if/when updating an analysis from CES 3 to CES 4.0 data.\nUsers should account for these missing values – and their different encodings – as needed when doing any analysis using CES data.\nFor more information about the missing (and zero) values in the CES 4.0 dataset, see the data dictionary (PDF file) that accompanies the CalEnviroScreen 4.0 results Excel workbook, both of which are available for download as a zip file.\n\n\n\nIn the CES 4.0 data (for the version available as of April 2023), the shapefile containing CES 4.0 scores uses a simplified version of the polygons that represent 2010 census tracts. The boundaries of the census tracts defined by these simplified polygons do not always align with the boundaries of neighboring census tracts, resulting in slight gaps or overlaps between some neighboring census tracts. These inconsistencies are not likely to have a significant impact on most uses of the CES data, but they could impact some types of analysis based on CES data. For example, when assessing sites or facilities based on the CES score of the census tract they are located in, sites located near a census tract boundary could be associated with more than one census tract (and more than one CES score) in areas where there are overlapping census tract polygons, or not associated with any census tract (and no CES score) in areas where there are gaps between census tract polygons.\nThis issue may be addressed in a future release of the CES dataset; in the meantime, a possible workaround is to use the official 2010 census tract boundaries from the US Census Bureau for any calculations, then use census tract IDs to tie this information to the associated CES score for each tract.\n\n\n\n\nAnalysis of Race/Ethnicity and CalEnviroScreen 4.0 Scores Storymap | Report\nSB 535 Disadvantaged Communities\nCalEnviroScreen page of the California Water Boards Racial Equity Resource Hub\n\n\n\n\n\nEJScreen is EPA’s environmental justice mapping and screening tool that provides EPA with a nationally consistent dataset and approach for combining environmental and socioeconomic indicators.\nFirst-time users may find the 5-minute EJScreen in 5: A Quick Overview of EJScreen video helpful as an introduction to the tool.\n\n\n\nEJScreen User Guide for navigating the various features of the tool,\nEJScreen Glossary for better understanding the map layers and indicators being displayed in the tool, and\nFrequent Questions about EJScreen\n\n\n\n\n\nMost organizations, including the Water Boards, have various types of Administrative Data which includes internal demographics data related to the workforce in the organization. This data is normally confidential but can be very valuable when working on addressing workforce equity. If and when these data are used, it is critical to ensure the protection and security of the data to preserve confidentiality through the development and sharing (or not) of the final data product. See the Data Collection and Processing section of the Planning Phase for more guidance.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#surveys",
    "href": "collect-process/collection.html#surveys",
    "title": "Data Collection",
    "section": "",
    "text": "There may be instances where the data you need are not already available and you need to collect it yourself through the use of survey(s).\n\n\nCreating surveys that yield actionable insights is all about the details - and writing effective survey questions is the first step. You do not have to be an expert to build and distribute an effective online survey, but by checking your survey against tried-and-tested benchmarks, you can help ensure you are collecting the best data possible.  \nTips for Building an Effective Survey:\n\nMake Sure That Every Question Is Necessary\nKeep it Short and Simple\nAsk Direct Questions\nAsk One Question at a Time\nAvoid Leading and Biased Questions\nSpeak Your Respondent’s Language\nUse Response Scales Whenever Possible\nAvoid Using Grids or Matrices for Responses\nRephrase Yes/No Questions if Possible\nTake Your Survey for a Test Drive\n\nGuides for good survey design include:\n\nCreating Effective Surveys - Best Practices\nHow to Create an Effective Survey\n\n\n\n\nAs you develop your survey, it’s important to design and implement the survey in a way that minimizes or eliminates the biases as much as possible. Below are three tables that provide an overview of common survey biases to be aware of and avoid1:\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: problematic wording, data for the intended purpose of the question is missing or inadequate, the scale included the question is faulty, questions that are leading, intrusive, or inconsistent.\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nAbrupt question\nThe question is too short and can come off as jarring to the respondent.\nAdd a transition or introduction to the beginning of the question.\n\n\nAmbiguous question\nThe question leads respondents to understand the question differently than was intended and, therefore, to answer a different question than was intended\nHave a trusted partner, but one who had not participated in survey development, review and provide feedback on questions.\n\n\nComplex question\nThe question is usually long, vague, too formal, or overly complex.\nKeep questions short and to the point.\n\n\nData degradation\nThe question is phrased in a way that encourages respondents to provide less accurate information, which, can not be recovered or made more accurate after the survey is complete.\nPhrase questions in a manner that collects the most accurate information (ideally in the form of continuous data)\nExample:\nWhat age category do you belong to? ➔ What is your age? OR What year were you born?\nNote that sometimes it is more beneficial to collect generalized information (e.g. age category) so that you can prevent the unnecessary collection of Personal Identifiable Information (PII).\n\n\nDouble-barrelled question\nThe question is made up of two or more questions, and therefore make it difficult for the respondent to know which part of the question to answer and for the investigator to know which part of the question the respondent actually answered\nSeparate questions so that each question is only asking for one thing.\n\n\nForced choice (aka insufficient category)\nThe question provides too few categories that results in the respondents being forced to choose an imprecise option.\nAdd one or more of the below category options to your question:\n\nUnsure\nDon’t know\nOther\nNot Applicable (N/A)\n\n\n\nHypothetical question\nThe question asks the respondent about their beliefs (hypothetical), which can yield more generalized results than are helpful.\nKeep questions specific to the respondent’s behaviors.\nExample:\nDo you think…? ➔ Have you ever…?\n\n\nIncomplete interval\nThe question does not include all possible intervals so respondents cannot select a category that most accurately reflects their experience.\nAdd more intervals, or broaden interval categories, when appropriate.\nExample - insufficient intervals\n\nOnce per month\nOnce per week\nMore than once per week\n\nExample - sufficient intervals\n\nLess than once per month\nOnce per month to once per week\nMore than once per week\n\n\n\nInsensitive scales\nThe question does not contain a range or scale that would result in sufficient discriminating power to differentiate the respondents because of the limited categories.\nUse a scale has five or more categories.\nExample:\nOn a scale of 1-3…? ➔ On a scale of 1-5… OR On a scale of 1-10…?\n\n\nIntrusive question\nThe question is requesting sensitive information (e.g. PII, income, identity, culture, etc.) too abruptly or directly and can feel intrusive to the respondent.\nThis can result in the respondent electively suppressing information, providing inaccurate information and can influence how the respondent answers subsequent questions.\nConfirm the information you are asking for is truly essential to your project. If it is, frame the question in a way that respects the sensitivity of the information you are requesting.\nTry adding a transition into the question that provides context and explains why the information is needed.\n\n\nLeading question\nThe question is worded in a way that subtly guides respondents toward a certain answer.\nUse wording that is generalized and unbiased towards the respective answer choices.\nExample:\nDon’t you agree that…? ➔ Do you agree or disagree that…? OR What are your thoughts about…?\n\n\nOverlapping interval\nThe question contains intervals that overlap, which can result in respondent confusion related to which interval should be selected\nUse intervals that do not overlap\nExample - Overlapping intervals\n\nNone\n5 or less\n5 - 10\n10 or more\n\nExample - sufficient intervals\n\nNone\n1 - 4\n5 - 9\n10 or more\n\n\n\nScale format\nAn even or an odd number of categories in the scale for the respondents to choose from may produce different results.\nQuestions with an odd number of categories in the scale can result in neutral answers, whereas those with even categories forces respondents to pick a side on the scale.\nThere is no consensus on which approach is better overall (i.e. even vs. odd).\nWhat’s important is that you select the scale that is most appropriate for the question and data you need.\n\n\nTechnical jargon\nThe question includes technical terms that are specific to a profession that may not be understood by those outside of that field.\nReplace jargon with more plain, accessible, and inclusive language.\n\n\nTime period\nThe question does not identify a common time period for the respondents experience\nInclude a specific time period in the question.\nExample:\nIn the last 12 months…? ➔ Between Jan 1 to Dec 31 of last year…?\n\n\nUncommon word\nThe question uses uncommon or difficult words.\nReplace uncommon words with those that are more commonly used by your survey audience.\nExample Uncommon/Common word pairs\n\n\nVague word\nThe question includes words that undefined or may have multiple definitions.\nReplace vague words with those that are more precise.\nExamples:\n\nOccasionally ➔ one per month\nRegularly ➔ once per week\nBi-weekly ➔ twice per week OR every other week\nBi-monthly ➔ twice per month OR every other month\n\n\n\nWord choice\nThe question may use words or phrases that pull focus or frame aspects of the question in a way that increases the likelihood of respondents choosing an answer that does not accurately reflect their intended choice.\nUse consistent question structure and terminology.\nExample - poor word choice:\nWhich operation would you prefer?\n\nAn operation that has a 5% mortality.\nAn operation in which 90% of the patients will survive.\n\nPatients scheduled for surgery may choose the second option when they see or hear the words “90%” and “survive,” but in fact a 90% survival rate (or 10% mortality) is worse than a 5% mortality\nExample - improved word choice:\nWhich operation would you prefer?\n\nAn operation in which 95% of the patients will survive.\nAn operation in which 90% of the patients will survive.\n\n\n\n\n\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: inconsistencies with past surveys which makes it difficult to compare responses over time, problematic formatting or design that can cause confusion or fatigue among respondents and result in inaccurate answers.\n\n\n\n\n\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nHorizontal formatting\nFor questions associated with multiple choice responses, displaying multiple choices horizontally can cause confusion.\nThis is especially important for surveys completed on paper.\nList multiple choices vertically.\nExample - horizontal formatting\nExcellent … [ ] Good … [ ] Fair … [ ] Poor … [ ]\nExample - vertical formatting\nExcellent …….. [ ]\nGood …………. [ ]\nFair …………… [ ]\nPoor ………….. [ ]\n\n\nInconsistency among surveys\nComponents of a survey are changed over time and over the course of multiple offerings of the survey.\nWhen components of a survey change, it can influence how people respond and the results of the different surveys may not be comparable.\nSurvey components include:\n\nFormatting\nWord choice\nScales / multiple choice options\nDefinitions used\n\nIf there is interest in comparing responses to surveys that have been administered at multiple points in time, keep the sections of the survey that you want to compare identical.\nIf there are instances where new questions arise and are essential to include in future surveys, add the new questions to the end of the existing survey so the respondent’s experience for the initial questions are as comparable as possible. If new questions are added - be sure the survey length does not exceed the times recommended in the “response fatigue” row below.\n\n\nJuxtaposed scale (aka Likert scale)\nThis is often referred to as a Likert Scale question, which displays a list of single-answer questions and a rating scale for the answers, so a respondent can select a value from the scale to answer each question.\nLikert Scale questions tend to ask about:\n\nAgreement\nFrequency\nSatisfaction\nImportance\nLikelihood\nQuality\nInterest\nUsefulness\nEase of use\n\nThe advantage of using Juxtaposed (aka Likert) scale formatting is that it can force respondents to think about and compare their responses for each item because they are side by side.\nHowever, this format has been shown to cause confusion among respondents who are less educated. If you suspect that may apply to your intended survey audience, you may prefer to separate the questions so respondents only review and answer one question at a time.\n\n\nNo-saying / yes-saying (aka nay-saying / yea-saying)\nFor groups of questions that only include statements associated with yes/no response options, respondents tend to answer yes to all questions or no to all questions.\nUse both positive and negative statements about the same issue sprinkled through the group of questions to break up the pattern and encourage respondents to consider one question at a time, rather than to group them together.\n\n\nNon-response\nEven with adequate sample representation (see below), individuals are choosing not to respond to your survey.\nConsider when and how you are reaching out to your audience. Maybe changing the timing or method through which you are offering the survey will improve the response rate.\nConsider reducing the length of time it take to complete the survey. Remember that the average attention span for adults in the U.S. is approximately 8 seconds - taking even a five minute survey may feel like too much for your audience.\nConsider offering incentives for completing the survey.\n\n\nOpen-ended questions\nOpen-ended questions allow respondents to provide short or long text responses to a question. There is no way to standardize the quality and vocaublary used in the responses which can make analysis more challenging.\nMoreover, respondents are less likely to take time to answer the questions fully.\nOnly use open-ended questions when necessary.\nOpen-ended questions are more appropriate than close-ended questions, particularly in surveys of knowledge and attitudes, and can yield a wealth of information.\nIf you decide to use an open-ended question, be sure to decide on how you will analyze the responses using appropriate qualitative methods.\n\n\nSample representation (aka sampling bias, selection bias)\nThe sample of individuals selected to complete the survey is not representative of the population, which can lead it inaccurate results and conclusions.\nEnsure you are delivering the survey randomly to the audience(s) that represent the population, and that you have enough responses to be representative of the population.\nConsider the best way(s) to reach your target audience - and use methods that would be the most effective for and accessible to them.\nConsider extending the deadline to complete the survey and sending reminders to those who have not yet responded.\n\n\nSkipping questions\nQuestions that instruct respondents to skip to another question based on their response can lead to the loss of important information.\nBe sure questions that will be skipped cannot be applied to different respondents based on their first response. Work with partners to test the survey before it is administered to work out any such issues.\nExample of a skipped question:\n(1) Are you self-employed?\n\nYes\nNo (Go to question 8)\n\nIn this case, individuals who are not self-employed would not be able to complete questions 2 - 7. If the information requested in one or more of those skipped questions is pertinent to all respondents, try re-ordering or grouping questions so respondents are able to provide all essential information regardless of their choices.\n\n\nResponse choice alignment\nIf interviewers are completing the survey for the respondents (e.g., during in-person or telephone interviews): placing the check-box to the left of (before) the possible options can result in errors.\nIf respondents are completing the survey themselves (e.g., mailed, online surveys): placing the check-box to the right of (after) the possible options can make it more difficult for respondents to complete the survey.\nSelect the response choice alignment according to how you will be delivering the survey to reduce confusion and errors.\nIf interviewers are completing the survey for the respondents, use a right-aligned format:\nExcellent …….. [ ]\nGood …………. [ ]\nFair …………… [ ]\nPoor ………….. [ ]\nIf respondents are completing the survey themselves, use a left-aligned format:\n[ ] excellent\n[ ] good\n[ ] fair\n[ ] poor\n\n\nResponse fatigue\nThe survey is too long, which induces fatigue among respondents and can result in rushed, uniform and/or inaccurate answers.\nFor example, towards the end of a lengthy survey, respondents tend to say all yes or all no or refuse to answer all remaining questions.\nReview the original purpose of your project and survey and only include essential questions.\nThe length of time to complete surveys should not exceed the following times:\n\nSelf-administered survey (e.g. online, by mail): 10 to 20 minutes\nTelephone survey: 30 to 60 minutes\nIn-person survey: 50 to 90 minutes\n\n\n\n\n\n\n\nWhile the specific sources below vary, they can generally be caused by one or more of the following issues: the interviewer is not objective, respondent’s conscious or subconscious reactions, learning, inaccurate recall, or perception of questions based on their lived experiences and culture.\n\n\n\nBias Source\nDescription\nTips to Minimize Bias\n\n\n\n\nAcquiescence (aka yes bias, friendliness bias, confirmation bias)\nRespondents tend to agree with survey questions, regardless of their actual beliefs, to avoid being disagreeable in the eyes of the interviewer or to expedite completing the survey.\nReframe questions so they use more neutral language and avoid asking for agreement on a topic.\nAvoid close-ended questions that do not leave room for nuance; allow for multiple choice and scale questions and provide space for additional open-ended responses.\nTrain interviewers to deliver surveys consistently and objectively - so respondents don’t feel pressured to agree with the interviewer.\nEnable respondents to complete the survey anonymously and without an interviewer present.\n\n\nCultural differences\nThe culture and lived experiences of the respondents can affect their perception of questions and therefore their answers.\nHave a trusted partner, but one who had not participated in survey development, review and provide feedback on questions.\n\n\nEnd aversion (aka central tendency)\nRespondents usually avoid ends of scales in their answers and tend to provide responses that are somewhere closer to the middle of the response options.\nExample: Respondents are more likely to check “Agree” or “Disagree” than “Strongly agree” or “Strongly disagree”\nNone - but be aware of this potential bias as you analyze your results.\n\n\nExtreme response bias\nRespondents tend to submit responses that are at the ends of scales and provide responses that are at the extremes of the possible response options.\nExample: Respondents are less likely to check “Agree” or “Disagree” than “Strongly agree” or “Strongly disagree”\nNone - but be aware of this potential bias as you analyze your results.\n\n\nFaking bad (aka hello-goodbye effect)\nRespondents try to appear worse off than they actually are to qualify for support or resources that could be granted in according to their responses.\nIf receipt of resources is tied to how people complete the survey, consider when and how to communicate how respondent data will be used to make such decisions.\n\n\nFaking good (aka social desirability, conformity bias, obsequiousness)\nRespondents may alter their responses in the direction they perceive to be desired by the investigator or society at large. Socially undesirable answers tend to be under-reported.\nUse wording that is generalized and neutral.\nAsk questions that might be associated with individual or social desirability toward the end of the questionnaire so that they will not affect other questions.\nLet respondents complete the survey anonymously. Make questions about name or contact information optional. Instead of requiring in-person or telephone interviews, let respondents submit anonymous, mailed in surveys.\nWhen asking about socially undesirable behaviors, it is better to ask whether the person had ever engaged in the behavior in the past before asking about current practices, because past events are less threatening.\n\n\nHypothesis guessing\nRespondents may systematically alter their responses when, during the process of answering the survey, they think they know the study hypothesis.\nNone - but be aware of this potential bias as you analyze your results.\n\n\nInterviewer data gathering methods\nThe interviewer can pose questions, or gather data in a way that is informed and led by their own biases, information they think they know about the respondent, etc. This can result in errors that impact survey results.\nTrain interviewers to deliver surveys consistently and objectively.\n\n\nNonblinding\nWhen an interviewer is not blind to the study hypotheses, they may consciously gather selective data\nTrain interviewers to deliver surveys consistently and objectively and ensure those delivering the survey are blind to the study hypotheses.\n\n\nPositive satisfaction (aka positive skew)\nRespondents tend to give positive answers when answering questions on satisfaction.\nNone - but be aware of this potential bias as you analyze your results.\n\n\nPrimacy and recency\nResearch has indicated that in mailed surveys, respondents may tend to choose the first few response options on the list (primacy bias), though in telephone or personal interview surveys, they are more likely to respond in favor of the later categories (recency bias).\nReduce the number of categories presented to respondents and randomize the order of categories in the survey.\nRandomize the answer option order.\n\n\nProxy respondent (aka surrogate data)\nSoliciting information from proxies (e.g., spouse, family members) may result in inaccurate information.\nKeep questions related to the the respondent’s experience. Do not ask someone to answer attitudinal, knowledge, or behavior questions for others.\n\n\nRecall / Telescope Bias\nThis type of bias is because of differences in accuracy or completeness of respondents recall prior to major events or experiences (general recall), and because respondents may recall an event or experience in the distant past as happening more recently (telescope).\nNone - but be aware of this potential bias as you analyze your results.\n\n\nRespondent’s learning\nHaving thought about prior questions can affect the respondent’s answer to subsequent questions through the learning process as the questionnaire is completed.\nRandomize the order of the questions for different respondents.\n\n\nUnacceptability\nQuestions or measurements that can hurt, embarrass, invade privacy, or require excessive commitment may be systematically refused or evaded.\nWhenever possible, do not ask such questions.\nIf asking such a question is absolutely essential and unavoidable, do so with sensitivity and consider using incentives to increase participation rate.\n\n\n\n\n\n\n\nMost Water Board staff will use Microsoft Forms which is available to all staff through the Microsoft 365 suite of applications. Microsoft Forms has a lot of advantages because of its integration with other Microsoft tools like Excel and PowerBI which allow for the survey results to be analyzed and visualized.  See this 6 min video on Using Microsoft Forms data with Power BI for guidance on how to make the connection between Forms and PowerBi via Sharepoint that allows for consistent updating of results\nNote that those who use Microsoft Forms and other free software like Google Forms will likely need to transform the form output from a wide format to a long format for analysis. See the Processing Data with an Equity Lens section of the Data Processing page for more guidance.",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "collect-process/collection.html#footnotes",
    "href": "collect-process/collection.html#footnotes",
    "title": "Data Collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content in this section has been informed by: A Catalog of Biases in Questionnaires↩︎",
    "crumbs": [
      "Collect & Process",
      "Data Collection"
    ]
  },
  {
    "objectID": "store.html",
    "href": "store.html",
    "title": "Preservation & Storage",
    "section": "",
    "text": "By now, you’ve completed some or all of your product development steps (Data Analysis, Data Visualization) and are ready to save copies of product components for short-, medium-, and/or long-term preservation and storage.\nThis phase of the data life cycle involves determining or confirming the retention policy for product components and archiving copies of product components accordingly.\nAlso see a short list of best practices for this phase.\n\n\n\n\n\n\nTip\n\n\n\nReview this content and make your preservation and storage plan during the Plan & Prepare Phase of your project! Depending on your project needs, you may even integrate your responses to questions below into your project’s data management plan. Doing so will make it easier for all team members to prepare their project components accordingly and will save you time when implementing this phase of the project.\n\n\n\n\nAs you wrap up your data project, you will want to think through whether and how project components should be archived and preserved for short-, medium-, and/or long-term viability, utility, and accessibility.\nFrom a data science lens - Investing the time to properly preserve and archive project components ensures they will be available for future project team members (including yourself!) to reference or reuse components in the future. This will reduce the amount of time future teams need to spend recreating or building upon project components, thus increasing future efficiency and decreasing frustration and workload for all.\nFrom an equity lens - Investing the time to properly preserve and archive project components makes it easier for current and future project partners and communities to reference or reuse components of the project now and into the future. In addition to the benefits listed above, preserving and storing project components appropriately can also increase trust between project developers, partners, and the communities impacted by the project.\nAll of this can make it easier for current partners and communities to want to invest their time and energy into participation and engagement in current and future work, which will slowly but surely increase your network of collaborators, contributors, and partners.\n\n\n\nAs you embark on this phase of the data life cycle you will want to apply the 5 Ws and H Framework to your data preservation and storage project. \n\n\n\nWho will be responsible for preparing components of the data project for preservation and storage?\nWho will have access to which project components after preservation and archiving is complete? Be sure to think about internal team members and external partners and communities and note that each group might want or need access to different things. It’s best to work with partners and communities to determine which components they will want/need access to.\n\n\n\n\n\nWhat project components will be preserved and stored? It might not be appropriate or helpful to save everything - think through ALL of the interim and final products you developed during each phase of the data life cycle and determine which items/products should be preserved and stored, and which should be deleted.\n\nIt may be helpful to refer to your Region, Division, Office, or Program’s data or file governance and retention policy (aka Records Retention Schedule) to make these decisions, if such a policy exists. If it doesn’t you might consider creating one for your project.\nThe Water Board DIT Web Document Remediation Guidance (requires VPN) may also be a useful reference for this step.\n\n\n\n\n\n\nWhen will project components be preserved and stored? Depending on the project and needs, it may be appropriate to preserve and store some components of the project sooner than others.\nWhen will project components be deleted? Keeping all project components for all time is unrealistic and likely unnecessary. Once you have completed the “What” step above and have a list of ALL of the interim and final products you developed during each phase of the data life cycle, review each item and determine when each item will be deleted (think short-, medium-, long-term and FOREVER).\nWhen will internal team members and external partners and communities be able to access project components before and after preservation and storage is complete?\nWhen will external partners and communities be notified of the preservation and storage plan and its implementation? Ideally, external partners and communities will help you develop the preservation and storage plan during your project’s Plan & Prepare phase so everyone is on the same page with respect to project preservation and storage. If that was not the case, be sure to consult with partners (at the very least) and communities now to develop, refine, or finalize your preservation and storage plan, which should include when folks will be notified of when this phase begins and/or is complete.\n\n\n\n\n\nWhere will project components be preserved and stored? This will likely include a variety of locations (e.g., databases, open data portals, GitHub repositories, Water Boards or partner web pages, internal file locations, etc.). Be sure to be explicit about where each component will be stored, including webpage URLs or internal file paths, as appropriate.\n\n\n\n\n\nWhy are you making the decisions you’re making regarding project preservation and storage? Just as with every other phase of the data life cycle, it’s critical to document why you’re making decisions and ensure those justifications are thoroughly documented and in plain, accessible, and inclusive language, as appropriate.\n\n\n\n\n\nHow will the project preservation and storage plan be implemented?\nHow will data and products be protected and made secure through the life of the project and during preservation and storage (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\nHow will communities and project partners be able to request their personal information be removed or deleted? If this is applicable to your data project, be sure to establish a mechanism for this in partnership with the communities and project partners and during the planning phase.\nHow will products and files be named? How we name things can make or break our ability to find and resue things in the future. Jenny Bryan has developed guidance on naming things, including:\n\nMake names machine readable\nMake names human readable\nMake sure the name “plays well” with default ordering (e.g. add a date at the beginning of the file name)\n\n\n\nGraphic with a Public Service Announcement regarding how we write dates as numbers. Graphic from Bryan (2015) Naming Things slides\n\n\nFor more guidance and resources on on naming things, visit Jenny Bryan’s How to Name Files GitHub Repositiry",
    "crumbs": [
      "Preserve & Store"
    ]
  },
  {
    "objectID": "store.html#why-preserve-store-data-products",
    "href": "store.html#why-preserve-store-data-products",
    "title": "Preservation & Storage",
    "section": "",
    "text": "As you wrap up your data project, you will want to think through whether and how project components should be archived and preserved for short-, medium-, and/or long-term viability, utility, and accessibility.\nFrom a data science lens - Investing the time to properly preserve and archive project components ensures they will be available for future project team members (including yourself!) to reference or reuse components in the future. This will reduce the amount of time future teams need to spend recreating or building upon project components, thus increasing future efficiency and decreasing frustration and workload for all.\nFrom an equity lens - Investing the time to properly preserve and archive project components makes it easier for current and future project partners and communities to reference or reuse components of the project now and into the future. In addition to the benefits listed above, preserving and storing project components appropriately can also increase trust between project developers, partners, and the communities impacted by the project.\nAll of this can make it easier for current partners and communities to want to invest their time and energy into participation and engagement in current and future work, which will slowly but surely increase your network of collaborators, contributors, and partners.",
    "crumbs": [
      "Preserve & Store"
    ]
  },
  {
    "objectID": "store.html#project-preservation-storage-with-an-equity-lens",
    "href": "store.html#project-preservation-storage-with-an-equity-lens",
    "title": "Preservation & Storage",
    "section": "",
    "text": "As you embark on this phase of the data life cycle you will want to apply the 5 Ws and H Framework to your data preservation and storage project. \n\n\n\nWho will be responsible for preparing components of the data project for preservation and storage?\nWho will have access to which project components after preservation and archiving is complete? Be sure to think about internal team members and external partners and communities and note that each group might want or need access to different things. It’s best to work with partners and communities to determine which components they will want/need access to.\n\n\n\n\n\nWhat project components will be preserved and stored? It might not be appropriate or helpful to save everything - think through ALL of the interim and final products you developed during each phase of the data life cycle and determine which items/products should be preserved and stored, and which should be deleted.\n\nIt may be helpful to refer to your Region, Division, Office, or Program’s data or file governance and retention policy (aka Records Retention Schedule) to make these decisions, if such a policy exists. If it doesn’t you might consider creating one for your project.\nThe Water Board DIT Web Document Remediation Guidance (requires VPN) may also be a useful reference for this step.\n\n\n\n\n\n\nWhen will project components be preserved and stored? Depending on the project and needs, it may be appropriate to preserve and store some components of the project sooner than others.\nWhen will project components be deleted? Keeping all project components for all time is unrealistic and likely unnecessary. Once you have completed the “What” step above and have a list of ALL of the interim and final products you developed during each phase of the data life cycle, review each item and determine when each item will be deleted (think short-, medium-, long-term and FOREVER).\nWhen will internal team members and external partners and communities be able to access project components before and after preservation and storage is complete?\nWhen will external partners and communities be notified of the preservation and storage plan and its implementation? Ideally, external partners and communities will help you develop the preservation and storage plan during your project’s Plan & Prepare phase so everyone is on the same page with respect to project preservation and storage. If that was not the case, be sure to consult with partners (at the very least) and communities now to develop, refine, or finalize your preservation and storage plan, which should include when folks will be notified of when this phase begins and/or is complete.\n\n\n\n\n\nWhere will project components be preserved and stored? This will likely include a variety of locations (e.g., databases, open data portals, GitHub repositories, Water Boards or partner web pages, internal file locations, etc.). Be sure to be explicit about where each component will be stored, including webpage URLs or internal file paths, as appropriate.\n\n\n\n\n\nWhy are you making the decisions you’re making regarding project preservation and storage? Just as with every other phase of the data life cycle, it’s critical to document why you’re making decisions and ensure those justifications are thoroughly documented and in plain, accessible, and inclusive language, as appropriate.\n\n\n\n\n\nHow will the project preservation and storage plan be implemented?\nHow will data and products be protected and made secure through the life of the project and during preservation and storage (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\nHow will communities and project partners be able to request their personal information be removed or deleted? If this is applicable to your data project, be sure to establish a mechanism for this in partnership with the communities and project partners and during the planning phase.\nHow will products and files be named? How we name things can make or break our ability to find and resue things in the future. Jenny Bryan has developed guidance on naming things, including:\n\nMake names machine readable\nMake names human readable\nMake sure the name “plays well” with default ordering (e.g. add a date at the beginning of the file name)\n\n\n\nGraphic with a Public Service Announcement regarding how we write dates as numbers. Graphic from Bryan (2015) Naming Things slides\n\n\nFor more guidance and resources on on naming things, visit Jenny Bryan’s How to Name Files GitHub Repositiry",
    "crumbs": [
      "Preserve & Store"
    ]
  },
  {
    "objectID": "inspo.html",
    "href": "inspo.html",
    "title": "Inspiration",
    "section": "",
    "text": "Inspiration\nThe impetus for developing this Data Equity Handbook began in August 2020 when the State Water Resources Control Board (State Water Board) publicly acknowledged that the historical effects of institutional racism must be confronted throughout government, and it directed staff to develop a priority plan of action.\nSince then, the State Water Board, it’s Office of Information Management and Analysis (OIMA) and OIMA’s many internal and external partners have been developing and compiling material, and adding to this Data Equity Handbook as time and bandwidth allow.\nThis Data Equity Handbook is inspired by many sources, including:\n\nWater Boards Racial Equity - Resolution and Related Actions\nGovernment Alliance on Race and Equity (GARE)\nOpenscapes and their Approach Guide\nNOAA Fisheries (NMFS) Open Science Resource Book",
    "crumbs": [
      "Inspiration"
    ]
  },
  {
    "objectID": "eval.html",
    "href": "eval.html",
    "title": "Evaluation",
    "section": "",
    "text": "After a project is complete (or has gone through a complete iteration) - it’s important to assess the project and evaluate the extent to which it has achieved the goals you set for it and advanced and improved equity outcomes. There are several ways to achieve this. Below is a list of methods one might consider utilizing for this phase of the data life cycle.\nDepending on what you discover during the evaluation process, you may need to undergo a second phase or iteration of the project to integrate lessons and get closer to achieving the original goals and desired equity outcomes.\n\n\n\n\n\n\nTip\n\n\n\nWhile the processes below include numerical steps - that does not mean steps must be completed in that order, or that steps must be completed all at once. Moreover, teams don’t need to wait until the project is complete to benefit from using these tools! It may be helpful to use some or all of these tools during the Plan & Prepare phase, and then revisit and iterate some or all components at multiple points during your project or process.\nWhen in doubt - make time to use these tools early and often!\n\n\n\n\nRoot cause analysis is a technique that helps identify the fundamental reasons, or root causes, of a problem or unwanted outcome. Once the root causes are identified, actions can be taken to address those causes and minimize or eliminate their impact on outcomes. Identifying the root cause(s) is the most effective way to prevent future problems and ultimately remedy existing ones.\n\n\nIt can be tempting to jump straight into the Root Cause Analysis process. Before you do, it’s important to think through the below considerations and prepare accordingly.\n\n\nRoot Cause Analysis should not be completed in a silo. At the very least, the team should include individuals within our organization that are interested in working on and correcting the problem at hand. At best, the team would include partners from communities that are impacted by the problem at hand. Inclusion of external partners enables us to give voice to important perspectives and the lived experiences of those who have been and/or are currently being impacted by the problem. When the team does not include such critical perspectives, there is a tendency to rush to propose solutions that address symptoms of the problem, rather than the actual root causes of the issue.\n\n\n\n\n\n\nEffectively Partnering with Communities Requires Commitment\n\n\n\nRemember that effectively partnering with communities requires making a commitment that extends beyond soliciting feedback in focus groups and surveys. A sustained, long-term, reciprocal relationship must be built on trust and open, honest communication where power is shared. Taking the time to strengthen those relationships will not only benefit the organization but also the outcomes of the communities served.\nQuote adapted from Advancing Health Equity’s Root Cause Analysis Page\n\n\nSee Step 3 of the Planning Phase for more guidance.\n\n\n\nIt takes time to truly and deeply interrogate the “whys” and data associated with the problem at hand. Depending on the complexity of the problem, the team should plan to meet over the course of a few weeks to a few months to complete the process.\n\n\n\nIf you’ve made it this far into your process, it probably means that you are truly interested and invested in operationalizing and advancing equity into your data-intensive work. Part of that process is acknowledging hard truths and sitting with discomfort. Team members may share perspectives, experiences, or identify root causes during the process that bring up uncomfortable or unfamiliar feelings. While experiencing those feelings is normal, it’s important to remember to honor hard truths, and prioritize the process over your comfort.\n\nIntegrity is choosing courage over comfort\nIt’s choosing what’s right over what’s fun, fast, or easy\nAnd it’s practicing your values, not just professing them\n— Brené Brown, Dare to Lead\n\nWhen it comes to the Root Cause Analysis process specifically, it’s important to be aware of and push back on views that reinforce individual responsibility over the role of systemic, structural, and institutional racism.\n\n\n\n\n\nIdentify the problem\nBuilt a team that includes people who are impacted by the problem\nSchedule time over the course of a few weeks to a few months to complete the process. How you use this time is up to you. Using meetings to iterate on the brainstorming and feedback processes so you can:\n\nDig deep into the issue by working with team members to compile or analyze data to help you understand the issue\nidentify the true root causes of the issue\nIdentify potential actions to address the root causes\nPrioritize actions that the team can take to address the problem and improve equity outcomes.\n\nInterrogate the problem and associated data with an equity lens by asking “why” at least 5 times.\nPrioritize actions you/your team can take to address the root cause(s) of the problem. You will likely find root causes and potential actions that are outside of your control. Be sure to prioritize actions that you can actually address, and be realistic about the time and resources needed to fully address them.\n\nStep 4 above is the core of the process, and can be completed using many different methodologies or approaches. Below are a few simple graphical templates you can use to get started!\n\n\n\nGraphical templates illustrating approaches to the problem interrogation process. A. is a simple template to walk teams through the 5 whys and identify priority actions. B. is a more nuanced template with prompts for teams to think about potential causes of each “why” response. C. is a flow chart illustrating the complexity that can result in a thorough Root Cause Analysis process.\n\n\n\n\n\n\nResults-Based Accountability (RBA)1 is a tool to support a disciplined way of thinking and taking action that can be used to improve the performance of programs or projects and their associated outcomes.\nThe RBA process involves two parts, each asking a set of seven questions, and starts with the desired results (outcomes, impact) and works backward towards the means (solutions, strategies, actions) to ensure that plans work toward community-centered results and implementation. The RBA process also helps to distinguish between population level (whole groups) indicators, that are the responsibility of multiple systems and take a long time to shift, and performance measures (activity-specific) that teams can use to determine whether what they do is having an impact.\n\n\n\n\nIn step 4 below, teams will use Root Cause Analysis to understand the “whys” behind the trends they see in the data related to their work. As such, it will be critical to remember the considerations in the Root Cause Analysis section and apply them to Step 4 (and all other steps) of the RBA Process as well: Put a team together, Allocate time to the process, Don’t avoid hard truths.\n\n\n\nTo truly achieve the impact we require with our racial equity goals, it is essential that the work be community based and participatory. The process requires taking time to develop relationship, trust, and true partnership with the communities that are most impacted by our work so that solutions to the problem at hand are co-created with and groundtruthed by those who will ultimately benefit or be burdened by the intended and unintended impacts of the decisions.\nFor more information, see the Put a team together consideration under the Root Cause Analysis section above, and the below Living Cities RBA Video on the importance of community participatory practice.\nIn the video, Theo Miller (Executive Director of Hope SF) notes that a truly rigorous, data-driven, and equity centered RBA process requires:\n\nintegrating community in the data process\nacknowledging of the power dynamics of data\ntransparency of data and results throughout the process (see the Publish and Share section for guidance on how to make data and products transparent, open, and accessible)\n\n\n\n\n\n\nCulture is a set of living relationships working towards a shared goal. It’s not something you are, it’s something you do.\n— Daniel Coyle, The Culture Code\n\nOur organization’s culture lives at the intersection of our values and behavior. Our decisions reflect what we value (i.e., racial equity) and how we behave (i.e., our actions). Shifting our institutional cultures to be more data driven, equitable, inclusive, and kind takes time and can be uncomfortable.\nAs noted in the Beginners Guide to Anti-Racist RBA:\n\nAnti-racist RBA requires that you look inward and understand how your own power, privilege, biases, world view and identity impacts everything that you do, and then having done that personal work use your power, understanding of history, awareness of your blind spots, and ability to identify and interrupt instances of racism to make more equitable decisions in your role. These are the behaviors that lead to systems change.\n\nIn the below Living Cities RBA Video, Robin Brule (The Integration Initiative Director) details the journey and tensions she and her team at City Alive underwent and are continuously work through as they implement the RBA framework:\n\n\n\n\n\nWhen embarking on the journey to operationalize equity into our organizations and our work, it can be overwhelming and difficult to know where to start. The first part of the RBA process provides a framework for determining what actions we can take first to begin to achieve our racial equity goals.\n\nWhat are the desired results? These statements should (1) reflect the equity outcomes you want to see in the whole population (i.e., communities, cities, states), and (2) positive conditions (e.g., “healthy” vs. “not sick”). If you’re stuck, try filling in the following statement: “We want communities that are [ insert positive condition that reflects an equity outcome ]”\nWhat would the results look like? These statements should be (1) culturally relevant, contextualized, and connected to the vision the team has with respect to equity outcomes you want to see in the whole population, and (2) specific to the community of interest, not just any community. If you’re stuck, think about what experiencing the outcome/result/condition of well being would look like for that community? What would it feel like? How would people be better off?\nWhat are the community indicators that would measure the desired results? These indicators should be population-level, large-scale measures. Because of the scope and scale of the indicators, population level data often comes from government or agencies (i.e., federal, state, city, or county; e.g. source Census data). These indicators will likely reflect generations of systemic inequities that cannot be changed quickly. However, the small number of indicators that are selected can be used to focus and concentrate the team’s efforts, and to hold the team’s efforts accountable to population-level systems change over time.\nWhat do the data tell us? This step requires two parts. First, look at the data trends for each indicator using whatever demographic breakdowns that are relevant to your work (e.g. aggregated and/or disaggregated by race/ethnicity; see the Consider how you group the data for guidance). Then, conduct a Root Cause Analysis to understand the “whys” behind the data trends. Conducting a thorough root cause analysis is critical to help move teams past superficial understandings of racial inequity (e.g. symptoms) and to the underlying (root) causes of inequity.\nWho are our partners? Who should the team work with to ensure we reach our equity goals and have an impact on the community? You may have identified some of your potential partners during the Planning phase; now is another good time to take another look at the list and ask yourself:\n\nWho is missing from our partner list? Have we considered groups that we haven’t worked with (or even have avoided) in the past that could give critical perspective and insight to maximize our impact?\nFor each partner on the list: Why are they important? What do we need them for? How do we think they can contribute to the process and help us advance outcomes? Note that your response to this last question might differ from how the group would respond for themselves.\n\nNote that engaging with each partner on your list is a completely different step that requires time to build relationships and trust. For some partners on the list, that engagement might happen concurrently as you are completing this process. For others, it might come later.\nWhat works to change the data trend towards racial equity? What specific actions can be taken to address root causes and ultimately decrease racial disparities. During this step, the group should brainstorm a large number of actions, ideally in a judgement free zone. If you’re stuck, try thinking of ideas that fall under the following categories: Low-cost / no-cost ideas; ideas from community, traditional or indigenous knowledge; promising practices; evidence-based practices; creative out-of-the-box or “imagine if” ideas. More is more in this step get all ideas out there without worrying about costs, practicality, implementation, etc.\n\n\n\n\n\n\nOrganize your thoughts post-brainstorm!\n\n\n\nOnce you’ve completed the generative brainstorming process, you will likely (hopefully!) have a large number of ideas. Take some time to review ideas and group like ideas together. This will help identify similar ideas stated in different ways and will make it easier gather your thoughts when you begin the prioritization process in the next step.\n\n\nWhat actions should we start with? For each action (or group of similar action ideas), answer the questions associated with each criteria category to help determine which actions will be prioritized during implementation:\n\nValues: Is the action strengths-based, people-centered, culturally relevant, and anti-racist? Does it advance our racial equity goals?\nLeverage: How likely is the action to change the trend line towards equity? What additional resources for change does it activate?\nReach: Is the action feasible given our current resources? Will completing this action actually benefit communities experiencing inequities?\nSpecificity: Does the action have a timeline with deliverables that answer the questions who, what, when, where, and how?\n\n\n\n\n\n\nFeasibility does not equal business as usual\n\n\n\nNote that we might need to adjust and re-prioritize our workloads (i.e., how we spend our time) and resources (i.e., how we spend our money) to make an action truly feasible. If this is necessary but it also makes completing the action feasible, it should be considered a feasible action!\nThis process and way of thinking disrupts historical patterns of “doing (or not doing) what we’ve always done because we’ve always done it that way.” That way of work, regardless of our good intentions, does not produce the racial equity we demand in our communities - and ultimately perpetuates inequitable outcomes.\nIf we are truly interested and invested in advancing and operationalizing equity into our work, we need to seriously consider making different choices. This is an opportunity to walk the talk of equity - to put our time, money, and resources where our proverbial mouths are - to go from preformative words about the importance of equity to making the difficult decisions and taking the time and resources needed to actually practice and operationalize equity.\n\n\n\n\n\n\n\nIn this phase of the RBA process, teams will develop performance measures to ensure the implementation of each action will:\n\nwork to decrease racial disparities\nhelp hold teams accountable to their commitments\ngive teams an understanding of how well implementation went and whether anyone is better off\n\nFor each of the priority actions identified in Part 1, teams will need to answer the following questions:\n\nWho do you serve? Identifying who you serve (whether an institution, people, a group, or a system) helps you gain clarity about the intended impact of your work and not attempt to make people accountable for change outside their scope of work.\nWhat is the action’s intended impact? Answering this question will generate the action’s performance measures and will mean the difference between doing business as usual, which has produced racially inequitable results for generations, and being accountable for the impact of our work.\nFirst, teams will answer the following questions to understand the intended impact of the action. Review how teams responded to “What would the results look like?” during Part 1 of the RBA process and refine the vision captured in those responses to make them specific to the action.\n\nWhat is the intended impact?\nHow would we know if the action worked?\nHow would we know if anyone is better off as a result of this action?\n\nThen, while keeping the broader intentions of the action in mind, teams will answer the following questions to develop performance measures that can be used to quantify the impact of the action.\n\nHow much did you do? (Quantity; e.g., number of clients and/or activities)\nHow well did you do it? (Quality; e.g., percentage of activity that was of high quality, percentage of common measures of appropriate/high quality)\nIs anyone better off? (Impact; e.g., number or percentage change in skills/knowledge, attitude/opinion, behavior, or circumstance)\n\nWhat is the quality of the action? The purpose of this group of metrics it to ensure the action is being done well. Consider metrics that can measure cultural relevance, language access, and participation rates to more technical measures of staff training and staff-to-client ratios. If the “better off” measures show no change, quality measures sometimes tell us why we are not having an impact. Alternately, just because the action is being implemented in a high-quality manner, does not mean that the “better off” data will move in the right direction.\nWhat is the story behind the data? It is critical to review the data at the performance level, ask why, and get to root causes - many might be the same as the community level, but it’s critical to take time to regularly review, confirm and adjust as needed. Review how teams responded to “What do the data tell us?” during Part 1 of the RBA process to refresh on root causes at the population level, and then refine understanding (as needed) to make them applicable and specific to the action. Completing this step makes the difference between perpetuating systemic failures to address racially disproportionate outcomes and disrupting them to operationalize equity and justice.\nWho are the partners with a role to play? Identify which partners would be the most effective and impactful to support completing this action. Review how teams responded to “Who are our partners?” during Part 1 of the RBA process and consider who is needed and in what role to move the work of this specific action forward. When teams fail to think about the partners that would make their work more effective, it results in the continuation of business as usual, and prevents them from having the thought partners at the table that would allow them to take their work from good to transformative.\nWhat works to have greater impact? As teams implement the action, take time periodically to reflect, especially when things are not going well, as planned, or the data coming in does not show the expected impact. Remember that you have already identified root cases at the population level (Part 1, What do the data tell us?) and the performance level (Part 2, What is the story behind the data?), and know that it takes time to see transformative and cultural change. If sufficient time has passed, consider how to use lessons learned, agency, partner, and community experience, and data to inform how to adapt the action to have a greater impact.\nWhat are the next steps? A commitment to action, just like in the population level process, is critical. This is where we turn our words into actions. Teams answer the questions below with as much precision and specificity as possible.\n\nWho will do what, by when?\nWhat resources are needed to get it done?\nIs this a long-term action that needs time or can it be done tomorrow?\nWhat is the active role of community leadership in making these decisions?\n\n\n\n\n\n\n\nDocument commitments to support collective accountability\n\n\n\nWhite answers to the above questions down and get confirmation from all parties to ensure everyone is on the same page with respect to agreed upon commitments.\nData will be required to evaluate the performance measures the team developed. Integrate those performance measures, data needs, commitments, and timelines into the Data & Product Evaluation section of the project’s Data Management Plan!\n\n\n\nBelow is a graphical illustration of the main RBA Part 2 process components.\n\n\n\nGraphical illustration of the components of Part 2 of the RBA Process teams will go through to develop performance measures for each priority action, which will enable collective accountability throughout the implementation process.\n\n\n\n\n\n\nThe questions below can helps us integrate an explicit focus on racial equity as we develop, review, and revise our projects. Some of the questions below are similar to the questions posed in the Root Cause Analysis and RBA sections, as well as in the Planning Phase. The main difference is that Root Cause Analysis, RBA, and planning phases can involve formal longer-term processes whereas the questions below are more flexible and can be applied as needed during any phase of a project.\n\nWhat’s the policy, program, practice, or budget decision under consideration? What are the desired results and outcomes?\nWhat’s the data and what does it tell us?\nHow have communities been engaged? Are there opportunities to expand engagement?\nWho will benefit from or be burdened by the proposal or program, etc.?\nWhat are the strategies for advancing racial equity or mitigating unintended consequences?\nWhat is the plan for implementation?\nHow will you ensure accountability, communicate, and evaluation results?\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s really important for us to remember that when racial equity is left off the table entirely, added to a process haphazardly, or not considered until the last minute, using the above racial equity processes is less likely to be fruitful, and for the communities we’re trying to serve – regardless of our good intentions – it can feel more like a box checking performance rather than an earnest effort to advance equity.\nStarting these processes early in the project life cycle, revisiting them often, and in partnership with communities, can help us build relationships and trust, and speaks to the need for our teams, organizations, and systems to operationalize equity with urgency and care.\n\n\n\n\n\n\nAdvancing Health Equity Root Cause Analyses Guide\nAmerican Society for Quality Root Cause Analysis Guide\nRoot Cause Analysis Templates for PowerPoint & Google Slides\nExamples related to identifying root causes of health inequities\n\nPioneer Valley Planning Exchange - Root Cause Solutions Exchange Process\nNational Center for Education in Maternal and Child Health - Ready, Set, Go approach that utilizes Root Cause Analysis\n\nGARE guide that connects a racial equity lens to the RBA methodology\nA beginners guide to anti-racist Results Based Accountability (RBA)",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#root-cause-analysis",
    "href": "eval.html#root-cause-analysis",
    "title": "Evaluation",
    "section": "",
    "text": "Root cause analysis is a technique that helps identify the fundamental reasons, or root causes, of a problem or unwanted outcome. Once the root causes are identified, actions can be taken to address those causes and minimize or eliminate their impact on outcomes. Identifying the root cause(s) is the most effective way to prevent future problems and ultimately remedy existing ones.\n\n\nIt can be tempting to jump straight into the Root Cause Analysis process. Before you do, it’s important to think through the below considerations and prepare accordingly.\n\n\nRoot Cause Analysis should not be completed in a silo. At the very least, the team should include individuals within our organization that are interested in working on and correcting the problem at hand. At best, the team would include partners from communities that are impacted by the problem at hand. Inclusion of external partners enables us to give voice to important perspectives and the lived experiences of those who have been and/or are currently being impacted by the problem. When the team does not include such critical perspectives, there is a tendency to rush to propose solutions that address symptoms of the problem, rather than the actual root causes of the issue.\n\n\n\n\n\n\nEffectively Partnering with Communities Requires Commitment\n\n\n\nRemember that effectively partnering with communities requires making a commitment that extends beyond soliciting feedback in focus groups and surveys. A sustained, long-term, reciprocal relationship must be built on trust and open, honest communication where power is shared. Taking the time to strengthen those relationships will not only benefit the organization but also the outcomes of the communities served.\nQuote adapted from Advancing Health Equity’s Root Cause Analysis Page\n\n\nSee Step 3 of the Planning Phase for more guidance.\n\n\n\nIt takes time to truly and deeply interrogate the “whys” and data associated with the problem at hand. Depending on the complexity of the problem, the team should plan to meet over the course of a few weeks to a few months to complete the process.\n\n\n\nIf you’ve made it this far into your process, it probably means that you are truly interested and invested in operationalizing and advancing equity into your data-intensive work. Part of that process is acknowledging hard truths and sitting with discomfort. Team members may share perspectives, experiences, or identify root causes during the process that bring up uncomfortable or unfamiliar feelings. While experiencing those feelings is normal, it’s important to remember to honor hard truths, and prioritize the process over your comfort.\n\nIntegrity is choosing courage over comfort\nIt’s choosing what’s right over what’s fun, fast, or easy\nAnd it’s practicing your values, not just professing them\n— Brené Brown, Dare to Lead\n\nWhen it comes to the Root Cause Analysis process specifically, it’s important to be aware of and push back on views that reinforce individual responsibility over the role of systemic, structural, and institutional racism.\n\n\n\n\n\nIdentify the problem\nBuilt a team that includes people who are impacted by the problem\nSchedule time over the course of a few weeks to a few months to complete the process. How you use this time is up to you. Using meetings to iterate on the brainstorming and feedback processes so you can:\n\nDig deep into the issue by working with team members to compile or analyze data to help you understand the issue\nidentify the true root causes of the issue\nIdentify potential actions to address the root causes\nPrioritize actions that the team can take to address the problem and improve equity outcomes.\n\nInterrogate the problem and associated data with an equity lens by asking “why” at least 5 times.\nPrioritize actions you/your team can take to address the root cause(s) of the problem. You will likely find root causes and potential actions that are outside of your control. Be sure to prioritize actions that you can actually address, and be realistic about the time and resources needed to fully address them.\n\nStep 4 above is the core of the process, and can be completed using many different methodologies or approaches. Below are a few simple graphical templates you can use to get started!\n\n\n\nGraphical templates illustrating approaches to the problem interrogation process. A. is a simple template to walk teams through the 5 whys and identify priority actions. B. is a more nuanced template with prompts for teams to think about potential causes of each “why” response. C. is a flow chart illustrating the complexity that can result in a thorough Root Cause Analysis process.",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#results-based-accountability-rba",
    "href": "eval.html#results-based-accountability-rba",
    "title": "Evaluation",
    "section": "",
    "text": "Results-Based Accountability (RBA)1 is a tool to support a disciplined way of thinking and taking action that can be used to improve the performance of programs or projects and their associated outcomes.\nThe RBA process involves two parts, each asking a set of seven questions, and starts with the desired results (outcomes, impact) and works backward towards the means (solutions, strategies, actions) to ensure that plans work toward community-centered results and implementation. The RBA process also helps to distinguish between population level (whole groups) indicators, that are the responsibility of multiple systems and take a long time to shift, and performance measures (activity-specific) that teams can use to determine whether what they do is having an impact.\n\n\n\n\nIn step 4 below, teams will use Root Cause Analysis to understand the “whys” behind the trends they see in the data related to their work. As such, it will be critical to remember the considerations in the Root Cause Analysis section and apply them to Step 4 (and all other steps) of the RBA Process as well: Put a team together, Allocate time to the process, Don’t avoid hard truths.\n\n\n\nTo truly achieve the impact we require with our racial equity goals, it is essential that the work be community based and participatory. The process requires taking time to develop relationship, trust, and true partnership with the communities that are most impacted by our work so that solutions to the problem at hand are co-created with and groundtruthed by those who will ultimately benefit or be burdened by the intended and unintended impacts of the decisions.\nFor more information, see the Put a team together consideration under the Root Cause Analysis section above, and the below Living Cities RBA Video on the importance of community participatory practice.\nIn the video, Theo Miller (Executive Director of Hope SF) notes that a truly rigorous, data-driven, and equity centered RBA process requires:\n\nintegrating community in the data process\nacknowledging of the power dynamics of data\ntransparency of data and results throughout the process (see the Publish and Share section for guidance on how to make data and products transparent, open, and accessible)\n\n\n\n\n\n\nCulture is a set of living relationships working towards a shared goal. It’s not something you are, it’s something you do.\n— Daniel Coyle, The Culture Code\n\nOur organization’s culture lives at the intersection of our values and behavior. Our decisions reflect what we value (i.e., racial equity) and how we behave (i.e., our actions). Shifting our institutional cultures to be more data driven, equitable, inclusive, and kind takes time and can be uncomfortable.\nAs noted in the Beginners Guide to Anti-Racist RBA:\n\nAnti-racist RBA requires that you look inward and understand how your own power, privilege, biases, world view and identity impacts everything that you do, and then having done that personal work use your power, understanding of history, awareness of your blind spots, and ability to identify and interrupt instances of racism to make more equitable decisions in your role. These are the behaviors that lead to systems change.\n\nIn the below Living Cities RBA Video, Robin Brule (The Integration Initiative Director) details the journey and tensions she and her team at City Alive underwent and are continuously work through as they implement the RBA framework:\n\n\n\n\n\nWhen embarking on the journey to operationalize equity into our organizations and our work, it can be overwhelming and difficult to know where to start. The first part of the RBA process provides a framework for determining what actions we can take first to begin to achieve our racial equity goals.\n\nWhat are the desired results? These statements should (1) reflect the equity outcomes you want to see in the whole population (i.e., communities, cities, states), and (2) positive conditions (e.g., “healthy” vs. “not sick”). If you’re stuck, try filling in the following statement: “We want communities that are [ insert positive condition that reflects an equity outcome ]”\nWhat would the results look like? These statements should be (1) culturally relevant, contextualized, and connected to the vision the team has with respect to equity outcomes you want to see in the whole population, and (2) specific to the community of interest, not just any community. If you’re stuck, think about what experiencing the outcome/result/condition of well being would look like for that community? What would it feel like? How would people be better off?\nWhat are the community indicators that would measure the desired results? These indicators should be population-level, large-scale measures. Because of the scope and scale of the indicators, population level data often comes from government or agencies (i.e., federal, state, city, or county; e.g. source Census data). These indicators will likely reflect generations of systemic inequities that cannot be changed quickly. However, the small number of indicators that are selected can be used to focus and concentrate the team’s efforts, and to hold the team’s efforts accountable to population-level systems change over time.\nWhat do the data tell us? This step requires two parts. First, look at the data trends for each indicator using whatever demographic breakdowns that are relevant to your work (e.g. aggregated and/or disaggregated by race/ethnicity; see the Consider how you group the data for guidance). Then, conduct a Root Cause Analysis to understand the “whys” behind the data trends. Conducting a thorough root cause analysis is critical to help move teams past superficial understandings of racial inequity (e.g. symptoms) and to the underlying (root) causes of inequity.\nWho are our partners? Who should the team work with to ensure we reach our equity goals and have an impact on the community? You may have identified some of your potential partners during the Planning phase; now is another good time to take another look at the list and ask yourself:\n\nWho is missing from our partner list? Have we considered groups that we haven’t worked with (or even have avoided) in the past that could give critical perspective and insight to maximize our impact?\nFor each partner on the list: Why are they important? What do we need them for? How do we think they can contribute to the process and help us advance outcomes? Note that your response to this last question might differ from how the group would respond for themselves.\n\nNote that engaging with each partner on your list is a completely different step that requires time to build relationships and trust. For some partners on the list, that engagement might happen concurrently as you are completing this process. For others, it might come later.\nWhat works to change the data trend towards racial equity? What specific actions can be taken to address root causes and ultimately decrease racial disparities. During this step, the group should brainstorm a large number of actions, ideally in a judgement free zone. If you’re stuck, try thinking of ideas that fall under the following categories: Low-cost / no-cost ideas; ideas from community, traditional or indigenous knowledge; promising practices; evidence-based practices; creative out-of-the-box or “imagine if” ideas. More is more in this step get all ideas out there without worrying about costs, practicality, implementation, etc.\n\n\n\n\n\n\nOrganize your thoughts post-brainstorm!\n\n\n\nOnce you’ve completed the generative brainstorming process, you will likely (hopefully!) have a large number of ideas. Take some time to review ideas and group like ideas together. This will help identify similar ideas stated in different ways and will make it easier gather your thoughts when you begin the prioritization process in the next step.\n\n\nWhat actions should we start with? For each action (or group of similar action ideas), answer the questions associated with each criteria category to help determine which actions will be prioritized during implementation:\n\nValues: Is the action strengths-based, people-centered, culturally relevant, and anti-racist? Does it advance our racial equity goals?\nLeverage: How likely is the action to change the trend line towards equity? What additional resources for change does it activate?\nReach: Is the action feasible given our current resources? Will completing this action actually benefit communities experiencing inequities?\nSpecificity: Does the action have a timeline with deliverables that answer the questions who, what, when, where, and how?\n\n\n\n\n\n\nFeasibility does not equal business as usual\n\n\n\nNote that we might need to adjust and re-prioritize our workloads (i.e., how we spend our time) and resources (i.e., how we spend our money) to make an action truly feasible. If this is necessary but it also makes completing the action feasible, it should be considered a feasible action!\nThis process and way of thinking disrupts historical patterns of “doing (or not doing) what we’ve always done because we’ve always done it that way.” That way of work, regardless of our good intentions, does not produce the racial equity we demand in our communities - and ultimately perpetuates inequitable outcomes.\nIf we are truly interested and invested in advancing and operationalizing equity into our work, we need to seriously consider making different choices. This is an opportunity to walk the talk of equity - to put our time, money, and resources where our proverbial mouths are - to go from preformative words about the importance of equity to making the difficult decisions and taking the time and resources needed to actually practice and operationalize equity.\n\n\n\n\n\n\n\nIn this phase of the RBA process, teams will develop performance measures to ensure the implementation of each action will:\n\nwork to decrease racial disparities\nhelp hold teams accountable to their commitments\ngive teams an understanding of how well implementation went and whether anyone is better off\n\nFor each of the priority actions identified in Part 1, teams will need to answer the following questions:\n\nWho do you serve? Identifying who you serve (whether an institution, people, a group, or a system) helps you gain clarity about the intended impact of your work and not attempt to make people accountable for change outside their scope of work.\nWhat is the action’s intended impact? Answering this question will generate the action’s performance measures and will mean the difference between doing business as usual, which has produced racially inequitable results for generations, and being accountable for the impact of our work.\nFirst, teams will answer the following questions to understand the intended impact of the action. Review how teams responded to “What would the results look like?” during Part 1 of the RBA process and refine the vision captured in those responses to make them specific to the action.\n\nWhat is the intended impact?\nHow would we know if the action worked?\nHow would we know if anyone is better off as a result of this action?\n\nThen, while keeping the broader intentions of the action in mind, teams will answer the following questions to develop performance measures that can be used to quantify the impact of the action.\n\nHow much did you do? (Quantity; e.g., number of clients and/or activities)\nHow well did you do it? (Quality; e.g., percentage of activity that was of high quality, percentage of common measures of appropriate/high quality)\nIs anyone better off? (Impact; e.g., number or percentage change in skills/knowledge, attitude/opinion, behavior, or circumstance)\n\nWhat is the quality of the action? The purpose of this group of metrics it to ensure the action is being done well. Consider metrics that can measure cultural relevance, language access, and participation rates to more technical measures of staff training and staff-to-client ratios. If the “better off” measures show no change, quality measures sometimes tell us why we are not having an impact. Alternately, just because the action is being implemented in a high-quality manner, does not mean that the “better off” data will move in the right direction.\nWhat is the story behind the data? It is critical to review the data at the performance level, ask why, and get to root causes - many might be the same as the community level, but it’s critical to take time to regularly review, confirm and adjust as needed. Review how teams responded to “What do the data tell us?” during Part 1 of the RBA process to refresh on root causes at the population level, and then refine understanding (as needed) to make them applicable and specific to the action. Completing this step makes the difference between perpetuating systemic failures to address racially disproportionate outcomes and disrupting them to operationalize equity and justice.\nWho are the partners with a role to play? Identify which partners would be the most effective and impactful to support completing this action. Review how teams responded to “Who are our partners?” during Part 1 of the RBA process and consider who is needed and in what role to move the work of this specific action forward. When teams fail to think about the partners that would make their work more effective, it results in the continuation of business as usual, and prevents them from having the thought partners at the table that would allow them to take their work from good to transformative.\nWhat works to have greater impact? As teams implement the action, take time periodically to reflect, especially when things are not going well, as planned, or the data coming in does not show the expected impact. Remember that you have already identified root cases at the population level (Part 1, What do the data tell us?) and the performance level (Part 2, What is the story behind the data?), and know that it takes time to see transformative and cultural change. If sufficient time has passed, consider how to use lessons learned, agency, partner, and community experience, and data to inform how to adapt the action to have a greater impact.\nWhat are the next steps? A commitment to action, just like in the population level process, is critical. This is where we turn our words into actions. Teams answer the questions below with as much precision and specificity as possible.\n\nWho will do what, by when?\nWhat resources are needed to get it done?\nIs this a long-term action that needs time or can it be done tomorrow?\nWhat is the active role of community leadership in making these decisions?\n\n\n\n\n\n\n\nDocument commitments to support collective accountability\n\n\n\nWhite answers to the above questions down and get confirmation from all parties to ensure everyone is on the same page with respect to agreed upon commitments.\nData will be required to evaluate the performance measures the team developed. Integrate those performance measures, data needs, commitments, and timelines into the Data & Product Evaluation section of the project’s Data Management Plan!\n\n\n\nBelow is a graphical illustration of the main RBA Part 2 process components.\n\n\n\nGraphical illustration of the components of Part 2 of the RBA Process teams will go through to develop performance measures for each priority action, which will enable collective accountability throughout the implementation process.",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#other-racial-equity-lens-evaluation-questions",
    "href": "eval.html#other-racial-equity-lens-evaluation-questions",
    "title": "Evaluation",
    "section": "",
    "text": "The questions below can helps us integrate an explicit focus on racial equity as we develop, review, and revise our projects. Some of the questions below are similar to the questions posed in the Root Cause Analysis and RBA sections, as well as in the Planning Phase. The main difference is that Root Cause Analysis, RBA, and planning phases can involve formal longer-term processes whereas the questions below are more flexible and can be applied as needed during any phase of a project.\n\nWhat’s the policy, program, practice, or budget decision under consideration? What are the desired results and outcomes?\nWhat’s the data and what does it tell us?\nHow have communities been engaged? Are there opportunities to expand engagement?\nWho will benefit from or be burdened by the proposal or program, etc.?\nWhat are the strategies for advancing racial equity or mitigating unintended consequences?\nWhat is the plan for implementation?\nHow will you ensure accountability, communicate, and evaluation results?\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s really important for us to remember that when racial equity is left off the table entirely, added to a process haphazardly, or not considered until the last minute, using the above racial equity processes is less likely to be fruitful, and for the communities we’re trying to serve – regardless of our good intentions – it can feel more like a box checking performance rather than an earnest effort to advance equity.\nStarting these processes early in the project life cycle, revisiting them often, and in partnership with communities, can help us build relationships and trust, and speaks to the need for our teams, organizations, and systems to operationalize equity with urgency and care.",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#additional-resources",
    "href": "eval.html#additional-resources",
    "title": "Evaluation",
    "section": "",
    "text": "Advancing Health Equity Root Cause Analyses Guide\nAmerican Society for Quality Root Cause Analysis Guide\nRoot Cause Analysis Templates for PowerPoint & Google Slides\nExamples related to identifying root causes of health inequities\n\nPioneer Valley Planning Exchange - Root Cause Solutions Exchange Process\nNational Center for Education in Maternal and Child Health - Ready, Set, Go approach that utilizes Root Cause Analysis\n\nGARE guide that connects a racial equity lens to the RBA methodology\nA beginners guide to anti-racist Results Based Accountability (RBA)",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "eval.html#footnotes",
    "href": "eval.html#footnotes",
    "title": "Evaluation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe bulk of content in the Results Based Accountability (RBA) section has been informed by: The Government Alliance on Race and Equity (GARE) guide to applying an equity lens to the RBA methodology↩︎",
    "crumbs": [
      "Discover & Integrate"
    ]
  },
  {
    "objectID": "collect-process/index.html",
    "href": "collect-process/index.html",
    "title": "Collect & Process",
    "section": "",
    "text": "Collect & Process\nAt this phase, you should have already completed your equity assessment (Planning) and developed your data management plan using an equity lens (Data Preparation). Now you’re ready to begin the data work!\nThis phase of the data life cycle involves gathering the data you identified during the Plan & Prep phase (Data Collection) and getting it ready so it can be used in your analyses or product development steps (Data Processing).",
    "crumbs": [
      "Collect & Process"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "In recent years, the United States government has made strides towards promoting equitable data and environmental justice, as reflected in documents released by the White House such as a Vision for Equitable Data (2021), a fact sheet on the Federal Government’s plans to advance racial equity (2024), and The Environmental Justice Science, Data, and Research Plan (2024). The U.S. Environmental Protection Agency has similarly provided guidance for Assessing Environmental Justice in Regulatory Analysis and maintains a working list of resources for related to equity and water data at the Health & Environmental Research Online (HERO) Database.\nIn alignment with national priorities, the State Water Resources Control Board (State Water Board) publicly acknowledged during its August 18, 2020 meeting that the historical effects of institutional racism must be confronted throughout government, and it directed staff to develop a priority plan of action (Aug 18, 2020 Meeting Agenda, Recording). The State Water Board’s Racial Equity Team held public and employee listening sessions to help develop a draft resolution. After a public comment period on the draft resolution in spring 2021, the Racial Equity Team made significant updates to the resolution. On November 16, 2021, the State Water Board adopted Resolution No. 2021-0050, “Condemning Racism, Xenophobia, Bigotry, and Racial Injustice and Strengthening Commitment to Racial Equity, Diversity, Inclusion, Access, and Anti-Racism” (Racial Equity Resolution; English, Español) which affirms the State Water Board’s commitment to racial equity and directs staff to undertake a variety of actions to achieve racial equity throughout all State Water Board programs and activities (Nov 16, 2021 Meeting Agenda, Recording).\nThe Racial Equity Resolution is one milestone on our ongoing journey to operationalize equity throughout our organization. The next step is to implement the Racial Equity Action Plan (2023-2025; English, Español), which includes specific actions the State Water Board will take to address racial inequities, as well as metrics to measure our progress. With this Action Plan, we envision a sustainable California where race no longer predicts where clean water is available or who has access to it. \n\n\n\n\n\n\nEquity is an outcome\n\n\n\nIt’s important to note that racial equity and equity in general is an outcome and there is no such thing as a “racial equity data-set” or “racial equity data”.\nInstead we should think of data as a tool to help us achieve the overall outcome of equity - and how we view, use, and act on data and related tools is what will determine whether we operationalize equity or perpetuate injustice.\n\n\nDevelopment of the Racial Equity Action Plan began in Spring 2022 and involved public and employee engagement and tribal consultations. The Water Boards’ Racial Equity Team presented the Racial Equity Action Plan (2023-2025) to the State Water Board as an informational item at the Board Meeting on January 18, 2023 (Agenda, Slides, Recording).\nGoal 1a of the Racial Equity Action Plan is to ensure Water Boards data are accessible, equitable, and culturally relevant. One action captured under that goal is the development of a Racial Equity Data Action Plan which must: \n\nDevelop training and best practices guidance for Water Boards staff on incorporating racial equity concepts into the planning and design of data collection methods and visualizations (e.g., maps, factsheets, etc.) projects.\nIdentify and expand existing opportunities for public participation in science and community data gathering programs to develop new data collection methods, support existing programs, and incorporate community datasets into the database. \nCreate a publicly accessible data catalog tool / interface that includes existing demographic data, Water Boards program data, and other available data (such as heat maps or flood hazard maps) to inform the implementation of the Racial Equity Action Plan.\n\nThe Racial Equity Data Action Plan is being developed by the Water Board’s Racial Equity Data Subcommittee of the Environmental Justice Roundtable which is a group of volunteer staff from across the Water Board led by the Office of Information Management and Analysis and the Office of Public Participation. This Handbook is intended to address Item 1 above by providing staff a best practices guide for incorporating racial equity concepts into the Water Board programs using data and information. Item 2 above will be iterative and grow as staff and programs begin to utilize the guidance and tools found within this handbook. To fulfill Item 3 above staff have created a California Water Boards Racial Equity Data Resource Hub which will grow as more programs create and publish racial equity based tools and visualizations.\n\n\n\nRacial Equity at the Water Boards\nRacial Equity Resolution Annotated References (2021)\nCalifornia Governor Executive Order on Racial Equity (Executive Order N-16-22)",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "background.html#additional-resources",
    "href": "background.html#additional-resources",
    "title": "Background",
    "section": "",
    "text": "Racial Equity at the Water Boards\nRacial Equity Resolution Annotated References (2021)\nCalifornia Governor Executive Order on Racial Equity (Executive Order N-16-22)",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "publish-data.html",
    "href": "publish-data.html",
    "title": "Publish Water Boards Data",
    "section": "",
    "text": "Publish Water Boards Data\nIn accordance with the first principle of the Water Board’s Open Data Resolution (Resolution No. 2018-0032; see below), it is the responsibility of Water Boards data stewards to make our data open and accessible to the public.\n\nMake Data Accessible (“Open First”): our organization values transparency and strives to make all critical public data available in machine-readable datasets with metadata and data dictionaries.\n\nWhen it comes to doing racial equity data work, a critical first step that should occur before we consider taking on additional data related projects (e.g. making maps or other visualizations) is to make the data that already exits and for which we steward and/or are responsible, open, transparent, and accessible to the public (as appropriate).\nNot only does making data open help improve transparency, but doing so before you build your data interpretation tools (e.g. visualizations, reports) makes it easier for the communities we serve to access the data behind our interpretations and can ultimately help build relationships and trust.\nMaking data open is more than adding a spreadsheet or flat file to a webpage. Truly making your data open involves investing the time and resources required to follow the guidance on open data publishing at the Water Boards is available in the Open Data Handbook, including:\n\nmaking data machine readable\nformatting data in a way that aligns with common data standards\nensuring the quality of data is known and documented\ngoing through data security process\ngenerating and ensuring all metadata and documentation related to the data is comprehensive and complete\npublishing data and all documentation in an open data platform / format\n\nContact OIMA (OIMA-Helpdesk@waterboards.ca.gov) for specific guidance or support with navigating through the open data publication process at the Water Boards.",
    "crumbs": [
      "Publish Your Data"
    ]
  },
  {
    "objectID": "use-cases/index.html",
    "href": "use-cases/index.html",
    "title": "Use Cases",
    "section": "",
    "text": "There is no single correct way to operationalize equity in a data project. Each project has it’s own objectives, data, and audience - and truly operationalizing equity in your data work means dedicating the time, attention, and resources needed to meet the needs of the project (i.e. avoid one-size fits all approaches).\nIn this Use Cases section, you will find some examples of how some Water Boards staff have approached operationalizing equity into their data-intensive work, including use cases from:\n\nThe Statewide Surface Water Ambient Monitoring Program (SWAMP)\nDemographics Data Integration Examples\n\n\n\n\n\n\n\nHelp us highlight your data equity work!\n\n\n\nIf you are the lead of a data-intensive project that has worked to operationalize equity into some or all phases of your project’s data life cycle, and would like to add a page to this Handbook to highlight the work - let us know!\nPlease complete the “Additonal Feedback” section of the Equity Data Handbook Feedback Form to send us a short description of your project and what you’d like to highlight!",
    "crumbs": [
      "Use Cases"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation is a love letter that you write to your future self.\n— Damian Conway, computer scientist, a public speaker, and the author of several books.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember: if your project or process is not well-documented to the point of being largely reproducible - it’s incomplete!\n\n\n\n\nDocumentation is a critical to improving transparency, clarity, and consistency of products and process. When done well, documentation can significantly reduce the amount of time wasted by team members, partners, the public (and ourselves!) who are looking for answers, institutional knowledge, or process and eliminate unnecessary duplication of work.\n\n\n\nComic illustrating one team member passing along a project to another team member without proper documentation…confusion, frustration, and lots of wasted time ensues. Original source of the comic is unknown.\n\n\nDetailed documentation of every aspect of a project and all phases of the data and project life cycle - from planning, to data sources, analysis methods, code, workflows, and even why decisions were made - is critical because it helps ensure that data, results, and products are meaningful, accessible, and actionable for our teams, partners, communities, and the public. Documentation in our data-intensive workflows can also help identify areas of improvement in how we work with ourselves and others, and make it easier to identify solutions to project sticking points and create other paths forward. Moreover, clear documentation allows us to pick up where we left off, or easily transition projects from one person or team to another when roles change thus preventing the loss of incredibly valuable institutional knowledge that only comes with time and experience. Without proper documentation, it can be difficult for others to have confidence in, ground truth/validate, or build on existing work.\nIndividuals and teams that invest the time to create clear and complete documentation tend to find projects to be a bit more manageable because they have resources to guide them. Over the long-term, that investment into clear and comprehensive documentation can increase the reproducibility, transparency, and accessibility of their work. All of this, in turn, can make it easier to collaborate with internal and external partners, help build relationships and trust with partners and communities impacted by the project thus helping to operationalize equity into our projects, products, and process.\n\n\n\nIllustration of core benefits of documentation, which require the documentation to be reproducible, transparent, and accessible. Illustration adapted from LaneFour\n\n\n\n\n\nThere are abundantly clear benefits to ourselves, our teams, and our partners when we invest in comprehensive and clear documentation. As you embark on your documentation journey, be sure to also consider what documentation would be helpful for others outside of your normal or core project team, keep the below considerations in mind, and remember to document with an equity lens.\nReproducibility: Reproducibility is a vital aspect of documentation that plays a key role in advancing and operationalizing equity into our data-intensive work. When our work is documented in a way that allows ourselves and others to replicate process and findings, it ensures that folks can access and validate the data as part of their understanding and workflows.\nTransparency: Transparency in documentation is particularly vital for building trust with communities that have historically and to this day experience a lack of openness or consideration by government. By prioritizing transparent documentation, we can clearly communicate and connect the dots for folks when it comes to how we work, what data and methods we use, what that data tells us and the reasoning and actions we take throughout our processes, thus better enabling communities to understand how and why we make certain decisions and how all of that may impact them and others. This openness can empower current and potential future partners and communities to engage meaningfully with our work, ensuring that their voices are respected and needs are considered in ongoing and future work.\n\n\n\n\n\n\nOpen and Transparent Doesn’t Always Mean Public\n\n\n\nWhile we want to make as much of our process and products open and transparent as possible, this does not mean the EVERYTHING should be made public. It’s important to balance the need to keep confidential and private information secure while also making process and products open, transparent, and publicly available. This balance varies by project.\nWhat’s important is for teams to discuss what this balance looks like for them and act, document, and share accordingly.\n\n\nAccessibility: When our is projects, process, and products are presented in ways that center accessibility— such as offering offline materials, using multiple languages, or documenting in plain language — it lowers barriers and makes it easier for individuals to participate and engage. Prioritizing accessibility in our documentation not only increases access to vital information but also demonstrates a genuine commitment to inclusivity and respect for diverse experiences.\nCollaboration: If documentation exists and that documentation is comprehensive, reproducible, transparent, and accessible then (and only then!) will our projects, products, and process help us rebuild trust, repair relationships and/or begin to build new ones, which ultimately make it easier for communities to want to engage, collaborate and meaningfully contribute to our processes, or better yet, co-create products and resources with us. Prioritizing and investing in documentation, and creating it with an equity lens, can help us bridge gaps in communication and understanding, ultimately fostering trust, collaboration, and relationships with communities that have had the experience of being excluded or otherwise overlooked in our projects, products, and process.\nComprehensive documentation using an equity lens can be yet another tool we can use to operationalize equity and advance more equitable outcomes in our work.\n\n\n\nPoor documentation is rarely used or referred to after it is written, and feels like (and often is) a waste of time.\nGreat documentation is an investment that is built over time. No one documents something and gets everything down perfectly the first time. Teams with great documentation decide progress is better than perfection and make time to iteratively improve, build on, and evolve what is documented over time so that it continues to be comprehensive and useful beyond when it was first drafted. This is why it’s critical for teams to think through:\n\nwhat the team will document\nhow documentation will be developed\nwhere documentation will be stored\nhow it will be shared (or not)\nwho will be responsible for creating and maintaining documentation over time\n\nWhen teams are beginning their documentation discussions and process, they should expect to document (almost) everything in some form, and plan to invest time accordingly. A list of things to consider documenting is provided in the graphic below.\n\n\n\n\n\n\nTip\n\n\n\nTrying to document everything below all at once can feel overwhelming and impossible. That’s because it is!\nRemember that perfection is the enemy of good. Focus on making iterative progress and improvements given your teams priorities, bandwidth, and capacity.\nWith time and consistent investment in documentation, your team can have everything below (and more!) documented comprehensively and in ways that advance and operationalize equity.\n\n\n\n\n\nIllustration of the different types of things to document for a project under four documentation categories: Team, Process, Technical, and User\n\n\n\n\n\n\nOpenscapes Pathway - Join Openscapes at the Water Boards to learn more and build your Team Pathway!\nSchema.org – Promoting common data structures on the internet\nAccessibility tips using Markdown (Smashing Magazine)\nWritethedocs.org, plus a piece on documenting for beginners\nTemplates for documenting on GitHub (e.g., on a ReadMe)\nCreating and maintaining a Data Dictionary\nUS EPA Guidance for Content Item Metadata in ArcGIS Online\nDocument code through comments and #tidydata\nGitHub projects best practices\n\n\n\n\nNOAA NMFS Open Science Resources - A Compilation of Resources and Experience to Support Open Science Communities in NOAA Fisheries\nNOAA Fisheries Integrated Toolbox - Browse software tools for NOAA Fisheries scientific work\nNASA Earthdata Cloud Cookbook - Supporting NASA Research Teams’ Migration to the Cloud\nOpenscapes Approach Guide - Guidance for Openscapes leaders on how to implement Openscapes programs and approaches",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#why-document",
    "href": "documentation.html#why-document",
    "title": "Documentation",
    "section": "",
    "text": "Documentation is a critical to improving transparency, clarity, and consistency of products and process. When done well, documentation can significantly reduce the amount of time wasted by team members, partners, the public (and ourselves!) who are looking for answers, institutional knowledge, or process and eliminate unnecessary duplication of work.\n\n\n\nComic illustrating one team member passing along a project to another team member without proper documentation…confusion, frustration, and lots of wasted time ensues. Original source of the comic is unknown.\n\n\nDetailed documentation of every aspect of a project and all phases of the data and project life cycle - from planning, to data sources, analysis methods, code, workflows, and even why decisions were made - is critical because it helps ensure that data, results, and products are meaningful, accessible, and actionable for our teams, partners, communities, and the public. Documentation in our data-intensive workflows can also help identify areas of improvement in how we work with ourselves and others, and make it easier to identify solutions to project sticking points and create other paths forward. Moreover, clear documentation allows us to pick up where we left off, or easily transition projects from one person or team to another when roles change thus preventing the loss of incredibly valuable institutional knowledge that only comes with time and experience. Without proper documentation, it can be difficult for others to have confidence in, ground truth/validate, or build on existing work.\nIndividuals and teams that invest the time to create clear and complete documentation tend to find projects to be a bit more manageable because they have resources to guide them. Over the long-term, that investment into clear and comprehensive documentation can increase the reproducibility, transparency, and accessibility of their work. All of this, in turn, can make it easier to collaborate with internal and external partners, help build relationships and trust with partners and communities impacted by the project thus helping to operationalize equity into our projects, products, and process.\n\n\n\nIllustration of core benefits of documentation, which require the documentation to be reproducible, transparent, and accessible. Illustration adapted from LaneFour",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#documenting-with-an-equity-lens",
    "href": "documentation.html#documenting-with-an-equity-lens",
    "title": "Documentation",
    "section": "",
    "text": "There are abundantly clear benefits to ourselves, our teams, and our partners when we invest in comprehensive and clear documentation. As you embark on your documentation journey, be sure to also consider what documentation would be helpful for others outside of your normal or core project team, keep the below considerations in mind, and remember to document with an equity lens.\nReproducibility: Reproducibility is a vital aspect of documentation that plays a key role in advancing and operationalizing equity into our data-intensive work. When our work is documented in a way that allows ourselves and others to replicate process and findings, it ensures that folks can access and validate the data as part of their understanding and workflows.\nTransparency: Transparency in documentation is particularly vital for building trust with communities that have historically and to this day experience a lack of openness or consideration by government. By prioritizing transparent documentation, we can clearly communicate and connect the dots for folks when it comes to how we work, what data and methods we use, what that data tells us and the reasoning and actions we take throughout our processes, thus better enabling communities to understand how and why we make certain decisions and how all of that may impact them and others. This openness can empower current and potential future partners and communities to engage meaningfully with our work, ensuring that their voices are respected and needs are considered in ongoing and future work.\n\n\n\n\n\n\nOpen and Transparent Doesn’t Always Mean Public\n\n\n\nWhile we want to make as much of our process and products open and transparent as possible, this does not mean the EVERYTHING should be made public. It’s important to balance the need to keep confidential and private information secure while also making process and products open, transparent, and publicly available. This balance varies by project.\nWhat’s important is for teams to discuss what this balance looks like for them and act, document, and share accordingly.\n\n\nAccessibility: When our is projects, process, and products are presented in ways that center accessibility— such as offering offline materials, using multiple languages, or documenting in plain language — it lowers barriers and makes it easier for individuals to participate and engage. Prioritizing accessibility in our documentation not only increases access to vital information but also demonstrates a genuine commitment to inclusivity and respect for diverse experiences.\nCollaboration: If documentation exists and that documentation is comprehensive, reproducible, transparent, and accessible then (and only then!) will our projects, products, and process help us rebuild trust, repair relationships and/or begin to build new ones, which ultimately make it easier for communities to want to engage, collaborate and meaningfully contribute to our processes, or better yet, co-create products and resources with us. Prioritizing and investing in documentation, and creating it with an equity lens, can help us bridge gaps in communication and understanding, ultimately fostering trust, collaboration, and relationships with communities that have had the experience of being excluded or otherwise overlooked in our projects, products, and process.\nComprehensive documentation using an equity lens can be yet another tool we can use to operationalize equity and advance more equitable outcomes in our work.",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#what-to-document",
    "href": "documentation.html#what-to-document",
    "title": "Documentation",
    "section": "",
    "text": "Poor documentation is rarely used or referred to after it is written, and feels like (and often is) a waste of time.\nGreat documentation is an investment that is built over time. No one documents something and gets everything down perfectly the first time. Teams with great documentation decide progress is better than perfection and make time to iteratively improve, build on, and evolve what is documented over time so that it continues to be comprehensive and useful beyond when it was first drafted. This is why it’s critical for teams to think through:\n\nwhat the team will document\nhow documentation will be developed\nwhere documentation will be stored\nhow it will be shared (or not)\nwho will be responsible for creating and maintaining documentation over time\n\nWhen teams are beginning their documentation discussions and process, they should expect to document (almost) everything in some form, and plan to invest time accordingly. A list of things to consider documenting is provided in the graphic below.\n\n\n\n\n\n\nTip\n\n\n\nTrying to document everything below all at once can feel overwhelming and impossible. That’s because it is!\nRemember that perfection is the enemy of good. Focus on making iterative progress and improvements given your teams priorities, bandwidth, and capacity.\nWith time and consistent investment in documentation, your team can have everything below (and more!) documented comprehensively and in ways that advance and operationalize equity.\n\n\n\n\n\nIllustration of the different types of things to document for a project under four documentation categories: Team, Process, Technical, and User",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "documentation.html#additional-resources",
    "href": "documentation.html#additional-resources",
    "title": "Documentation",
    "section": "",
    "text": "Openscapes Pathway - Join Openscapes at the Water Boards to learn more and build your Team Pathway!\nSchema.org – Promoting common data structures on the internet\nAccessibility tips using Markdown (Smashing Magazine)\nWritethedocs.org, plus a piece on documenting for beginners\nTemplates for documenting on GitHub (e.g., on a ReadMe)\nCreating and maintaining a Data Dictionary\nUS EPA Guidance for Content Item Metadata in ArcGIS Online\nDocument code through comments and #tidydata\nGitHub projects best practices\n\n\n\n\nNOAA NMFS Open Science Resources - A Compilation of Resources and Experience to Support Open Science Communities in NOAA Fisheries\nNOAA Fisheries Integrated Toolbox - Browse software tools for NOAA Fisheries scientific work\nNASA Earthdata Cloud Cookbook - Supporting NASA Research Teams’ Migration to the Cloud\nOpenscapes Approach Guide - Guidance for Openscapes leaders on how to implement Openscapes programs and approaches",
    "crumbs": [
      "Describe"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\n\n\n\n\n\n\nThis Handbook is a draft / work in progress and is currently undergoing a phased review process\n\n\n\nSome pages are still under development - new content may be added and existing content may change at any time without notice.\nWater Boards Staff who would like to provide feedback, comments, suggestions, corrections, or ask questions about content in this Handbook can do so through the Equity Data Handbook Feedback Form\nThe official review period for the Handbook will be Jan - Mar 2025. We will consolidate and address as much feedback as possible shortly after the close of this review period. However, we will keep the form open beyond Mar 2025 so that folks can continue to provide constructive feedback moving forward, and we will consider and integrate that feedback as time, resources, and capacity allow.\n\n\nThis Equity Data Handbook is an online resource written by the The State Water Resources Control Board (State Water Board) and the nine Regional Water Quality Control Boards (Regional Water Boards), collectively known as the California Water Boards (Water Boards).\nContent in this Handbook includes best practices and guidance for Water Boards staff on incorporating racial equity concepts into their data-related work. Specifically, this Handbook will provide guidance and resources to help Water Boards staff conduct each phase of the data life cycle through a racial equity lens - from the planning and design of a project to data collection methods, visualization development (e.g., maps, fact sheets, etc.) and more!\nA depiction of the Data Life Cycle is provided below for context. We must have meaningful engagement and partnership with our diverse communities during each phase of the Data Life Cycle, especially those that have been historically underserved namely Black, Indigenous, and other People of Color (BIPOC). If we focus on uplifting those most highly impacted we will inevitably improve the the data products and services we develop and, most importantly, the experiences and outcomes for all communities.\n\n\n\nGraphic of the data life cycle.\n\n\nThis Quarto book is an open, living, and continuously iterating resource. If you have suggestions for additions or revisions you think should be incorporated into this book, please email equitydatahelp@waterboards.ca.gov.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html",
    "href": "assure-analyze/qaqc.html",
    "title": "Quality Assurance",
    "section": "",
    "text": "Before diving into your analyses or product development steps - you’ll want to assess the quality of your data according to quality assurance and quality control processes and standards that are relevant to your project.\n\n\n\n\n\n\nBe sure to set up your quality system early!\n\n\n\nImpactful and effective quality assurance requires setting up systems and thinking about quality long before the data analysis phase - be sure to start thinking about project and data quality early and take the time to get effective systems in place. See the sections below for more guidance and resources on how to do so with equity in mind.\n\n\n\n\nAccording to the Water Boards Quality Assurance/Quality Control Website:\n\nQuality assurance and quality control are components of a quality system. A quality system is a structured system that describes the policies and procedures for ensuring that work processes, products, or services satisfy the user’s specifications and expectations. A quality system is the means by which an organization manages its quality aspects. These aspects are Project Management, Data Generation and Acquisition, and Assessment and Oversight.\n\nQuality Assurance (QA) involves an integrated system of management activities that involves planning, implementation, documentation, assessment, reporting, and quality improvement to ensure that a process, item, or service is of the type and quality required for the project needs.\nQuality Control (QC) involves a system of technical activities that measures the attributes and performance of a process, item, or service against defined standards to verify that they meet the stated requirements established by the customer. QC also involves analytical frequency requirements and control limits.\n\n\n\nSetting up your QA system with an equity lens requires thinking through the systems that are already in place and assessing the extent to which they are supporting the advancement of equity objectives associated with your project. Consider reviewing the pages associated with the aforementioned QA activities to see if your systems are on track:\n\nPlanning: Planning, Data Preparation\nImplementation: Data Collection, Data Processing, Data Analysis, Data Visualization, Preservation & Storage\nDocumentation: Describe (aka Documentation)\nAssessment: Discover & Integrate (aka Evaluation)\nReporting: Data Sharing\nQuality Improvement: If some or all of your QA systems are not in support of your equity objectives, think through what changes you/your team can make to your process NOW to better operationalize equity.\n\nSetting up your QC system with an equity lens requires thinking through how we’re measuring quality/performance and the standards we’re using to assess quality.\n\nHow do our current performance measures help us assess the extent to which we’re meeting project objectives?\nDo any of our current performance measures make it difficult for us to achieve our equity objectives? If so, how might we modify or eliminate them?\nWhat additional performance measures would better help us assess the extent to which this data-intensive project is meeting the project’s equity objectives?\nDo the standards, thresholds, or limits we’re using align with the needs of those facing inequity or disparities - are or likely will be affected by a policy, program, or process? If not, how might we modify the them to better align with those needs?\n\nFrom the data science perspective, conducting QA/QC of the data requires checking values of the dataset against the aforementioned standards to understand the quality of the data.\n\nIs the metadata and vocabulary associated with each dataset complete, easy to access, and simple enough to be understood by a variety of audiences?\nAre the standards we’re using to check the quality of our data sufficient and relevant to our project objectives and/or in support of advancing equity outcomes? If not, why not? How should our project standards be revised so they can be more supportive of equity outcomes?\nIf data is of insufficient quality and needs to be excluded from future steps - where and how will documentation summarize what measures/standards were used to make such determination and which data were excluded?\n\n\n\n\n\nWater Boards QAQC Webpage\nSWAMP Bioaccumulation Monitoring Program Training Series\n\nData Quality and Data Management - Slides 67-80 | Recording Part 3\nQuality Assurance & Quality Control - Slides 5 - 38 | Recording Part 1",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#quality-assurancequality-control-qaqc-grounding",
    "href": "assure-analyze/qaqc.html#quality-assurancequality-control-qaqc-grounding",
    "title": "Quality Assurance",
    "section": "",
    "text": "According to the Water Boards Quality Assurance/Quality Control Website:\n\nQuality assurance and quality control are components of a quality system. A quality system is a structured system that describes the policies and procedures for ensuring that work processes, products, or services satisfy the user’s specifications and expectations. A quality system is the means by which an organization manages its quality aspects. These aspects are Project Management, Data Generation and Acquisition, and Assessment and Oversight.\n\nQuality Assurance (QA) involves an integrated system of management activities that involves planning, implementation, documentation, assessment, reporting, and quality improvement to ensure that a process, item, or service is of the type and quality required for the project needs.\nQuality Control (QC) involves a system of technical activities that measures the attributes and performance of a process, item, or service against defined standards to verify that they meet the stated requirements established by the customer. QC also involves analytical frequency requirements and control limits.",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#qaqc-with-an-equity-lens",
    "href": "assure-analyze/qaqc.html#qaqc-with-an-equity-lens",
    "title": "Quality Assurance",
    "section": "",
    "text": "Setting up your QA system with an equity lens requires thinking through the systems that are already in place and assessing the extent to which they are supporting the advancement of equity objectives associated with your project. Consider reviewing the pages associated with the aforementioned QA activities to see if your systems are on track:\n\nPlanning: Planning, Data Preparation\nImplementation: Data Collection, Data Processing, Data Analysis, Data Visualization, Preservation & Storage\nDocumentation: Describe (aka Documentation)\nAssessment: Discover & Integrate (aka Evaluation)\nReporting: Data Sharing\nQuality Improvement: If some or all of your QA systems are not in support of your equity objectives, think through what changes you/your team can make to your process NOW to better operationalize equity.\n\nSetting up your QC system with an equity lens requires thinking through how we’re measuring quality/performance and the standards we’re using to assess quality.\n\nHow do our current performance measures help us assess the extent to which we’re meeting project objectives?\nDo any of our current performance measures make it difficult for us to achieve our equity objectives? If so, how might we modify or eliminate them?\nWhat additional performance measures would better help us assess the extent to which this data-intensive project is meeting the project’s equity objectives?\nDo the standards, thresholds, or limits we’re using align with the needs of those facing inequity or disparities - are or likely will be affected by a policy, program, or process? If not, how might we modify the them to better align with those needs?\n\nFrom the data science perspective, conducting QA/QC of the data requires checking values of the dataset against the aforementioned standards to understand the quality of the data.\n\nIs the metadata and vocabulary associated with each dataset complete, easy to access, and simple enough to be understood by a variety of audiences?\nAre the standards we’re using to check the quality of our data sufficient and relevant to our project objectives and/or in support of advancing equity outcomes? If not, why not? How should our project standards be revised so they can be more supportive of equity outcomes?\nIf data is of insufficient quality and needs to be excluded from future steps - where and how will documentation summarize what measures/standards were used to make such determination and which data were excluded?",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/qaqc.html#additional-resources",
    "href": "assure-analyze/qaqc.html#additional-resources",
    "title": "Quality Assurance",
    "section": "",
    "text": "Water Boards QAQC Webpage\nSWAMP Bioaccumulation Monitoring Program Training Series\n\nData Quality and Data Management - Slides 67-80 | Recording Part 3\nQuality Assurance & Quality Control - Slides 5 - 38 | Recording Part 1",
    "crumbs": [
      "Assure & Analyze",
      "Quality Assurance"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html",
    "href": "assure-analyze/vis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Effective and equitable communication of the information we have gleaned from our analyses involves strategic consideration of the audience and the mode of dissemination that most effectively conveys that information. There are many ways to communicate information, including briefs, interactive documents, websites, dashboards, social media content, data walks, posters, briefs, and infographics. Regardless of the form, content should integrate the recommendations below.\nEffective data visualizations - the ones that stick with us, can help us understand complex issues, and those that encourage us to change our behaviors and actions to be more equitable, inclusive, and kind - are more than points on a map. Great data visualizations are communication tools that are user-centered and tell a compelling story that connects with the audience.\n\n\n\nVenn diagram of the components of data communication that make a product work for a particular audience: Content, Medium and Access. Graphic from We All Count\n\n\nWhen thinking of the Data-Information-Knowledge-Wisdom graphic at the top of the Data Analysis page: data visualizations are tools we can use to add context and meaning to data to create information and knowledge. Ideally, the insights gleaned from our visualizations can then be used to make data- and equity-informed decisions and to take effective and impactful action.\n\n\nWhen it comes to creating data visualizations with an equity lens, it boils down to making decisions that consider equity and inclusion in the way results are shown and communicated, and that promote accessibility of the data, information, and tool as a whole. As you develop your data visualization or equivalent application - be sure to keep the below considerations in mind and make data, communication, and design choices that support the advancement of equity, inclusion, and justice.\n\n\nConsider who the audience will be for the product you’re developing and make decisions that will prioritize their needs so they are able to easily and efficiently use, engage and interact with the product.\n\n\n\nGraphic illustrating three key components of the user-centered design process: Research, Empathy, and Iteration. Image from Justinmind (2020)\n\n\nResearch - Let’s use an equity lens here. Instead of “researching” your users and audience, try getting to know them by prioritizing relationship and trust building through meaningful engagement. Ideally, by the time you’re at this data visualization phase, you have already identified the audience and key partners, and have been working or engaging with them during other phases of the project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\nEmpathy - According to Dr. Brené Brown: empathy is about feeling WITH people, and requires four qualities:\n\nPerspective taking and believing the other person when they share their experience with you\nStaying out of judgement and listening\nRecognizing emotion in another person that you have maybe felt before\nCommunicating that you can recognize that emotion\n\nIntegrating empathy into our data-intensive work requires us to consider how our audience and/or the communities whose data are being used will perceive or be impacted by our work. It means ensuring we’re developing the product and thinking about the data as more than mere points on a map or visualization, but as representing real humans, environments, and conditions that should be contextualized and considered with care. Some questions to consider include:\n\nWho is vulnerable in this context and how would they want to be counted?\nWhat information would they need to improve their lives?\nWho is undercounted or possibly missing entirely?\nWho was counted? Who did the counting? Why were they asking these people?\nWho benefits or is harmed if you forget the dots are people?\n\nIteration - The key here is knowing and planning for an iterative process from the beginning. Add the feedback, implementation, and testing loop(s) into your project plan and allocate appropriate time and resources to adequately support each step. When working with partners or communities using an iterative approach, be sure to consider:\n\nWhen it is appropriate to ask for feedback, and when it might be burdensome?\nWhat are the different ways feedback might be gathered? We might think sending an email with a poll or survey linked is the easiest, but our partner might find it easier to talk through questions over the phone with you. Knowing which method(s) to use comes with time and relationship. When in doubt - ask for what people prefer and do your best to accommodate those needs.\nHow much time is adequate for folks to be able to review what you send them and provide their feedback? When in doubt - plan for a longer feedback period than you think might be needed and confirm timelines with your partners (and adjust them if you can when your partners indicate more or less time is needed)\n\n\n\n\nCentering racial equity means paying attention to how to most effectively convey information to the audience, and ensuring that the content is using plain, accessible, and inclusive language.\nPlain Language - Plain language is writing designed to ensure the audience can understand what you’re trying to communicate as easily, quickly, and comprehensively as possible. This means:\n\nAvoiding convoluted or verbose language\nAvoiding the use of jargon and acronyms\nMaking critical information easy to see and understand\nUsing a conversational rather than legal or bureaucratic tone\n\nFor more guidance on plain language, see:\n\nCenter for Plain Language\nPlainLanguage.gov\nWater Boards Staff may also request plain language review from the Office of Public Participation, although that service is more geared towards the review of fact sheets, brochures, and FAQs rather than data visualizations or other data products at this time.\nHealthy Watershed Partnership Guidance on Communicating Results\n\nAccessible Language - Making language accessible to your audience may also require the translation of products (or product components) into the primary languages used by your audience. The Water Board’s Linguistic Isolation Tool can be used to help understand the different languages that are used by communities across the state.\nWater Boards Staff may also request translation services from the Office of Public Participation, although that service is more geared towards the translation of documents, rather than data visualizations or other data products at this time.\nInclusive Language - Words matter. The language we use in our products can be a way for us to show respect, empathy, and care for the communities connected to or impacted by the data. It’s important to be mindful of the language and terms we use (see Non-Inclusive Terms to Avoid for examples). The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we need to decide on language to use in our data products:\n\nDo the words seek to fix, blame, shame, or change communities that are most marginalized, OR do they seek to address the oppressive systems that impact these communities?\nAre the words racialized? Do the terms have a racist or colonialist implication? An example is the phrase “at risk.” Close your eyes and say the phrase “at risk.” Does a picture of a certain group or community come to mind? If so, stop and pick another word.\nIs the language people-centered? Or do the phrases objectify communities? Distinguish between calling communities a name and describing what they are experiencing. For example: “people with disabilities,” “a person with asthma,” or “communities of color.”\nHow am I framing the words? What is the context and culture we are creating and perpetuating by using the words? Are the words positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nDo the words dehumanize the communities we serve? Words that take away agency, self-determination, or personal power and do not recognize communities’ inherent strengths and assets should be avoided.\n\n\n\n\nReview considerations provided on the Data Collection page and think through how those should be applied as you visualize and communicate the data and results of your project.\n\n\n\n\n“While data may seem cut and dry, people are not”\n- P. Kim Bui, Director of audience innovation at the Arizona Republic and author of Designing data visualisations with empathy\n\nHumans are hardwired for story. Plopping points on a map or throwing together an interactive chart without adding context is not only insufficient, its ineffective!\nTake the time to tell a story with the visualization or product you’re developing. Walk the audience through the the messages you want to convey - connect with their emotions, and back it up with the data. If you’re unsure what the story of your data is, or how to communicate it effectively, try collecting and distilling your thoughts using a COMPASS Message Box:\n\n\n\nThe COMPASS Message Box is a tool to help scientists sort and distill their (often technical or complex) knowledge in a way that will resonate with their chosen audience. Users complete the sections in any order and can then use the distilled information to communicate their science more effectively.\n\n\nEscape from the Ivory Tower: A Guide to Making Your Science Matter (Baron, 2010) is another great science communication resource.\n\n\n\n\nThis section walks through the different design decisions data practitioners must think through when developing a data visualization or similar data product, and provides recommendations on how to approach the work through a lens of accessibility, equity, and inclusion.\nData visualization design is composed of a series of compromises: Complexity vs Clarity. Depth vs. Speed. Pretty vs. Practical. The best visualizations maximize all of these, but usually, you have to prioritize some things over others. When designing a data visualization or similar data communication product, data practitioners should make decisions with their audience and equity in mind and do their best to follow the recommendations below to make the final data product as accessible and inclusive as possible.\n\n\nAlt Text is descriptive text, usually associated with images or figures, that conveys the meaning and context of the item. When creating alt text, remember:\n\nIt should complement the caption and not replicate it\nUse language to describe what the audience should take away from looking at the item\nIf there is text in the item, that text should be spelled out in the alt text\n\nTest to see if your alt text is effective by asking a colleague or project partner to listen to you (or a screen reader - see below) read the text aloud to them BEFORE they look at the item. Then, have them look at the item and ask if there are other details about the item that stand out to them as contributing to the meaning behind the item. If there are, add them in!\nScreen readers are computer software that read the text aloud and include programs like NVDA, JAWS, Acrobat, Word. A screen reader may already be integrated into your data visualization tool - if not, NVDA is free to download and is the Water Boards standard.\n\n\n\nColor can be a powerful communicator that should be selected purposefully. If we aren’t intentional with the colors we choose we may be unintentionally perpetuating stereotypes, creating hierarchies, or using color palettes that are not accessible to those with color vision differences.\nWhen selecting colors for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Data Visualization Style Guide:\n\nSelect the color palettes according to the message you’re trying to convey:\n\nCategorical palettes are best when you want to distinguish discrete chunks of data that do not have an inherent ordering. It’s best to use a palette that has bold, contrasting, non-gradient colors.\nSequential palettes are best when data range from relatively low or uninteresting values to relatively high or interesting values. It’s best to use a palette that has a relatively subtle shift in hue accompanied by a large shift in brightness and saturation.\n\nAvoid reinforcing stereotypes (e.g. using pink to represent female populations, and blue to represent male populations, using colors associated with skin tones or racial stereotypes).\nWhen thinking about demographic data:\n\nNot using color to differentiate between demographic groups helps avoid unintentionally creating a hierarchy between groups and reinforcing stereotypes, and instead enables color to be used to highlight group differences and assist with interpreting the data.\nIf color is determined to be necessary to differentiate between groups, the palette should be chosen carefully. Avoid using incremental color palettes (e.g., light to dark) to represent different demographic groups. Bold and contrasting, non-gradient color palettes are a best practice when displaying demographic data and incorporating color.\nConsider plotting data disaggregated by race or ethnicity in their own separate charts, instead of plotting all disaggregated groups together in a single chart.\n\nPlotting all groups on the same chart encourages comparison using a “deficit-based perspective” that focuses attention on what low-performing / worse off groups are lacking when compared with the high performing / better off groups.\nPlotting a set of small multiples faceted by each racial or ethnic group can better encourage viewers to think about the specific needs and challenges facing each group.\n\n\nAvoid using red to green color combinations. Daltonism (i.e., Dichromacy) is the most common type of color vision difference, affecting up to 1 in 12 males (8%) and 1 in 200 females (0.5%), and it makes certain shades of green look more red.\nDo not use color or shading alone to convey data or information. Instead pair color with patterns, textures, or shapes to convey data or information.\nAvoid using too many colors, shapes, and/or patterns at the same time. Our brains can really only hold (or effectively understand) three to five complex concepts at once. If you need to use six or more colors to represent the data - it might feel cluttered or overwhelming to the viewers. Consider consolidating categories or breaking up the chart into multiple smaller charts as described above.\nA 3:1 contrast ratio is required for figures if a person must perceive a graphic in order to understand the content (e.g., colored lines in graphs, points on maps). However, text that is embedded in a graphic must meet a contrast ratio of 4.5:1 unless the text is incidental or there is no way of presenting the graphic with sufficient contrast without undermining the meaning. Use the Colour Contrast Analyser (CCA) or equivalent software to determine if the figure meets the ratio requirements.\n\nTools to help with color selection may already be integrated into your data visualization tools - if not, some other resources include:\n\nColor Brewer – tool to help create accessible color schemes for maps\nNon-exhaustive list of color vision difference simulators – Color Oracle, Coblis\nNon-exhaustive list of color palette packages for R – Base R Color Palette Guide, RColorBrewer, rcartocolor, viridis\nPaletton – tool to help explore colors and palettes\nWebAIM – color contract checker\n\n\n\n\nAs data practitioners, you decide how your audience will be able to view the data through your visualization or data product. While we might think that data are objective (spoiler - they’re not), how they’re presented impacts how they’re interpreted, perceived, and ultimately valued and acted upon. How we present the data impacts which stories we tell (or don’t tell).\nThe graphic below, from Data Feminism, illustrates how the same data can tell different stories depending on their presentation. The graphic on the left tells a story of how the unemployment has fallen substantially since it’s recent peak and might be interpreted as “good news”. The graphic on the right tells a story of how the unemployment rate has been consistently above 8% for a long time period and might be interpreted as “bad news”.\n\n\n\nTwo figures presenting the same data in different ways. Graphic from Data Feminism\n\n\nWhen selecting figure or chart types for your visualization or data product, consider:\n\nHow does the chart you use frame or present the data? What is the context and culture we are creating and perpetuating by presenting the data in that way? Are the charts we’re using positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nIs the chart used appropriate for the data you want to display? Think about your variables (string/categorical and numeric), the volume of data, and the question you are attempting to answer or the story you’re trying to tell through the visualization or data product.\nWill the chart type be easily interpreted by the audience? Sometimes data practitioners might be inclined to use a complex chart so we can display more data at once (or just because we think it’s cool!). In moments like these, remember the words of Leonardo da Vinci — “Simplicity is the ultimate sophistication.” Think about what will be the most easy to understand and impactful for your audience.\nHow can I use the textual components of the chart (i.e., title, axis and other labels, legend, notes, caption, tooltips) to communicate simply and with clarity? More is not always more here. If context is provided elsewhere in the visualization or data product, maybe the title, caption, or tooltips are redundant. If there is only one data category that can be described by the title or caption, maybe a legend is not necessary. Think about what is essential and what can be removed or omitted to improve your clarity and communication.\nHow can other chart components (i.e. gridlines, axis lines/ticks, scales, spacing) be used to communicate simply and with clarity? If you have a lot of data on the chart - do gridlines help make sense of things, or cause confusion and clutter? Are there enough empty or negative spaces in the chart so that viewers are given the “visual breathing room” needed to easily absorb the messages you’re hoping the data conveys?\n\nTools to help with chart selection include:\n\nCool Infographics Data Visualization Guides – collection of data visualization chart choosers, reference guides, cheat sheets, websites and infographics about data visualization design best practices.\n\n\n\n\nThe fonts and typography we use determine how easy (or not) it is for the audience to efficiently read and comprehend whatever it is you’re trying to communicate. Typography (the art and technique of arranging type to make written language legible, readable and appealing when displayed) within a chart and visualization is a way to establish a hierarchy among elements within the visualization or data product and guides the reader through the visual product.\nWhen selecting fonts and typography for your visualization or data product, consider:\n\nAre the fonts you’re using accessible? The Water Boards recommends that all fonts (including in tables and figures) must be sans serif (e.g. Arial, Calibri, Helvetica), 12 point or larger. Serif fonts (e.g. Times New Roman) and fonts sized 11 points or lower (even in captions, tables, figures, etc.) should be avoided.\nHow many different font and size combinations are being used at once? Similar to the principles outlined in the color section above, using too many different types of fonts and sizes can make the visualization or data product feel cluttered or overwhelming to the viewers. Consider consolidating your use of fonts by giving different types of information with the same level of importance the same font and size.\nDoes the text have enough space around it? Some users may have cognitive disabilities or visual impairments that results in them having trouble reading lines of text when they’re too closely spaced. By prioritizing types of cognitive accessibility through thoughtful spacing, you contribute to a more inclusive visualization or data product.\n\nFor paragraphs - give the reader at least 1.15 pt spacing within paragraphs and at least 6 pt spacing after (or between) each paragraph.\nFor lists - if the list contains only a few words on each line, 1.15 pt spacing may be sufficient. If each item in the list has multiple lines of text, treat it as you would a paragraph and use 1.15 pt spacing within each list and at least 3 pt spacing after (or between) each item in the list.\nFor items within a chart - make sure the default spacing is sufficient. If items feel cramped, see if you can add spacing among items or reduce the amount of text needed for that particular item (e.g. shorten the title so it’s only a few words on a single line rather that a full sentence)\nSpecific recommendations the World Wide Web Consortium Web Content Accessibility Guidelines (WCAG) 2.1 on Text Spacing include:\n\nLine height (line spacing) to at least 1.5 times the font size;\nSpacing following paragraphs to at least 2 times the font size;\nLetter spacing (tracking) to at least 0.12 times the font size;\nWord spacing to at least 0.16 times the font size.\n\n\n\nTools to help with fonts & typography include:\n\nUrban Institute Data Visualization Style Guide - Chart Typography Section\n\n\n\n\nThe labels we use in our data products highlight or center the extent to which we reduce people and their experiences to components of our visualization or position and frame those same communities to live in their personal and collective power while addressing systemic oppression. Being mindful of the labels we use in our data products can help us connect and build trust with the communities that the data represents.\nWhen selecting the labels for your visualization or data product:\n\nUse clear and concise labels.\nUse full labels such as “Black people” rather than “Black.”\nAvoid using labels that reinforce stereotypes or inequitable or unjust norms.\nWhen describing people from certain demographics, use descriptors that reflect the preferences of the communities it represents as much as you can.\n\nLanguage continues to evolve. Certain labels that may have been acceptable years ago may no longer be, and we should adjust our labeling accordingly. The UNC Health’s Equity and Inclusion Analytics Workgroup notes that, for consistency and clarity, the data descriptors should match the categories used during data collection. However, if the data source you’re using has outdated language, updated language should be used in the label, and the outdated language can be noted in tooltips, comments, or footnotes of your visualization.\n\nBe mindful of how labels are ordered. Ordering items as they appear in the data may reflect and reinforce historical biases. The Urban Institute Do No Harm Guide recommends considering alternative sorting parameters such as study focus, specific story or argument, quantitative relationship (i.e., magnitude of the results), alphabetical, or sample size (weighted or unweighted).\n\n\n\n\nHow we layout the different components of our visualizations and data products - like color and typography - guides the reader through the their experience with said product. The order in which we present things establishes a hierarchy among elements that can indicate what we deem as important, significant or most valuable within product.\nWhen selecting the layout of the different components of your visualization or data product, consider:\n\nDoes the structure of your visualization resonate with how your audience sees the aspects of the data? \nAre you including (or excluding) elements arbitrarily or by default? Think about what would be meaningful to include (or not) for your audience. Are you adding items that don’t add value or context to the data story? If so, consider removing them. Are you removing items out of habit that should actually be included? If so, consider adding them.\nAre your captions, legends, and notes in intuitive locations?\n\nCaptions should go above tables, but below charts, images, and figures.\nLegends should stretch across the top of the chart, or to the right to make it easier for the viewer to orient themselves to the information in the product. \nNotes about a specific item (e.g. table, chart) should go below the item (and its caption, as applicable).\nFootnotes should go at the bottom of the page and only include crucial contextual information that is not explained elsewhere.\n\n\nWhen ordering different components of your visualization or data product, consider:\n\nAre data and categories ordered with intention and in a way that does not perpetuate historical biases? Which group(s) we choose to list first in figures, tables, and legends can influence how relationships and hierarchies are perceived.\n\nThe Urban Institute notes that, always starting with “White” or “Men” can make these groups appear as the default against which other groups should be compared, suggesting they’re the most important populations. How we choose to order may also reflect who we view as the intended audience for our visualizations. Starting with “White” or “Men” can make it seem as though those are the most important groups we are trying to communicate with.\n\nThe Urban Institute Do No Harm Guide recommends considering alternative sorting parameters for data, tables, and graphs such as:\n\nDoes your study focus on a particular community? If it does, that group should be presented first.\nIs there a particular argument or story you are trying to tell? If so, the order or presentation of results should reflect that argument.\nIs there a quantitative relationship that can guide how the groups are ordered? Can they be sorted alphabetically or by population size, sample size (weighted or unweighted), or magnitude or effect of the results?\n\n\n\n\n\nMaps can be powerful and intuitive ways to give place to the data you’re using in your visualization. However, just like all other visualizations, a map with points, lines, or polygons and zero context is not helpful and can actually be counter productive.\nWhen thinking about and building your map, consider:\n\nIs showing your geographic data on a map the best way to get your message across? The Urban Institute notes that at times, there may be more efficient forms of storytelling (other than maps) that can get your point across more clearly. Specifically, if your data shows a very clear geographic trend or if the absolute location of a place or event matters, maps might be the best approach, but sometimes the reflexive impulse to map the data can make you forget that showing the data in another form might answer other—and sometimes more important—questions. Consider using other graphic types when the interesting patterns are not geographic patterns, or when the geographic data is more effective for analysis than for presentation for your audience (i.e., a simple bar chart, column chart, scatterplot or table).\nIs the map projection you’re using appropriate?\n\nSimple interactive county- or state-level maps also use the Albers projection.\nUS maps for print publication, or use in reports should use the Albers Equal Area projection.\nZoomable tile-based interactive maps use the Mercator projection.\nNote that how a map is projected can influence the story being told, who is centered, and who is valued by the viewed, especially as you scale up. A great example of how map projections can impact our perception is summarized in the below clip from The West Wing. This Forbes article summarizes the scene well: The White House Press Secretary C.J. Gregg (Allison Janney) grants an audience to a group of socially minded cartographers on a noble mission: to convince the President to officially adopt a new global map that is more respectful of Third World countries. The group explains why conventional world maps are wrong and, worse, how this leads to misguided social perspectives that extend far beyond geography.\n\n\nIs the geography you’re using to visualize the data appropriate? The considerations we outlined above about grouping data also apply here, with an added geographic context, since the geography you use in your map (e.g., zip codes, census tracts, counties, watersheds, regions, etc.) will impact how the audience interacts with the map or visualization. The UNC Health’s Equity and Inclusion Analytics Workgroup notes that, zip codes may be convenient and readily available, but they represent expedient routes for postal delivery, not “natural” communities. This can cause analysis that disguises disparities, such as the example in Flint made by Sadler (2019).\nIs the color on the map effective? Maps tend to have more colors involved by nature (i.e. because of the inclusion of base maps, etc.). Take special care to adopt the considerations about color outlined above to ensure the map, data, and story you’re trying to tell are as accessible as possible.\n\nTools to help with map development include:\n\nState of Minnesota Map Accessibility Page – provides information on best practices for accessibility in static maps, web maps and applications.\nNon-exhaustive list of ESRI resources:\n\nAccessible Web Mapping Apps – recording from the 2020 ESRI Developer Summit\nESRI ArcGIS Accessibility Showcase (requires ESRI login)\nESRI GitHub Accessibility Repository – information on making AGOL/Portal maps accessible\nWeb Accessibility Best Practices – slides from 2019 ESRI GeoDev Webinar\nImprove accessibility of your Story Map by adding alternative text – ESRI blog post\nArcGIS Web AppBuilder accessibility support documentation\nArcGIS Social Equity Analysis tool documentation\nArcGIS Online Tips & Best Practices to Extend the Reach of your GIS\nArcGIS Hub guidance on improving the display of your content on a site and in search results\nArcGIS, ISO and INSPIRE Content Category recommendations\n\n\n\n\n\nSimilar to color, the shapes and icons we use in our our data visualizations or other data products can help the audience connect with the data, but we need to be thoughtful about what shapes and icons we use, how we use them, and how they may be perceived to avoid unintentionally perpetuating stereotypes or creating hierarchies.\nWhen selecting shapes and icons for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Do No Harm Guide:\n\nAvoid reinforcing stereotypes (e.g., a woman as a nurse but a man as a doctor). Mis- or underrepresentation of certain groups in imagery and iconography fails to take a racial or gender equity awareness perspective toward our data visualization work and can reinforce and perpetuate stereotypes. A 2018 study by the Pew Research Center found that:\n\nImage results for common job searches overrepresent men relative to women - an estimated 60% of the individuals that appeared in the image results were men (40% were women).\nFor more than half the tested job categories, images appearing in searches underrepresent women relative to their actual participation in those jobs\nImages containing women appear further down the page in search results for many jobs.\n\nAvoid discriminatory and racist imagery.\nUse images that show people as empowered and dignified, and avoid images that depict people as helpless victims\nWhen showing groups of people, we should consider a mix of genders, races, ethnicities, and abilities.\n\nTools to help with icon selection include:\n\nTips for designing icons that depict gender from Noun Project’s lead content curator Erika Kim\n\n\n\n\nMany data visualization applications enable data practitioners to display a visualization (chart, map) alongside a table. Including or embedding tables in a visualization can increases transparency and enable users to dig into the data behind the visualization without leaving said product. However, like every other design consideration we’ve discussed, the inclusion of tables should be considered with care.\nWhen determining how you will include table(s) in your visualization or data product, consider:\n\nIs including a table helpful or does it cause confusion? If the visualization is busy by itself, including a table that shows the exact same information can add to information overwhelm. If tables and figures/maps are not linked, adding filters in one place and then not seeing those same filters applied elsewhere can cause confusion. If this is the case for your visualization & table combination, consider excluding the table from the visualization to better focus on the story and instead make it easy for viewers to download the data to view outside of the visualization, or include the table in the visualization application but on a different tab (so it’s there, but not distracting).\nWhat information should be added to the caption to make interpretation of the table easier for viewers? While the data may make perfect sense to you, remember that is likely because you have invested hours of your time to collect, process, analyze, and visualize the data. This makes it easy for the you to quickly glance at the table(s) and figure(s) within the visualization and “instinctively” know what the viewers are supposed to take away. Your audience likely doesn’t have the time (or interest) to invest as you have, and forcing them to do so will only result in loss of interest and use of the resource. ALWAYS include a caption above the table to make it easier for users to quickly orient themselves to the information included in the table, and understand how the table contributes to or backs up the story you’re trying to tell. The caption should describe the table using as much detail as needed to make it a stand-alone object (i.e. viewers should not need to read the body text or other parts of the visualization or application to understand the table).\nAre table notes needed to increase clarity? Preferably, most of the text associated with a table is in the table caption. However, if it is necessary to include notes that are associated with the table that are separate from the caption, be sure to keep them simple and only include crucial contextual information that is not explained elsewhere.\nAre the data shown in the table essential to the story? If you’ve followed the guidance in the Data Processing step, then the data you use to create the visualization or data product should be tidy. Depending on the story you’re trying to tell and the audience you’re trying to connect with, including ALL of columns and rows of the processed data within visualization can be helpful or overwhelming. Consider only displaying the essential rows and columns within the visualization and making the complete dataset available for download.\n\nIf you’re unsure of what columns or rows are essential - try asking a colleague or project partner to listen to you (or a screen reader) read the first couple of lines of the table aloud to them BEFORE they look at the table and caption. Then, have them look at the table and caption and ask which pieces of the table should be added or removed to make the desired meaning more clear.\n\nIs highlighting being used to convey information? Screen readers do not tell the viewer the color of a cell. So, do not use color or shading alone to convey data or information. Authors may use color to improve the esthetics of a table, but if the color is representative of data or information, it should be coupled by text, symbols, or data that also convey that information so that those with color vision differences can access the information you’re trying to convey.\nWhat do blank cells mean? Screen readers do not tell the viewer that a cell is blank; they just skip the cell altogether. If a blank cell has a certain meaning in your data story (e.g. NA, 0, no data, etc.), state that within the cell explicitly or add a dash ( - ) to the blank cell(s) and add a note at the bottom of the table (or within the caption) to make viewers aware of the meaning of the dash.\n\n\n\n\n\n\nJustinmind Blog (2020) User-centered design: a beginner’s guide\nUrban Institute Resources\n\nData Visualization Style Guide\nDo No Harm Guide: Applying Equity Awareness in Data Visualization\n\nWe All Count - Reverse Engineering Data Viz for Equity\nTEDx Talk (2014) Hardwired for story\n\n\n\n\nCalifornia Water Board’s Tribal Water Data Map & Manual\nCalifornia Public Domain Allotment (PDA) Water Rights Map\nUnequal access to water: How societal factors shape vulnerability to water insecurity",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html#general-data-visualization-considerations",
    "href": "assure-analyze/vis.html#general-data-visualization-considerations",
    "title": "Data Visualization",
    "section": "",
    "text": "When it comes to creating data visualizations with an equity lens, it boils down to making decisions that consider equity and inclusion in the way results are shown and communicated, and that promote accessibility of the data, information, and tool as a whole. As you develop your data visualization or equivalent application - be sure to keep the below considerations in mind and make data, communication, and design choices that support the advancement of equity, inclusion, and justice.\n\n\nConsider who the audience will be for the product you’re developing and make decisions that will prioritize their needs so they are able to easily and efficiently use, engage and interact with the product.\n\n\n\nGraphic illustrating three key components of the user-centered design process: Research, Empathy, and Iteration. Image from Justinmind (2020)\n\n\nResearch - Let’s use an equity lens here. Instead of “researching” your users and audience, try getting to know them by prioritizing relationship and trust building through meaningful engagement. Ideally, by the time you’re at this data visualization phase, you have already identified the audience and key partners, and have been working or engaging with them during other phases of the project. See the Planning section on collecting expert input for more guidance on outreach and engagement.\nEmpathy - According to Dr. Brené Brown: empathy is about feeling WITH people, and requires four qualities:\n\nPerspective taking and believing the other person when they share their experience with you\nStaying out of judgement and listening\nRecognizing emotion in another person that you have maybe felt before\nCommunicating that you can recognize that emotion\n\nIntegrating empathy into our data-intensive work requires us to consider how our audience and/or the communities whose data are being used will perceive or be impacted by our work. It means ensuring we’re developing the product and thinking about the data as more than mere points on a map or visualization, but as representing real humans, environments, and conditions that should be contextualized and considered with care. Some questions to consider include:\n\nWho is vulnerable in this context and how would they want to be counted?\nWhat information would they need to improve their lives?\nWho is undercounted or possibly missing entirely?\nWho was counted? Who did the counting? Why were they asking these people?\nWho benefits or is harmed if you forget the dots are people?\n\nIteration - The key here is knowing and planning for an iterative process from the beginning. Add the feedback, implementation, and testing loop(s) into your project plan and allocate appropriate time and resources to adequately support each step. When working with partners or communities using an iterative approach, be sure to consider:\n\nWhen it is appropriate to ask for feedback, and when it might be burdensome?\nWhat are the different ways feedback might be gathered? We might think sending an email with a poll or survey linked is the easiest, but our partner might find it easier to talk through questions over the phone with you. Knowing which method(s) to use comes with time and relationship. When in doubt - ask for what people prefer and do your best to accommodate those needs.\nHow much time is adequate for folks to be able to review what you send them and provide their feedback? When in doubt - plan for a longer feedback period than you think might be needed and confirm timelines with your partners (and adjust them if you can when your partners indicate more or less time is needed)\n\n\n\n\nCentering racial equity means paying attention to how to most effectively convey information to the audience, and ensuring that the content is using plain, accessible, and inclusive language.\nPlain Language - Plain language is writing designed to ensure the audience can understand what you’re trying to communicate as easily, quickly, and comprehensively as possible. This means:\n\nAvoiding convoluted or verbose language\nAvoiding the use of jargon and acronyms\nMaking critical information easy to see and understand\nUsing a conversational rather than legal or bureaucratic tone\n\nFor more guidance on plain language, see:\n\nCenter for Plain Language\nPlainLanguage.gov\nWater Boards Staff may also request plain language review from the Office of Public Participation, although that service is more geared towards the review of fact sheets, brochures, and FAQs rather than data visualizations or other data products at this time.\nHealthy Watershed Partnership Guidance on Communicating Results\n\nAccessible Language - Making language accessible to your audience may also require the translation of products (or product components) into the primary languages used by your audience. The Water Board’s Linguistic Isolation Tool can be used to help understand the different languages that are used by communities across the state.\nWater Boards Staff may also request translation services from the Office of Public Participation, although that service is more geared towards the translation of documents, rather than data visualizations or other data products at this time.\nInclusive Language - Words matter. The language we use in our products can be a way for us to show respect, empathy, and care for the communities connected to or impacted by the data. It’s important to be mindful of the language and terms we use (see Non-Inclusive Terms to Avoid for examples). The UNC Health’s Equity and Inclusion Analytics Workgroup recommends we ask ourselves the following questions when we need to decide on language to use in our data products:\n\nDo the words seek to fix, blame, shame, or change communities that are most marginalized, OR do they seek to address the oppressive systems that impact these communities?\nAre the words racialized? Do the terms have a racist or colonialist implication? An example is the phrase “at risk.” Close your eyes and say the phrase “at risk.” Does a picture of a certain group or community come to mind? If so, stop and pick another word.\nIs the language people-centered? Or do the phrases objectify communities? Distinguish between calling communities a name and describing what they are experiencing. For example: “people with disabilities,” “a person with asthma,” or “communities of color.”\nHow am I framing the words? What is the context and culture we are creating and perpetuating by using the words? Are the words positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nDo the words dehumanize the communities we serve? Words that take away agency, self-determination, or personal power and do not recognize communities’ inherent strengths and assets should be avoided.\n\n\n\n\nReview considerations provided on the Data Collection page and think through how those should be applied as you visualize and communicate the data and results of your project.\n\n\n\n\n“While data may seem cut and dry, people are not”\n- P. Kim Bui, Director of audience innovation at the Arizona Republic and author of Designing data visualisations with empathy\n\nHumans are hardwired for story. Plopping points on a map or throwing together an interactive chart without adding context is not only insufficient, its ineffective!\nTake the time to tell a story with the visualization or product you’re developing. Walk the audience through the the messages you want to convey - connect with their emotions, and back it up with the data. If you’re unsure what the story of your data is, or how to communicate it effectively, try collecting and distilling your thoughts using a COMPASS Message Box:\n\n\n\nThe COMPASS Message Box is a tool to help scientists sort and distill their (often technical or complex) knowledge in a way that will resonate with their chosen audience. Users complete the sections in any order and can then use the distilled information to communicate their science more effectively.\n\n\nEscape from the Ivory Tower: A Guide to Making Your Science Matter (Baron, 2010) is another great science communication resource.",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html#data-visualization-design-considerations",
    "href": "assure-analyze/vis.html#data-visualization-design-considerations",
    "title": "Data Visualization",
    "section": "",
    "text": "This section walks through the different design decisions data practitioners must think through when developing a data visualization or similar data product, and provides recommendations on how to approach the work through a lens of accessibility, equity, and inclusion.\nData visualization design is composed of a series of compromises: Complexity vs Clarity. Depth vs. Speed. Pretty vs. Practical. The best visualizations maximize all of these, but usually, you have to prioritize some things over others. When designing a data visualization or similar data communication product, data practitioners should make decisions with their audience and equity in mind and do their best to follow the recommendations below to make the final data product as accessible and inclusive as possible.\n\n\nAlt Text is descriptive text, usually associated with images or figures, that conveys the meaning and context of the item. When creating alt text, remember:\n\nIt should complement the caption and not replicate it\nUse language to describe what the audience should take away from looking at the item\nIf there is text in the item, that text should be spelled out in the alt text\n\nTest to see if your alt text is effective by asking a colleague or project partner to listen to you (or a screen reader - see below) read the text aloud to them BEFORE they look at the item. Then, have them look at the item and ask if there are other details about the item that stand out to them as contributing to the meaning behind the item. If there are, add them in!\nScreen readers are computer software that read the text aloud and include programs like NVDA, JAWS, Acrobat, Word. A screen reader may already be integrated into your data visualization tool - if not, NVDA is free to download and is the Water Boards standard.\n\n\n\nColor can be a powerful communicator that should be selected purposefully. If we aren’t intentional with the colors we choose we may be unintentionally perpetuating stereotypes, creating hierarchies, or using color palettes that are not accessible to those with color vision differences.\nWhen selecting colors for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Data Visualization Style Guide:\n\nSelect the color palettes according to the message you’re trying to convey:\n\nCategorical palettes are best when you want to distinguish discrete chunks of data that do not have an inherent ordering. It’s best to use a palette that has bold, contrasting, non-gradient colors.\nSequential palettes are best when data range from relatively low or uninteresting values to relatively high or interesting values. It’s best to use a palette that has a relatively subtle shift in hue accompanied by a large shift in brightness and saturation.\n\nAvoid reinforcing stereotypes (e.g. using pink to represent female populations, and blue to represent male populations, using colors associated with skin tones or racial stereotypes).\nWhen thinking about demographic data:\n\nNot using color to differentiate between demographic groups helps avoid unintentionally creating a hierarchy between groups and reinforcing stereotypes, and instead enables color to be used to highlight group differences and assist with interpreting the data.\nIf color is determined to be necessary to differentiate between groups, the palette should be chosen carefully. Avoid using incremental color palettes (e.g., light to dark) to represent different demographic groups. Bold and contrasting, non-gradient color palettes are a best practice when displaying demographic data and incorporating color.\nConsider plotting data disaggregated by race or ethnicity in their own separate charts, instead of plotting all disaggregated groups together in a single chart.\n\nPlotting all groups on the same chart encourages comparison using a “deficit-based perspective” that focuses attention on what low-performing / worse off groups are lacking when compared with the high performing / better off groups.\nPlotting a set of small multiples faceted by each racial or ethnic group can better encourage viewers to think about the specific needs and challenges facing each group.\n\n\nAvoid using red to green color combinations. Daltonism (i.e., Dichromacy) is the most common type of color vision difference, affecting up to 1 in 12 males (8%) and 1 in 200 females (0.5%), and it makes certain shades of green look more red.\nDo not use color or shading alone to convey data or information. Instead pair color with patterns, textures, or shapes to convey data or information.\nAvoid using too many colors, shapes, and/or patterns at the same time. Our brains can really only hold (or effectively understand) three to five complex concepts at once. If you need to use six or more colors to represent the data - it might feel cluttered or overwhelming to the viewers. Consider consolidating categories or breaking up the chart into multiple smaller charts as described above.\nA 3:1 contrast ratio is required for figures if a person must perceive a graphic in order to understand the content (e.g., colored lines in graphs, points on maps). However, text that is embedded in a graphic must meet a contrast ratio of 4.5:1 unless the text is incidental or there is no way of presenting the graphic with sufficient contrast without undermining the meaning. Use the Colour Contrast Analyser (CCA) or equivalent software to determine if the figure meets the ratio requirements.\n\nTools to help with color selection may already be integrated into your data visualization tools - if not, some other resources include:\n\nColor Brewer – tool to help create accessible color schemes for maps\nNon-exhaustive list of color vision difference simulators – Color Oracle, Coblis\nNon-exhaustive list of color palette packages for R – Base R Color Palette Guide, RColorBrewer, rcartocolor, viridis\nPaletton – tool to help explore colors and palettes\nWebAIM – color contract checker\n\n\n\n\nAs data practitioners, you decide how your audience will be able to view the data through your visualization or data product. While we might think that data are objective (spoiler - they’re not), how they’re presented impacts how they’re interpreted, perceived, and ultimately valued and acted upon. How we present the data impacts which stories we tell (or don’t tell).\nThe graphic below, from Data Feminism, illustrates how the same data can tell different stories depending on their presentation. The graphic on the left tells a story of how the unemployment has fallen substantially since it’s recent peak and might be interpreted as “good news”. The graphic on the right tells a story of how the unemployment rate has been consistently above 8% for a long time period and might be interpreted as “bad news”.\n\n\n\nTwo figures presenting the same data in different ways. Graphic from Data Feminism\n\n\nWhen selecting figure or chart types for your visualization or data product, consider:\n\nHow does the chart you use frame or present the data? What is the context and culture we are creating and perpetuating by presenting the data in that way? Are the charts we’re using positioning the communities we serve to live in their personal and collective power while addressing systemic oppression?\nIs the chart used appropriate for the data you want to display? Think about your variables (string/categorical and numeric), the volume of data, and the question you are attempting to answer or the story you’re trying to tell through the visualization or data product.\nWill the chart type be easily interpreted by the audience? Sometimes data practitioners might be inclined to use a complex chart so we can display more data at once (or just because we think it’s cool!). In moments like these, remember the words of Leonardo da Vinci — “Simplicity is the ultimate sophistication.” Think about what will be the most easy to understand and impactful for your audience.\nHow can I use the textual components of the chart (i.e., title, axis and other labels, legend, notes, caption, tooltips) to communicate simply and with clarity? More is not always more here. If context is provided elsewhere in the visualization or data product, maybe the title, caption, or tooltips are redundant. If there is only one data category that can be described by the title or caption, maybe a legend is not necessary. Think about what is essential and what can be removed or omitted to improve your clarity and communication.\nHow can other chart components (i.e. gridlines, axis lines/ticks, scales, spacing) be used to communicate simply and with clarity? If you have a lot of data on the chart - do gridlines help make sense of things, or cause confusion and clutter? Are there enough empty or negative spaces in the chart so that viewers are given the “visual breathing room” needed to easily absorb the messages you’re hoping the data conveys?\n\nTools to help with chart selection include:\n\nCool Infographics Data Visualization Guides – collection of data visualization chart choosers, reference guides, cheat sheets, websites and infographics about data visualization design best practices.\n\n\n\n\nThe fonts and typography we use determine how easy (or not) it is for the audience to efficiently read and comprehend whatever it is you’re trying to communicate. Typography (the art and technique of arranging type to make written language legible, readable and appealing when displayed) within a chart and visualization is a way to establish a hierarchy among elements within the visualization or data product and guides the reader through the visual product.\nWhen selecting fonts and typography for your visualization or data product, consider:\n\nAre the fonts you’re using accessible? The Water Boards recommends that all fonts (including in tables and figures) must be sans serif (e.g. Arial, Calibri, Helvetica), 12 point or larger. Serif fonts (e.g. Times New Roman) and fonts sized 11 points or lower (even in captions, tables, figures, etc.) should be avoided.\nHow many different font and size combinations are being used at once? Similar to the principles outlined in the color section above, using too many different types of fonts and sizes can make the visualization or data product feel cluttered or overwhelming to the viewers. Consider consolidating your use of fonts by giving different types of information with the same level of importance the same font and size.\nDoes the text have enough space around it? Some users may have cognitive disabilities or visual impairments that results in them having trouble reading lines of text when they’re too closely spaced. By prioritizing types of cognitive accessibility through thoughtful spacing, you contribute to a more inclusive visualization or data product.\n\nFor paragraphs - give the reader at least 1.15 pt spacing within paragraphs and at least 6 pt spacing after (or between) each paragraph.\nFor lists - if the list contains only a few words on each line, 1.15 pt spacing may be sufficient. If each item in the list has multiple lines of text, treat it as you would a paragraph and use 1.15 pt spacing within each list and at least 3 pt spacing after (or between) each item in the list.\nFor items within a chart - make sure the default spacing is sufficient. If items feel cramped, see if you can add spacing among items or reduce the amount of text needed for that particular item (e.g. shorten the title so it’s only a few words on a single line rather that a full sentence)\nSpecific recommendations the World Wide Web Consortium Web Content Accessibility Guidelines (WCAG) 2.1 on Text Spacing include:\n\nLine height (line spacing) to at least 1.5 times the font size;\nSpacing following paragraphs to at least 2 times the font size;\nLetter spacing (tracking) to at least 0.12 times the font size;\nWord spacing to at least 0.16 times the font size.\n\n\n\nTools to help with fonts & typography include:\n\nUrban Institute Data Visualization Style Guide - Chart Typography Section\n\n\n\n\nThe labels we use in our data products highlight or center the extent to which we reduce people and their experiences to components of our visualization or position and frame those same communities to live in their personal and collective power while addressing systemic oppression. Being mindful of the labels we use in our data products can help us connect and build trust with the communities that the data represents.\nWhen selecting the labels for your visualization or data product:\n\nUse clear and concise labels.\nUse full labels such as “Black people” rather than “Black.”\nAvoid using labels that reinforce stereotypes or inequitable or unjust norms.\nWhen describing people from certain demographics, use descriptors that reflect the preferences of the communities it represents as much as you can.\n\nLanguage continues to evolve. Certain labels that may have been acceptable years ago may no longer be, and we should adjust our labeling accordingly. The UNC Health’s Equity and Inclusion Analytics Workgroup notes that, for consistency and clarity, the data descriptors should match the categories used during data collection. However, if the data source you’re using has outdated language, updated language should be used in the label, and the outdated language can be noted in tooltips, comments, or footnotes of your visualization.\n\nBe mindful of how labels are ordered. Ordering items as they appear in the data may reflect and reinforce historical biases. The Urban Institute Do No Harm Guide recommends considering alternative sorting parameters such as study focus, specific story or argument, quantitative relationship (i.e., magnitude of the results), alphabetical, or sample size (weighted or unweighted).\n\n\n\n\nHow we layout the different components of our visualizations and data products - like color and typography - guides the reader through the their experience with said product. The order in which we present things establishes a hierarchy among elements that can indicate what we deem as important, significant or most valuable within product.\nWhen selecting the layout of the different components of your visualization or data product, consider:\n\nDoes the structure of your visualization resonate with how your audience sees the aspects of the data? \nAre you including (or excluding) elements arbitrarily or by default? Think about what would be meaningful to include (or not) for your audience. Are you adding items that don’t add value or context to the data story? If so, consider removing them. Are you removing items out of habit that should actually be included? If so, consider adding them.\nAre your captions, legends, and notes in intuitive locations?\n\nCaptions should go above tables, but below charts, images, and figures.\nLegends should stretch across the top of the chart, or to the right to make it easier for the viewer to orient themselves to the information in the product. \nNotes about a specific item (e.g. table, chart) should go below the item (and its caption, as applicable).\nFootnotes should go at the bottom of the page and only include crucial contextual information that is not explained elsewhere.\n\n\nWhen ordering different components of your visualization or data product, consider:\n\nAre data and categories ordered with intention and in a way that does not perpetuate historical biases? Which group(s) we choose to list first in figures, tables, and legends can influence how relationships and hierarchies are perceived.\n\nThe Urban Institute notes that, always starting with “White” or “Men” can make these groups appear as the default against which other groups should be compared, suggesting they’re the most important populations. How we choose to order may also reflect who we view as the intended audience for our visualizations. Starting with “White” or “Men” can make it seem as though those are the most important groups we are trying to communicate with.\n\nThe Urban Institute Do No Harm Guide recommends considering alternative sorting parameters for data, tables, and graphs such as:\n\nDoes your study focus on a particular community? If it does, that group should be presented first.\nIs there a particular argument or story you are trying to tell? If so, the order or presentation of results should reflect that argument.\nIs there a quantitative relationship that can guide how the groups are ordered? Can they be sorted alphabetically or by population size, sample size (weighted or unweighted), or magnitude or effect of the results?\n\n\n\n\n\nMaps can be powerful and intuitive ways to give place to the data you’re using in your visualization. However, just like all other visualizations, a map with points, lines, or polygons and zero context is not helpful and can actually be counter productive.\nWhen thinking about and building your map, consider:\n\nIs showing your geographic data on a map the best way to get your message across? The Urban Institute notes that at times, there may be more efficient forms of storytelling (other than maps) that can get your point across more clearly. Specifically, if your data shows a very clear geographic trend or if the absolute location of a place or event matters, maps might be the best approach, but sometimes the reflexive impulse to map the data can make you forget that showing the data in another form might answer other—and sometimes more important—questions. Consider using other graphic types when the interesting patterns are not geographic patterns, or when the geographic data is more effective for analysis than for presentation for your audience (i.e., a simple bar chart, column chart, scatterplot or table).\nIs the map projection you’re using appropriate?\n\nSimple interactive county- or state-level maps also use the Albers projection.\nUS maps for print publication, or use in reports should use the Albers Equal Area projection.\nZoomable tile-based interactive maps use the Mercator projection.\nNote that how a map is projected can influence the story being told, who is centered, and who is valued by the viewed, especially as you scale up. A great example of how map projections can impact our perception is summarized in the below clip from The West Wing. This Forbes article summarizes the scene well: The White House Press Secretary C.J. Gregg (Allison Janney) grants an audience to a group of socially minded cartographers on a noble mission: to convince the President to officially adopt a new global map that is more respectful of Third World countries. The group explains why conventional world maps are wrong and, worse, how this leads to misguided social perspectives that extend far beyond geography.\n\n\nIs the geography you’re using to visualize the data appropriate? The considerations we outlined above about grouping data also apply here, with an added geographic context, since the geography you use in your map (e.g., zip codes, census tracts, counties, watersheds, regions, etc.) will impact how the audience interacts with the map or visualization. The UNC Health’s Equity and Inclusion Analytics Workgroup notes that, zip codes may be convenient and readily available, but they represent expedient routes for postal delivery, not “natural” communities. This can cause analysis that disguises disparities, such as the example in Flint made by Sadler (2019).\nIs the color on the map effective? Maps tend to have more colors involved by nature (i.e. because of the inclusion of base maps, etc.). Take special care to adopt the considerations about color outlined above to ensure the map, data, and story you’re trying to tell are as accessible as possible.\n\nTools to help with map development include:\n\nState of Minnesota Map Accessibility Page – provides information on best practices for accessibility in static maps, web maps and applications.\nNon-exhaustive list of ESRI resources:\n\nAccessible Web Mapping Apps – recording from the 2020 ESRI Developer Summit\nESRI ArcGIS Accessibility Showcase (requires ESRI login)\nESRI GitHub Accessibility Repository – information on making AGOL/Portal maps accessible\nWeb Accessibility Best Practices – slides from 2019 ESRI GeoDev Webinar\nImprove accessibility of your Story Map by adding alternative text – ESRI blog post\nArcGIS Web AppBuilder accessibility support documentation\nArcGIS Social Equity Analysis tool documentation\nArcGIS Online Tips & Best Practices to Extend the Reach of your GIS\nArcGIS Hub guidance on improving the display of your content on a site and in search results\nArcGIS, ISO and INSPIRE Content Category recommendations\n\n\n\n\n\nSimilar to color, the shapes and icons we use in our our data visualizations or other data products can help the audience connect with the data, but we need to be thoughtful about what shapes and icons we use, how we use them, and how they may be perceived to avoid unintentionally perpetuating stereotypes or creating hierarchies.\nWhen selecting shapes and icons for your visualization or data product, consider the recommendations from the UNC Health’s Equity and Inclusion Analytics Workgroup and the Urban Institute Do No Harm Guide:\n\nAvoid reinforcing stereotypes (e.g., a woman as a nurse but a man as a doctor). Mis- or underrepresentation of certain groups in imagery and iconography fails to take a racial or gender equity awareness perspective toward our data visualization work and can reinforce and perpetuate stereotypes. A 2018 study by the Pew Research Center found that:\n\nImage results for common job searches overrepresent men relative to women - an estimated 60% of the individuals that appeared in the image results were men (40% were women).\nFor more than half the tested job categories, images appearing in searches underrepresent women relative to their actual participation in those jobs\nImages containing women appear further down the page in search results for many jobs.\n\nAvoid discriminatory and racist imagery.\nUse images that show people as empowered and dignified, and avoid images that depict people as helpless victims\nWhen showing groups of people, we should consider a mix of genders, races, ethnicities, and abilities.\n\nTools to help with icon selection include:\n\nTips for designing icons that depict gender from Noun Project’s lead content curator Erika Kim\n\n\n\n\nMany data visualization applications enable data practitioners to display a visualization (chart, map) alongside a table. Including or embedding tables in a visualization can increases transparency and enable users to dig into the data behind the visualization without leaving said product. However, like every other design consideration we’ve discussed, the inclusion of tables should be considered with care.\nWhen determining how you will include table(s) in your visualization or data product, consider:\n\nIs including a table helpful or does it cause confusion? If the visualization is busy by itself, including a table that shows the exact same information can add to information overwhelm. If tables and figures/maps are not linked, adding filters in one place and then not seeing those same filters applied elsewhere can cause confusion. If this is the case for your visualization & table combination, consider excluding the table from the visualization to better focus on the story and instead make it easy for viewers to download the data to view outside of the visualization, or include the table in the visualization application but on a different tab (so it’s there, but not distracting).\nWhat information should be added to the caption to make interpretation of the table easier for viewers? While the data may make perfect sense to you, remember that is likely because you have invested hours of your time to collect, process, analyze, and visualize the data. This makes it easy for the you to quickly glance at the table(s) and figure(s) within the visualization and “instinctively” know what the viewers are supposed to take away. Your audience likely doesn’t have the time (or interest) to invest as you have, and forcing them to do so will only result in loss of interest and use of the resource. ALWAYS include a caption above the table to make it easier for users to quickly orient themselves to the information included in the table, and understand how the table contributes to or backs up the story you’re trying to tell. The caption should describe the table using as much detail as needed to make it a stand-alone object (i.e. viewers should not need to read the body text or other parts of the visualization or application to understand the table).\nAre table notes needed to increase clarity? Preferably, most of the text associated with a table is in the table caption. However, if it is necessary to include notes that are associated with the table that are separate from the caption, be sure to keep them simple and only include crucial contextual information that is not explained elsewhere.\nAre the data shown in the table essential to the story? If you’ve followed the guidance in the Data Processing step, then the data you use to create the visualization or data product should be tidy. Depending on the story you’re trying to tell and the audience you’re trying to connect with, including ALL of columns and rows of the processed data within visualization can be helpful or overwhelming. Consider only displaying the essential rows and columns within the visualization and making the complete dataset available for download.\n\nIf you’re unsure of what columns or rows are essential - try asking a colleague or project partner to listen to you (or a screen reader) read the first couple of lines of the table aloud to them BEFORE they look at the table and caption. Then, have them look at the table and caption and ask which pieces of the table should be added or removed to make the desired meaning more clear.\n\nIs highlighting being used to convey information? Screen readers do not tell the viewer the color of a cell. So, do not use color or shading alone to convey data or information. Authors may use color to improve the esthetics of a table, but if the color is representative of data or information, it should be coupled by text, symbols, or data that also convey that information so that those with color vision differences can access the information you’re trying to convey.\nWhat do blank cells mean? Screen readers do not tell the viewer that a cell is blank; they just skip the cell altogether. If a blank cell has a certain meaning in your data story (e.g. NA, 0, no data, etc.), state that within the cell explicitly or add a dash ( - ) to the blank cell(s) and add a note at the bottom of the table (or within the caption) to make viewers aware of the meaning of the dash.",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assure-analyze/vis.html#additional-resources",
    "href": "assure-analyze/vis.html#additional-resources",
    "title": "Data Visualization",
    "section": "",
    "text": "Justinmind Blog (2020) User-centered design: a beginner’s guide\nUrban Institute Resources\n\nData Visualization Style Guide\nDo No Harm Guide: Applying Equity Awareness in Data Visualization\n\nWe All Count - Reverse Engineering Data Viz for Equity\nTEDx Talk (2014) Hardwired for story\n\n\n\n\nCalifornia Water Board’s Tribal Water Data Map & Manual\nCalifornia Public Domain Allotment (PDA) Water Rights Map\nUnequal access to water: How societal factors shape vulnerability to water insecurity",
    "crumbs": [
      "Assure & Analyze",
      "Data Visualization"
    ]
  },
  {
    "objectID": "get-started/common-language.html",
    "href": "get-started/common-language.html",
    "title": "Establishing Common Language",
    "section": "",
    "text": "When working with a racial equity lens we suggest establishing a common language and definitions to cultivate a collective understanding of underlying concepts and historical context. Creating and agreeing upon a common language can help foster transparency, challenge assumptions, and center the voices of marginalized communities; yet the efficiency of these efforts hinges on a shared language that facilitates understanding and collaboration. By grounding discussions in a common language, we can build trust and empower our team and community.\nEstablishing a common language and definitions are critical to creating a shared understanding, however we acknowledge that language can be used deliberately to engage and support community anti-racism coalitions and initiatives, or to inflame and divide them. It is important to note that although the language in this Handbook may be commonly used, the list of terms herein is not exhaustive, and may not be the sole definition of a term, and some may disagree with the definitions and their use. More specifically, in this resource we intentionally use the acronym BIPoC (Black, Indigenous, People of Color) as a term that seeks to recognize the unique experience of Black and Indigenous People within the United States. We recognize that naming is power, and we remain committed to using language that supports pro-Blackness and Native visibility, while dismantling white supremacy.\nBelow we have included a Glossary and Non-Inclusive Terms to Avoid to establish a common language within the Guidance Document as it relates to Racial Equity. In addition, data terms used throughout the Guidance document will be defined as they arise on the following pages. A more extensive list of data terms and definitions can also be found on the College of Water Informatics Data Glossary page.\n\n\nBelow are a set of key terms and definitions provided by the Water Board Racial Equity Team in the development of the Racial Equity Resolution and Racial Equity Action Plan and are those that we adhere to in this document. (citations can be found here). For a more comprehensive list of equity-related terms, see the Racial Equity Tools Glossary\n\nDisadvantaged Community (DAC) is defined by the California Water Code as a community in which the median household income (MHI) is less than 80 percent of the statewide annual MHI (CA Water Code Section 13149.2 and 79505.5(a)). At the time this document was developed, the statewide annual MHI from the U.S. Census Bureau from 2017-2021 was $84,097 (U.S. Census Bureau QuickFacts: California). Based on this data, a community with a household income of less than $67,278 qualifies as a DAC based on the Water Code definition.\nEquality describes circumstances in which each individual or group is given the same or equal treatment, including the same resources, opportunities, and support. However, because different individuals or groups have different histories, needs, and circumstances, they do not have equal positions in society or starting points. Providing the same resources, support, or treatment does not guarantee that everyone will have fair or equal outcomes.\nEthnicity is a term used to describe subgroups of a population that share characteristics such as language, values, behavioral patterns, history, and ancestral geographical base. Social scientists often use the terms ethnicity and ethnic group to avoid the perception of biological significance associated with race.\nIntersectionality is a term used to describe the complex, cumulative way in which the effects of multiple forms of discrimination (such as racism, sexism, and classism) combine, overlap and intersect especially in the experiences of marginalized individuals or groups.\nInstitutional racism describes the ways in which policies and practices perpetuated by institutions, including governments and private groups, produce different outcomes for different racial groups in a manner that benefits the dominant group. In the United States, institutional racism includes policies that may not mention race but still result in benefiting white people over people of color.\nRace is a social construct used to categorize humans into groups based on combinations of shared physical traits such as skin color, hair texture, nose shape, eye shape, or head shape. Although most scientists agree that such groupings lack biological meaning, racial groups continue to have a strong influence over contemporary social relations. Historically in the United States, race has frequently been used to concentrate power with white people and legitimize dominance over non-white people.\nRacial equity means Race can no longer be used to predict life outcomes and outcomes for all groups are improved. For example, when we hold income constant, there are still large inequities based on race across multiple indicators for success, including the environment, education, jobs, incarceration, health and housing. \nRacism is any prejudice against someone because of their race when systems of power reinforce those views. \nStructural racism is the normalization and legitimization of an array of historical, cultural, institutional, and interpersonal dynamics that routinely advantage whites while producing cumulative and chronic adverse outcomes for people of color. Structural racism encompasses the entire system of white domination, diffused, and infused in all aspects of society, including its history, culture, politics, economics, and whole social fabric. Structural racism is more difficult to locate in a particular institution because it involves the reinforcing effects of multiple institutions and cultural norms, past and present, continually reproducing old and producing new forms of racism. Structural racism is the most profound and pervasive form of racism; all other forms of racism emerge from structural racism.\nSystemic racism can be said to encompass both institutional and structural racism. Glenn Harris, president of Race Forward, defines systemic racism as “the complex interaction of culture, policy and institutions that holds in place the outcomes we see in our lives.”  The legacy of systemic racism can be seen in a variety of outcomes affecting people of color, such as housing insecurity, a ten-fold wealth gap between white and Black or Latinx households, a dramatic over-representation of people of color in prison, and disparities in education, health, and exposure to environmental pollution.\n\n\n\n\nBelow are a set of common terms that are non-inclusive particularly to Black and Indigenous People and are commonly used in the workplace that have . This is not an exhaustive list but we recommend that research be conducted to review non-inclusive terms that may be used and have a history prior to engaging with communities. A good resource to start with is the University of Arizona’s Antiracist language guide.\n\nBlacklist - Blacklist typically refers to the ostracizing of a person, group, or organization that prevents them from participating in specific activities or spaces. This issue with the word blacklist is the association of the color with negative, evil, or wrong, and the racist undertones associated with it, and can be harmful.\n\nAlternatives: blocked, closed off, inaccessible, blocked list, banned, or closed list\n\nBrown Bag - The term “brown bag” has a historical connotation with creating an exclusive gathering that required attendees to have a lighter skin tone than a brown paper bag to participate and gain access.\n\nAlternatives: lunch in, lunch and learn, presentation, seminar\n\nChief - this term is used throughout the Water Boards to indicate positions and job titles.  This term is appropriated from the Indigenous Peoples of North America and should be avoided wherever possible.\n\nAlternatives: manager, lead, head\n\nGrandfathered in - The American South created absurd voting requirements that targeted Black people and made it almost impossible to vote. The name for these requirements is the “Grandfather Clause.” They wrote the Amendment in a way to imply the practice was not discriminatory. They created stringent new voter requirements such as literacy tests. These requirements did not apply to people who had voted before 1867. Slaves did not know they were free until June 19, 1865. However, slavery was abolished on January 1, 1863, making it nearly impossible for a person formally kept in captivity to be legally allowed to vote.\n\nAlternatives:  legacied, exempted, preapproved\n\nMaster ____ - using the term “master” to describe something that is the main or centralized source of information is inappropriate due to the connotations associated with slavery.\n\nAlternatives: primary, main\n\nPow wow - Social gatherings for ceremonial and celebratory purposes conducted under strict protocols. Avoid using the phrase to refer to a quick business meeting or informal social gathering as this is a form of cultural appropriation.\n\nAlternatives: meeting, gathering, or huddle\n\nSpirit Animal - These are spiritual guides that take the form of animals often viewed as sacred in tribal cultures. Non-native people appropriate the term to relate themselves to an animal, inanimate object, or person and draw parallels between the person and object’s characteristics. For example, saying that a sloth is your spirit animal because you are slow, lazy, and/or sleepy.\n\nAlternatives: patronus, kindred spirit, reason for living, muse, guide, or familiar.\n\nTotem Pole - Pieces of wood carved with a person’s totems. It is a tradition particular to Native and Indigenous people on the Northwest Coast. They tend to convey a family or tribe’s history. Avoid using phrases like “low on the totem pole” or “climbing the totem pole” as these are forms of cultural appropriation. These phrases are also inaccurate because in some First Nation communities being lower on the totem pole is a higher honor.\n\nAlternatives: climbing the corporate ladder, the lowest rung on the latter, least significant, or promotion.\n\nWhite Paper - while this term is widely used to describe an authoritative document, the term has historical implications that evoke negative associations especially with Tribes. \n\nAlternatives: Issue paper, briefing document, prospectus",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "get-started/common-language.html#glossary",
    "href": "get-started/common-language.html#glossary",
    "title": "Establishing Common Language",
    "section": "",
    "text": "Below are a set of key terms and definitions provided by the Water Board Racial Equity Team in the development of the Racial Equity Resolution and Racial Equity Action Plan and are those that we adhere to in this document. (citations can be found here). For a more comprehensive list of equity-related terms, see the Racial Equity Tools Glossary\n\nDisadvantaged Community (DAC) is defined by the California Water Code as a community in which the median household income (MHI) is less than 80 percent of the statewide annual MHI (CA Water Code Section 13149.2 and 79505.5(a)). At the time this document was developed, the statewide annual MHI from the U.S. Census Bureau from 2017-2021 was $84,097 (U.S. Census Bureau QuickFacts: California). Based on this data, a community with a household income of less than $67,278 qualifies as a DAC based on the Water Code definition.\nEquality describes circumstances in which each individual or group is given the same or equal treatment, including the same resources, opportunities, and support. However, because different individuals or groups have different histories, needs, and circumstances, they do not have equal positions in society or starting points. Providing the same resources, support, or treatment does not guarantee that everyone will have fair or equal outcomes.\nEthnicity is a term used to describe subgroups of a population that share characteristics such as language, values, behavioral patterns, history, and ancestral geographical base. Social scientists often use the terms ethnicity and ethnic group to avoid the perception of biological significance associated with race.\nIntersectionality is a term used to describe the complex, cumulative way in which the effects of multiple forms of discrimination (such as racism, sexism, and classism) combine, overlap and intersect especially in the experiences of marginalized individuals or groups.\nInstitutional racism describes the ways in which policies and practices perpetuated by institutions, including governments and private groups, produce different outcomes for different racial groups in a manner that benefits the dominant group. In the United States, institutional racism includes policies that may not mention race but still result in benefiting white people over people of color.\nRace is a social construct used to categorize humans into groups based on combinations of shared physical traits such as skin color, hair texture, nose shape, eye shape, or head shape. Although most scientists agree that such groupings lack biological meaning, racial groups continue to have a strong influence over contemporary social relations. Historically in the United States, race has frequently been used to concentrate power with white people and legitimize dominance over non-white people.\nRacial equity means Race can no longer be used to predict life outcomes and outcomes for all groups are improved. For example, when we hold income constant, there are still large inequities based on race across multiple indicators for success, including the environment, education, jobs, incarceration, health and housing. \nRacism is any prejudice against someone because of their race when systems of power reinforce those views. \nStructural racism is the normalization and legitimization of an array of historical, cultural, institutional, and interpersonal dynamics that routinely advantage whites while producing cumulative and chronic adverse outcomes for people of color. Structural racism encompasses the entire system of white domination, diffused, and infused in all aspects of society, including its history, culture, politics, economics, and whole social fabric. Structural racism is more difficult to locate in a particular institution because it involves the reinforcing effects of multiple institutions and cultural norms, past and present, continually reproducing old and producing new forms of racism. Structural racism is the most profound and pervasive form of racism; all other forms of racism emerge from structural racism.\nSystemic racism can be said to encompass both institutional and structural racism. Glenn Harris, president of Race Forward, defines systemic racism as “the complex interaction of culture, policy and institutions that holds in place the outcomes we see in our lives.”  The legacy of systemic racism can be seen in a variety of outcomes affecting people of color, such as housing insecurity, a ten-fold wealth gap between white and Black or Latinx households, a dramatic over-representation of people of color in prison, and disparities in education, health, and exposure to environmental pollution.",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "get-started/common-language.html#non-inclusive-terms-to-avoid",
    "href": "get-started/common-language.html#non-inclusive-terms-to-avoid",
    "title": "Establishing Common Language",
    "section": "",
    "text": "Below are a set of common terms that are non-inclusive particularly to Black and Indigenous People and are commonly used in the workplace that have . This is not an exhaustive list but we recommend that research be conducted to review non-inclusive terms that may be used and have a history prior to engaging with communities. A good resource to start with is the University of Arizona’s Antiracist language guide.\n\nBlacklist - Blacklist typically refers to the ostracizing of a person, group, or organization that prevents them from participating in specific activities or spaces. This issue with the word blacklist is the association of the color with negative, evil, or wrong, and the racist undertones associated with it, and can be harmful.\n\nAlternatives: blocked, closed off, inaccessible, blocked list, banned, or closed list\n\nBrown Bag - The term “brown bag” has a historical connotation with creating an exclusive gathering that required attendees to have a lighter skin tone than a brown paper bag to participate and gain access.\n\nAlternatives: lunch in, lunch and learn, presentation, seminar\n\nChief - this term is used throughout the Water Boards to indicate positions and job titles.  This term is appropriated from the Indigenous Peoples of North America and should be avoided wherever possible.\n\nAlternatives: manager, lead, head\n\nGrandfathered in - The American South created absurd voting requirements that targeted Black people and made it almost impossible to vote. The name for these requirements is the “Grandfather Clause.” They wrote the Amendment in a way to imply the practice was not discriminatory. They created stringent new voter requirements such as literacy tests. These requirements did not apply to people who had voted before 1867. Slaves did not know they were free until June 19, 1865. However, slavery was abolished on January 1, 1863, making it nearly impossible for a person formally kept in captivity to be legally allowed to vote.\n\nAlternatives:  legacied, exempted, preapproved\n\nMaster ____ - using the term “master” to describe something that is the main or centralized source of information is inappropriate due to the connotations associated with slavery.\n\nAlternatives: primary, main\n\nPow wow - Social gatherings for ceremonial and celebratory purposes conducted under strict protocols. Avoid using the phrase to refer to a quick business meeting or informal social gathering as this is a form of cultural appropriation.\n\nAlternatives: meeting, gathering, or huddle\n\nSpirit Animal - These are spiritual guides that take the form of animals often viewed as sacred in tribal cultures. Non-native people appropriate the term to relate themselves to an animal, inanimate object, or person and draw parallels between the person and object’s characteristics. For example, saying that a sloth is your spirit animal because you are slow, lazy, and/or sleepy.\n\nAlternatives: patronus, kindred spirit, reason for living, muse, guide, or familiar.\n\nTotem Pole - Pieces of wood carved with a person’s totems. It is a tradition particular to Native and Indigenous people on the Northwest Coast. They tend to convey a family or tribe’s history. Avoid using phrases like “low on the totem pole” or “climbing the totem pole” as these are forms of cultural appropriation. These phrases are also inaccurate because in some First Nation communities being lower on the totem pole is a higher honor.\n\nAlternatives: climbing the corporate ladder, the lowest rung on the latter, least significant, or promotion.\n\nWhite Paper - while this term is widely used to describe an authoritative document, the term has historical implications that evoke negative associations especially with Tribes. \n\nAlternatives: Issue paper, briefing document, prospectus",
    "crumbs": [
      "Getting Started",
      "Establishing Common Language"
    ]
  },
  {
    "objectID": "plan-prep/index.html",
    "href": "plan-prep/index.html",
    "title": "Plan & Prepare",
    "section": "",
    "text": "Plan & Prepare\n\n\n\n\n\n\nDon’t forget to publish your data!\n\n\n\nIf you are a steward of Water Boards data - and it is not yet made appropriately open and accessible (including thorough documentation and complete metadata!) - you should complete the guidance on the Publish Your Data page before you begin thinking about planning for your project.\n\n\nBefore data collection begins, it is paramount that you think through why you want to collect the data you think you need and how you intend to use it once it is collected.\nThis phase of the data life cycle involves conducting an equity assessment (Planning) and developing your data management plan using an equity lens (Data Preparation).",
    "crumbs": [
      "Plan & Prepare"
    ]
  }
]